\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[10pt,ignorenonframetext,]{beamer}
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% Prevent slide breaks in the middle of a paragraph:
\widowpenalties 1 10000
\raggedbottom
\setbeamertemplate{part page}{
\centering
\begin{beamercolorbox}[sep=16pt,center]{part title}
  \usebeamerfont{part title}\insertpart\par
\end{beamercolorbox}
}
\setbeamertemplate{section page}{
\centering
\begin{beamercolorbox}[sep=12pt,center]{part title}
  \usebeamerfont{section title}\insertsection\par
\end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
\centering
\begin{beamercolorbox}[sep=8pt,center]{part title}
  \usebeamerfont{subsection title}\insertsubsection\par
\end{beamercolorbox}
}
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
\usetheme[]{Singapore}
\usefonttheme{serif}
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Module 8: Tree-based Methods},
            pdfauthor={Stefanie Muff, Department of Mathematical Sciences, NTNU},
            colorlinks=true,
            linkcolor=Maroon,
            filecolor=Maroon,
            citecolor=Blue,
            urlcolor=blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\newif\ifbibliography
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{multirow}

\title{Module 8: Tree-based Methods}
\providecommand{\subtitle}[1]{}
\subtitle{TMA4268 Statistical Learning V2022}
\author{Stefanie Muff, Department of Mathematical Sciences, NTNU}
\date{March 7 and 10, 2022}

\begin{document}
\frame{\titlepage}

\begin{frame}{Acknowledgements}
\protect\hypertarget{acknowledgements}{}

\begin{itemize}
\item
  The course was originally developed by Mette Langaas (original
  material:
  \url{https://github.com/mettelang/StatisticalLearningSpring2019}).
  Mette did a fantastic job and I am very thankful that I was allowed to
  modify and use her material.
\item
  Some of the figures and slides in this presentation are taken (or are
  inspired) from James et al. (2013).
\end{itemize}

\end{frame}

\begin{frame}{Introduction}
\protect\hypertarget{introduction}{}

\begin{block}{Learning material for this module}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  James et al (2013): An Introduction to Statistical Learning. Chapter
  8.\\
\item
  All the material presented on these module slides.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{What will you learn?}

\vspace{2mm}

You will get to know

\begin{itemize}
\tightlist
\item
  Decision trees

  \begin{itemize}
  \tightlist
  \item
    Regression trees\\
  \item
    Classification trees\\
  \end{itemize}
\item
  Pruning a tree
\item
  Bagging
\item
  Variable importance
\item
  Random forests
\item
  Boosting
\end{itemize}

\(~\)

and learn how to apply all that.

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Example 1 (from chapter 8.1; \texttt{Hitters} data)}

\vspace{1mm}

\begin{itemize}
\item
  Baseball players' salaries may depend on their experience (in years)
  and the number of hits.
\item
  High salaries (yellow, red) vs low salaries (blue, green), salaries
  given on \(\log\)-scale. How can these be stratified for prediction of
  the salary?
\end{itemize}

\centering

\includegraphics[width=0.6\textwidth,height=\textheight]{hits.png}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Main idea of tree-based methods}

\vspace{2mm}

\begin{itemize}
\item
  Divide the area into rectangles with similar salaries.
\item
  Idea: Derive a set of decision (splitting) rules for segmenting the
  predictor space into a number of finer and finer regions.
\item
  All points in the same region will be given the same predictive value
  (the mean of all values in that square, or a majority vote).
\end{itemize}

\(~\)

Visualisaztion in two dimensions:

\centering

\includegraphics[width=0.5\textwidth,height=\textheight]{../../ISLR/Figures/Chapter8/8.2.png}

\end{block}

\end{frame}

\begin{frame}

\begin{itemize}
\item
  CARTs (Classification and regression trees) are usually drawn upside
  down, where the top node is called the \emph{root}.
\item
  The series of splitting rules can be visualized with a
  \emph{regression tree}.
\item
  The following tree (which corresponds to the split in the previous
  slide) has three \emph{leafs} (terminal nodes), and two \emph{internal
  nodes}.
\end{itemize}

\(~\)

\centering

\includegraphics[width=0.45\textwidth,height=\textheight]{../../ISLR/Figures/Chapter8/8.1.png}

\end{frame}

\begin{frame}

\begin{block}{Interpretation}

\(~\)

\begin{itemize}
\tightlist
\item
  Years is the most important factor in determining Salary, and players
  with less experience earn lower salaries than more experienced
  players.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  Given that a player is less experienced, the number of Hits that he
  made in the previous year seems to play little role in his Salary.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  But among players who have been in the major leagues for five or more
  years, the number of Hits made in the previous year does affect
  Salary, and players who made more Hits last year tend to have higher
  salaries.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  Compared to a regression model, it is easy to display, interpret and
  explain.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}{Regression tree (continous outcome)}
\protect\hypertarget{regression-tree-continous-outcome}{}

Assume that we have a dataset consisting of \(n\) pairs
\((\boldsymbol{x}_i,y_i)\), \(i=1,\ldots,n\), and each predictor is
\({\boldsymbol{x}}_i=(x_{i1},x_{i2},...,x_{ip})\). The aim is to predict
\(y_i\).

\vspace{2mm}

Two steps:

\begin{enumerate}
\item
  Divide the predictor space into non-overlapping regions
  \(R_1,R_2,\ldots,R_J\).
\item
  For every observation that falls into region \(R_j\) we make the same
  prediction - which is the mean of the responses for the training
  observations that fall into \(R_j\).
\end{enumerate}

\vspace{2mm}

\textbf{But}: How to divide the predictor space into non-overlapping
regions \(R_1,R_2,\ldots,R_J\)?

\end{frame}

\begin{frame}

We could try to minimize the RSS (residual sums of squares) on the
training set given by

\[
\text{RSS}=\sum_{j=1}^J \sum_{i \in R_j}(y_i-\hat{y}_{R_j})^2,
\]

where \(\hat{y}_{R_j}\) is the mean response for the training
observations in region \(j\). The mean \(\hat{y}_{R_j}\) is also the
predicted value for a new observations that falls into region \(j\).

To do this we need to consider every partition of the predictor space,
and compute the RSS for each partition.

\textbf{But}: An exhaustive search over possible splits is
\emph{computationally infeasible}!
\footnote{In fact, constructing optimal binary decision trees is an NP-complete problem (Hyafil and Rivest, 1976).}

\end{frame}

\begin{frame}

\begin{block}{Recursive binary splitting}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  A \emph{\textcolor{red}{greedy}} approach is taken (aka top-down) -
  called \emph{\textcolor{red}{recursive binary splitting}}: Find a
  split that minimizes RSS at each
  step\footnote{This does not necessarily give the optimal global solution, but will give the best solution at each split, given what is done previously.}.
\end{itemize}

\vspace{1mm}

\begin{itemize}
\tightlist
\item
  Start at the top of the tree and divide the predictor space into two
  regions \(R_1(j,s)=\{x \mid x_j<s\}\) and
  \(R_2(j,s)=\{x \mid x_j\geq s\}\), by making a decision rule for one
  of the predictors \(x_1, x_2,...,x_p\).
\end{itemize}

\vspace{1mm}

\begin{itemize}
\tightlist
\item
  We thus need to find the (predictor) \(j\) and (splitting point) \(s\)
  that minimize
  \[\sum_{i: x_i \in R_1(j,s)}(y_i-\hat{y}_{R_1})^2+\sum_{i: x_i \in R_2(j,s)}(y_i -\hat{y}_{R_2})^2 \ ,\]
  where \(\hat{y}_{R_1}\) and \(\hat{y}_{R_2}\) are the mean responses
  for the training observations in \(R_1(j,s)\) and \(R_2(j,s)\)
  respectively. This way we get the two first branches in our decision
  tree.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{itemize}
\item
  We repeat the process to make branches further down in the tree.
\item
  For every iteration we let each single split depend on \emph{only one
  of the predictors}, giving us two new branches.
\item
  This is done \emph{successively} and in each step we choose the split
  that gives the
  \emph{\textcolor{red}{best split at that particular step}},
  \emph{i.e.,} the split that gives the smallest RSS.
\item
  However, this time, instead of splitting the entire predictor space,
  we split only
  \emph{\textcolor{red}{one of the previously identified regions}}.
\item
  Continue splitting the predictor space until we reach some
  \emph{stopping criterion}. For example we stop when a region contains
  less than 10 observations or when the reduction in the RSS is smaller
  than a specified limit.
\end{itemize}

\vspace{1mm}

\textbf{Q}: Why is this algorithm called \emph{greedy}?

\end{frame}

\begin{frame}[fragile]

\begin{block}{Regression tree: ozone example}

\vspace{2mm}

Consider the \texttt{ozone} data set from the \texttt{ElemStatLearn}
library. The data set consists of 111 observations on the following
variables:

\begin{itemize}
\tightlist
\item
  \texttt{ozone} : the concentration of ozone in ppb
\item
  \texttt{radiation}: the solar radiation (langleys)
\item
  \texttt{temperature} : the daily maximum temperature in degrees F
\item
  \texttt{wind} : wind speed in mph
\end{itemize}

\vspace{2mm}

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(myozone)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   ozone radi temp wind
## 1    41  190   67  7.4
## 2    36  118   72  8.0
## 3    12  149   74 12.6
## 4    18  313   62 11.5
## 5    23  299   65  8.6
## 6    19   99   59 13.8
\end{verbatim}

\end{block}

\end{frame}

\begin{frame}[fragile]

Let's fit a regression tree with \texttt{ozone} as our response variable
and \texttt{temperature} and \texttt{wind} as predictors.

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ozone.trainID =}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{111}\NormalTok{, }\DecValTok{75}\NormalTok{)}
\NormalTok{ozone.train =}\StringTok{ }\NormalTok{myozone[ozone.trainID, ]}
\NormalTok{ozone.test =}\StringTok{ }\NormalTok{myozone[}\OperatorTok{-}\NormalTok{ozone.trainID, ]}
\end{Highlighting}
\end{Shaded}

\normalsize

Use the default settings in the tree function:

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tree)}
\NormalTok{ozone.tree =}\StringTok{ }\KeywordTok{tree}\NormalTok{(ozone }\OperatorTok{~}\StringTok{ }\NormalTok{temp }\OperatorTok{+}\StringTok{ }\NormalTok{wind, }\DataTypeTok{data =}\NormalTok{ ozone.train)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{8Trees_files/figure-beamer/ozone1-1} \end{center}

\end{frame}

\begin{frame}[fragile]

\begin{itemize}
\item
  We see that \texttt{temperature} is the ``most important'' predictor
  for predicting the ozone concentration. Observe that we can split on
  the same variable several times.
\item
  Focus on the regions \(R_j\), \(j=1,\ldots, J\). What is \(J\) here?
\end{itemize}

\end{frame}

\begin{frame}

\begin{block}{Tree- vs region plot}

\vspace{2mm}

\includegraphics{8Trees_files/figure-beamer/unnamed-chunk-5-1.pdf}

\vspace{-2mm}

\textbf{Q}:

\begin{itemize}
\tightlist
\item
  Explain the connection between the tree and the region plot.
\item
  Advantages and disadvantages of letting each single split depend on
  only one of the predictors?
\item
  Does our tree include interactions between variables?
\end{itemize}

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{block}{R: function \texttt{tree} in library \texttt{tree}}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  R package \texttt{tree} by Brian D. Ripley (Ripley 2019).
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  Note: The default choice for a function to minimize is the deviance,
  and for normal data (as we may assume for regression), the deviance is
  proportional to the RSS.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  A competing R function is \texttt{rpart}, explained in
  \url{https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf}
\end{itemize}

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Stopping criterion}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  When building the tree, we can (in principle) split until each leaf
  corresponds to one data point.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  Usually we use a less stringent stopping criterion, like the minimal
  number of nodes per region and/or the minimum reduction in the RSS.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  For example, the default in \texttt{tree()}: is given by
  \texttt{mincut=5,\ minsize=10,\ mindev=0.01}, thus a minimal number of
  observations in a node is 5, the smallest nodes that are potentially
  split is 10, and the minimal reduction in deviance (RSS) equal to 0.01
  times the deviance of the root note.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  We could make the ozone tree much deeper:
\end{itemize}

\(~\)

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ozone.tree2 =}\StringTok{ }\KeywordTok{tree}\NormalTok{(ozone }\OperatorTok{~}\StringTok{ }\NormalTok{temp }\OperatorTok{+}\StringTok{ }\NormalTok{wind, }\DataTypeTok{data =}\NormalTok{ ozone.train, }\DataTypeTok{control =} \KeywordTok{tree.control}\NormalTok{(}\DecValTok{75}\NormalTok{, }
    \DataTypeTok{mincut =} \DecValTok{2}\NormalTok{, }\DataTypeTok{minsize =} \DecValTok{4}\NormalTok{, }\DataTypeTok{mindev =} \FloatTok{0.001}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\end{block}

\end{frame}

\begin{frame}

With the above command, the tree would become very fine. Overfitting?

\begin{center}\includegraphics[width=0.7\linewidth]{8Trees_files/figure-beamer/ozone2-1} \end{center}

\end{frame}

\begin{frame}

\begin{block}{Tree performance}

\(~\)

Test the predictive performance of our regression tree by using a
training and a test set. But:

\vspace{2mm}

\vspace{1mm}

\normalsize

\emph{\textcolor{red}{How do we know if our tree performs good, or if there would be trees with a better predictive performance?}}

\end{block}

\end{frame}

\begin{frame}{Pruning}
\protect\hypertarget{pruning}{}

\begin{itemize}
\item
  If we have a data set with many predictors or choose a stringent
  stopping criterion, we may fit a (too) large tree. \(\rightarrow\)
  \emph{\textcolor{red}{overfitting}}?
\item
  A smaller tree with fewer splits leads to fewer regions
  \(R_1, \ldots, R_J\) with more observations. \(\rightarrow\) Lower
  variance and better interpretation, but more bias.
\item
  One possible alternative to the process described above is to grow the
  tree only so long as the decrease in the RSS due to each split exceeds
  some (high) threshold.
\item
  This strategy will result in smaller trees, but is
  \emph{\textcolor{red}{too short-sighted}}: a seemingly worthless split
  early on in the tree might be followed by a very good split --- that
  is, a split that leads to a large reduction in RSS later on.
\end{itemize}

\end{frame}

\begin{frame}

\begin{block}{Cost complexity pruning}

\(~\)

\begin{itemize}
\tightlist
\item
  Better idea: to grow a very large tree \(T_0\), and then
  \emph{\textcolor{red}{prune}} it back in order to obtain a
  \emph{\textcolor{red}{subtree}}.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  \emph{\textcolor{red}{Cost complexity pruning}} is used for this: We
  try to find a subtree \(T\subset T_0\) that (for a given value of
  \(\alpha\)) minimizes \[
  C_{\alpha}(T)=Q(T)+\alpha |T|,
  \] where \(Q(T)\) is our cost function, \(|T|\) is the number of
  terminal nodes in tree \(T\). The parameter \(\alpha\) is then a
  parameter penalizing the number of terminal nodes, ensuring that the
  tree does not get too many branches.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  For regression trees (continous outcome) we choose
  \[Q(T)=\sum_{m=1}^{|T|}\sum_{x_i\in R_m}(y_i - \hat{y}_{R_m})^2 \ .\]
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{itemize}
\item
  For \(\alpha>0\) we get a pruned tree (note: the same pruned tree
  within a small range of \(\alpha\)).
\item
  For \(\alpha=0\) we get \(T_0\).
\item
  As \(\alpha\) increases we get shorter and shorter trees.
\end{itemize}

So, which value of \(\alpha\) is best?

\begin{itemize}
\item
  We can use \(K\)-fold cross-validation to find out!
\item
  Importantly, by increasing \(\alpha\), branches get pruned in a
  \emph{hierarchical (nested) fashion}.
\end{itemize}

Please study this
\href{https://www.math.ntnu.no/emner/TMA4268/2018v/notes/CART1MA87012017BoLindqvist.pdf}{note
from Bo Lindqvist in MA8701 in 2017 - Advanced topics in Statistical
Learning and Inference} for an example of how we perform cost complexity
pruning in detail\footnote{see also link on course website}.
Alternatively, this method, with proofs, are given in Ripley (1996),
Section 7.2.

\end{frame}

\begin{frame}

\begin{block}{Building a regression tree: Algorithm 8.1}

\(~\)

\begin{enumerate}
\tightlist
\item
  Use recursive binary splitting to grow a large tree on the training
  data, stopping only when each terminal node has fewer than some
  minimum number of observations.
\end{enumerate}

\vspace{2mm}

\begin{enumerate}
\setcounter{enumi}{1}
\tightlist
\item
  Apply cost complexity pruning to the large tree in order to obtain a
  sequence of best subtrees, as a function of \(\alpha\).
\end{enumerate}

\vspace{2mm}

\begin{enumerate}
\setcounter{enumi}{2}
\tightlist
\item
  Use \(K\)-fold cross-validation to choose \(\alpha\). That is, divide
  the training observations into \(k\) folds. For each
  \(k = 1,\ldots, K\):

  \begin{itemize}
  \tightlist
  \item
    Repeat Steps 1 and 2 on all but the \(k\)th fold of the training
    data.
  \item
    Evaluate the mean squared prediction error on the data in the
    left-out \(k\)th fold, as a function of \(\alpha\).
  \item
    Average the results for each value of \(\alpha\), and pick
    \(\alpha\) to minimize the average error.
  \end{itemize}
\end{enumerate}

\vspace{2mm}

\begin{enumerate}
\setcounter{enumi}{3}
\tightlist
\item
  Return the subtree from Step 2 that corresponds to the chosen value of
  \(\alpha\).
\end{enumerate}

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Pruning the ozone tree}

Let us start with the tree with many leaves, and then prune it with
5-fold CV. We can then plot the CV error as a function of tree size
(instead of \(\alpha\) -- why?):

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{563}\NormalTok{)}
\NormalTok{ozone.tree <-}\StringTok{ }\KeywordTok{tree}\NormalTok{(ozone }\OperatorTok{~}\StringTok{ }\NormalTok{temp }\OperatorTok{+}\StringTok{ }\NormalTok{wind, }\DataTypeTok{data =}\NormalTok{ ozone.train, }\DataTypeTok{control =} \KeywordTok{tree.control}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(myozone), }
    \DataTypeTok{mincut =} \DecValTok{2}\NormalTok{, }\DataTypeTok{minsize =} \DecValTok{4}\NormalTok{, }\DataTypeTok{mindev =} \FloatTok{0.001}\NormalTok{))}
\NormalTok{cv.ozone <-}\StringTok{ }\KeywordTok{cv.tree}\NormalTok{(ozone.tree, }\DataTypeTok{K =} \DecValTok{5}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(cv.ozone}\OperatorTok{$}\NormalTok{dev }\OperatorTok{~}\StringTok{ }\NormalTok{cv.ozone}\OperatorTok{$}\NormalTok{size, }\DataTypeTok{type =} \StringTok{"b"}\NormalTok{, }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }
    \DataTypeTok{xlab =} \StringTok{"Tree Size"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"Deviance"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.5\linewidth]{8Trees_files/figure-beamer/ozone.cv-1} \end{center}

\end{block}

\end{frame}

\begin{frame}[fragile]

Interestingly, a tree with 5 leaves performs best - which corresponds to
the original choice:

\vspace{2mm}

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prune.ozone <-}\StringTok{ }\KeywordTok{prune.tree}\NormalTok{(ozone.tree, }\DataTypeTok{best =} \DecValTok{5}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(prune.ozone)}
\KeywordTok{text}\NormalTok{(prune.ozone, }\DataTypeTok{pretty =} \DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.5\linewidth]{8Trees_files/figure-beamer/unnamed-chunk-7-1} \end{center}

\end{frame}

\begin{frame}{Classification trees (binary or categorical outcome)}
\protect\hypertarget{classification-trees-binary-or-categorical-outcome}{}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  Now allow for \(K\geq 2\) number of classes for the response.
\end{itemize}

\vspace{1mm}

\begin{itemize}
\tightlist
\item
  Building a decision tree in this setting is similar to building a
  regression tree for a quantitative response, but there are two main
  differences: \emph{the prediction} and \emph{the splitting criterion}.
\end{itemize}

\end{frame}

\begin{frame}

\textbf{1) The prediction:}

\begin{itemize}
\item
  In the regression case we use the mean value of the responses in
  \(R_j\) as a prediction for an observation that falls into region
  \(R_j\). \vspace{2mm}
\item
  For the \emph{\textcolor{red}{classification case}}, however, we have
  two possibilities:

  \begin{itemize}
  \item
    \textbf{Majority vote}: Predict that the observation belongs to the
    most commonly occurring class of the training observations in
    \(R_j\).\\
    \vspace{2mm}
  \item
    Estimate the \textbf{probability} that an observation \(x_i\)
    belongs to a class \(k\), \(\hat{p}_{jk}(x_i)\), given as the
    proportion of class \(k\) training observations in region \(R_j\).
    Region \(j\) has \(N_j\) observations and with \(n_{jk}\)
    observations lying in class k:
  \end{itemize}

  \[\hat{p}_{jk} = \frac{1}{N_j} \sum_{i:x_i \in R_j} I(y_i = k)=\frac{n_{jk}}{N_j}.\]
\end{itemize}

\end{frame}

\begin{frame}

\textbf{2) The splitting criterion:} We do not use RSS as a splitting
criterion for a qualitative variable. Instead we can use some
\emph{measure of impurity} of the node. For leaf node \(j\) and class
\(k=1,\ldots, K\):

\begin{itemize}
\tightlist
\item
  \textbf{Gini index}: \[
  G=\sum_{k=1}^K \hat{p}_{jk}(1-\hat{p}_{jk}) \ ,
  \] which is small if all of the \(\hat{p}_{jk}\)'s are close to 0 or
  1.
\end{itemize}

\vspace{1mm}

\begin{itemize}
\tightlist
\item
  \textbf{Cross entropy}: \[
  D=-\sum_{k=1}^K \hat{p}_{jk}\log\hat{p}_{jk} \ .
  \] Since \(0\leq\hat{p}_{jk}\leq 1\), it follows that
  \(0\leq -\hat{p}_{jk}\log\hat{p}_{jk}\), with values near zero if
  \(\hat{p}_{jk}\) is close to 0 or 1.
\end{itemize}

When making a split in our classification tree, we want to minimize the
Gini index or the cross-entropy.

\end{frame}

\begin{frame}

Why don't we just minimize the misclassification error
\[E= 1- \max_k{\hat{p}_{jk}} \ ?\]

\textbf{A}:

\begin{itemize}
\item
  The Gini index and Entropy are \emph{measure of impurity}. They are
  more sensitive to changes in the node probabilities.
\item
  Example for two classes: Assume we have two classes with 400 nodes
  each, written as \((400,400)\). Now we can choose between two splits:

  \begin{itemize}
  \tightlist
  \item
    Split 1: (100,300) and (300,100)\\
  \item
    Split 2: (200,400) and (200,0).
  \end{itemize}
\end{itemize}

Both splits result in 25\% misclassification. Which of these splits is
better? Probably the second one, because it produces a \emph{pure node}.

\end{frame}

\begin{frame}

\begin{itemize}
\tightlist
\item
  Moreover, The Gini index and Entropy are differentiable (preferred for
  numerical optimization!)
\end{itemize}

\begin{center}\includegraphics[width=0.6\linewidth]{8Trees_files/figure-beamer/purity-1} \end{center}

\end{frame}

\begin{frame}

\begin{block}{Example: Detection of Minor Head Injury}

\tiny

(Artificial data) \vspace{2mm}

\normalsize

\begin{itemize}
\tightlist
\item
  Data from patients that enter hospital. The aim is to quickly assess
  whether a patient as a brain injury or not.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  Patients are investigated and (possibly) asked questions.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\item
  \emph{\textcolor{red}{Our job}}: To build a good model to predict
  quickly if someone has a brain injury. The method should be
  \vspace{1mm}

  \begin{itemize}
  \item
    \textbf{easy} to interpret for the medical personell that are not
    skilled in statistics, and \vspace{1mm}
  \item
    \textbf{fast}, such that the medical personell quickly can identify
    a patient that needs treatment.
  \end{itemize}
\end{itemize}

\end{block}

\end{frame}

\begin{frame}[fragile]

The dataset includes data about 1321 patients and is a modified and
smaller version of the (simulated) dataset \texttt{headInjury} from the
\texttt{DAAG} library.

\scriptsize

\begin{verbatim}
##    amnesia bskullf GCSdecr GCS.13 GCS.15 risk consc oskullf vomit brain.injury
## 3        0       0       0      0      0    0     0       0     0            0
## 9        0       0       0      0      0    1     0       0     0            0
## 11       0       0       0      0      0    0     0       0     0            0
## 12       1       0       0      0      0    0     0       0     0            0
## 14       0       0       0      0      0    0     0       0     0            0
## 16       0       0       0      0      0    0     0       0     0            0
##    age
## 3   44
## 9   67
## 11  62
## 12   1
## 14  55
## 16  63
\end{verbatim}

\normalsize

\end{frame}

\begin{frame}[fragile]

\begin{block}{Split with cross-entropy}

\(~\)

We use the 850 training samples to get a tree using cross-entropy
(deviance):

\(~\)

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tree.HIClass =}\StringTok{ }\KeywordTok{tree}\NormalTok{(brain.injury }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ headInjury2, }\DataTypeTok{subset =}\NormalTok{ train, }
    \DataTypeTok{split =} \StringTok{"deviance"}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(tree.HIClass)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Classification tree:
## tree(formula = brain.injury ~ ., data = headInjury2, subset = train, 
##     split = "deviance")
## Variables actually used in tree construction:
## [1] "GCS.15"  "bskullf" "risk"    "age"     "GCS.13" 
## Number of terminal nodes:  9 
## Residual mean deviance:  0.66 = 555 / 841 
## Misclassification error rate: 0.126 = 107 / 850
\end{verbatim}

\vspace{6mm}

\textbf{Remark}: the deviance (\(-2\log(L)\)) is a scaled version of the
cross entropy:
\[-2\sum_{k=1}^K n_{jk} \log\hat{p}_{jk}\ , \text{where} \quad   \hat{p}_{jk}=\frac{n_{jk}}{N_j}\ , \]
thus \texttt{split="deviance"} implies that we split according to the
entropy criterion.

\end{block}

\end{frame}

\begin{frame}[fragile]

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(tree.HIClass, }\DataTypeTok{type =} \StringTok{"proportional"}\NormalTok{)}
\KeywordTok{text}\NormalTok{(tree.HIClass, }\DataTypeTok{pretty =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.5\linewidth]{8Trees_files/figure-beamer/headInjury-1} \end{center}

\(~\)

\normalsize

\begin{itemize}
\item
  With \texttt{type="proportional"} (default), the length of branches
  are proportional to the decrease in impurity.
\item
  The classification tree has two terminal nodes with factor ``0''
  originating from the same branch. Why do we get this ``unnecessary''
  split?
\end{itemize}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Split with Gini index}

\(~\)

The same analysis with the Gini index:

\(~\)

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tree.HIClassG =}\StringTok{ }\KeywordTok{tree}\NormalTok{(brain.injury }\OperatorTok{~}\StringTok{ }\NormalTok{., headInjury2, }\DataTypeTok{subset =}\NormalTok{ train, }\DataTypeTok{split =} \StringTok{"gini"}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(tree.HIClassG)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Classification tree:
## tree(formula = brain.injury ~ ., data = headInjury2, subset = train, 
##     split = "gini")
## Variables actually used in tree construction:
## [1] "GCS.15"  "bskullf" "risk"    "age"     "oskullf" "vomit"   "amnesia"
## [8] "consc"   "GCS.13" 
## Number of terminal nodes:  75 
## Residual mean deviance:  0.52 = 403 / 775 
## Misclassification error rate: 0.109 = 93 / 850
\end{verbatim}

\end{block}

\end{frame}

\begin{frame}[fragile]

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(tree.HIClassG, }\DataTypeTok{type =} \StringTok{"proportional"}\NormalTok{)}
\KeywordTok{text}\NormalTok{(tree.HIClassG, }\DataTypeTok{pretty =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{8Trees_files/figure-beamer/headInjury2-1} \end{center}

\normalsize

This is a very bushy tree!

\end{frame}

\begin{frame}[fragile]

\begin{block}{Checking predictions}

\vspace{2mm}

We also use the classification tree to predict the status of the
patients in the test set.

\(~\)

With the deviance:

\(~\)

\tiny

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)}
\NormalTok{tree.pred =}\StringTok{ }\KeywordTok{predict}\NormalTok{(tree.HIClass, headInjury2[test, ], }\DataTypeTok{type =} \StringTok{"class"}\NormalTok{)}
\NormalTok{(confMat <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(tree.pred, }\DataTypeTok{reference =}\NormalTok{ headInjury2[test, }
\NormalTok{    ]}\OperatorTok{$}\NormalTok{brain.injury)}\OperatorTok{$}\NormalTok{table)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Reference
## Prediction   0   1
##          0 356  42
##          1  24  49
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \OperatorTok{-}\StringTok{ }\KeywordTok{sum}\NormalTok{(}\KeywordTok{diag}\NormalTok{(confMat))}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(confMat[}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.140127
\end{verbatim}

\end{block}

\end{frame}

\begin{frame}[fragile]

\normalsize

With the Gini-index:

\(~\)

\tiny

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tree.predG =}\StringTok{ }\KeywordTok{predict}\NormalTok{(tree.HIClassG, headInjury2[test, ], }\DataTypeTok{type =} \StringTok{"class"}\NormalTok{)}
\NormalTok{(confMatG <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(tree.predG, }\DataTypeTok{reference =}\NormalTok{ headInjury2[test, }
\NormalTok{    ]}\OperatorTok{$}\NormalTok{brain.injury)}\OperatorTok{$}\NormalTok{table)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Reference
## Prediction   0   1
##          0 354  47
##          1  26  44
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \OperatorTok{-}\StringTok{ }\KeywordTok{sum}\NormalTok{(}\KeywordTok{diag}\NormalTok{(confMatG))}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(confMatG[}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.154989
\end{verbatim}

\end{frame}

\begin{frame}

\begin{block}{Group discussion}

\(~\)

Study the confusion matrices on the previous slides:

\vspace{2mm}

\begin{itemize}
\item
  Which algorithm makes more errors?
\item
  Is one type of mistakes more severe than the other?
\item
  Discuss if/how it is possible to change the algorithm in order to
  decrease the number of severe mistakes.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Building a classification tree: Algorithm 8.1}

\(~\)

\begin{itemize}
\tightlist
\item
  Like for regression trees, we would like to find classification trees
  that are good at prediction.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  Algorithm 8.1 can be used with misclassification, the Gini index, or
  cross-entropy instead of the RSS as quality measure.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Finding an optimal classification tree}

\vspace{2mm}

We prune the classification tree that was built with the deviance
criterion. We use the misclassification error to do
so\footnote{While trees should be built using the deviance or the Gini index, pruning is typically done with the misclassification criterion when the aim is predictive accuracy.}:

\vspace{3mm}

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{cv.head =}\StringTok{ }\KeywordTok{cv.tree}\NormalTok{(tree.HIClass, }\DataTypeTok{FUN =}\NormalTok{ prune.misclass)}
\KeywordTok{plot}\NormalTok{(cv.head}\OperatorTok{$}\NormalTok{size, cv.head}\OperatorTok{$}\NormalTok{dev, }\DataTypeTok{type =} \StringTok{"b"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"Terminal nodes"}\NormalTok{, }
    \DataTypeTok{ylab =} \StringTok{"Misclassifications"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.5\linewidth]{8Trees_files/figure-beamer/headInjury3-1} \end{center}

\end{block}

\end{frame}

\begin{frame}[fragile]

The function \texttt{cv.tree} automatically does \(10\)-fold
cross-validation. \texttt{dev} is here the number of misclassifications.

\vspace{3mm}

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(cv.head)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $size
## [1] 9 7 6 4 1
## 
## $dev
## [1] 129 129 149 149 166
## 
## $k
## [1] -Inf  0.0  6.0  6.5 11.0
## 
## $method
## [1] "misclass"
## 
## attr(,"class")
## [1] "prune"         "tree.sequence"
\end{verbatim}

\end{frame}

\begin{frame}[fragile]

\begin{itemize}
\item
  We have done cross-validation on our training set of 850 observations.
  According to the plot, the number of misclassifications is as low for
  7, 8 or 9 nodes, so we choose 7 terminal nodes as the smallest model
  (lowest variance).
\item
  We prune the classification tree according to this value:
\end{itemize}

\vspace{4mm}

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prune.HIClass =}\StringTok{ }\KeywordTok{prune.misclass}\NormalTok{(tree.HIClass, }\DataTypeTok{best =} \DecValTok{7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{frame}

\begin{frame}[fragile]

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(prune.HIClass)}
\KeywordTok{text}\NormalTok{(prune.HIClass, }\DataTypeTok{pretty =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{8Trees_files/figure-beamer/headInjury4-1} \end{center}

\normalsize

\(\rightarrow\) No unnecessary splits left, and we have a simple and
interpretable decision tree.

How is the predictive performance of the model affected?

\end{frame}

\begin{frame}[fragile]

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tree.pred.prune <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(prune.HIClass, headInjury2[test, ], }\DataTypeTok{type =} \StringTok{"class"}\NormalTok{)}
\NormalTok{(confMat <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(tree.pred.prune, headInjury2[test, ]}\OperatorTok{$}\NormalTok{brain.injury)}\OperatorTok{$}\NormalTok{table)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Reference
## Prediction   0   1
##          0 356  42
##          1  24  49
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{(}\KeywordTok{sum}\NormalTok{(}\KeywordTok{diag}\NormalTok{(confMat))}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(confMat[}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.140127
\end{verbatim}

\normalsize

\vspace{4mm}

\(\rightarrow\) We see that the misclassification rate is as small as
before, thus the pruned tree is as good as the original tree for the
test data.

\vspace{2mm}

\(\rightarrow\) If you want, you can apply the same pruning procedure to
the very bushy Gini-grown tree.

\end{frame}

\begin{frame}

\vspace{2mm}

\begin{block}{Questions:}

\vspace{2mm}

Discuss the \emph{bias-variance tradeoff of a regression tree} when
increasing/decreasing the number of terminal nodes, i.e:

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  What happens to the bias?
\item
  What happens to the variance of a prediction if we reduce the tree
  size?
\end{itemize}

\end{block}

\end{frame}

\begin{frame}{Trees versus linear models}
\protect\hypertarget{trees-versus-linear-models}{}

\begin{itemize}
\item
  What if we have \(x_1\) and \(x_2\) and the true class boundary (two
  classes) is linear in \(x_1\), \(x_2\) space. How can we do that with
  our binary recursive splits?
\item
  What about a rectangular boundary?
\end{itemize}

\centering

\includegraphics[width=0.5\textwidth,height=\textheight]{../../ISLR/Figures/Chapter8/8.7.png}

\end{frame}

\begin{frame}{From trees to forests}
\protect\hypertarget{from-trees-to-forests}{}

\textbf{Advantages (+)}

\begin{itemize}
\tightlist
\item
  Trees automatically select variables.
\item
  Tree-growing algorithms scale well to large \(n\), growing a tree
  greedily.
\item
  Trees can handle mixed features (continouos, categorical) seamlessly,
  and can deal with missing data.
\item
  Small trees are easy to interpret and explain to people.
\item
  Some believe that decision trees mirror human decision making.
\item
  Trees can be displayed graphically.
\end{itemize}

\textbf{Disadvantages (-)}

\begin{itemize}
\tightlist
\item
  Large trees are not easy to interpret.
\item
  Trees do not generally have good prediction performance (high
  variance).
\item
  Trees are not very robust, a small change in the data may cause a
  large change in the final estimated tree.
\end{itemize}

\end{frame}

\begin{frame}

\begin{block}{What is next?}

\vspace{2mm}

Several of the above listed disadvantages can be addressed by
\vspace{2mm}

\begin{itemize}
\item
  \textbf{Bagging}: grow many trees (from bootstrapped data) and average
  - to get rid of the non-robustness and high variance by averaging.
\item
  \textbf{Random forests}: inject more randomness (and even less
  variance) by just allowing a random selection of predictors to be used
  for the splits at each node.
\item
  \textbf{Boosting}: make one tree, then another based on the residuals
  from the previous, repeat. The final predictor is a weighted sum of
  these trees.
\end{itemize}

\vspace{2mm}

In addition:

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  \textbf{Variable importance} - to see which variables make a
  difference (now that we have many trees).
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Leo Breiman - the inventor of CART, bagging and random
forests}

\(~\)

\vspace{1mm}

See Wikipedia entry: \url{https://en.wikipedia.org/wiki/Leo_Breiman}

\vspace{1mm}

\(~\)

\begin{quote}
Breiman's work helped to bridge the gap between statistics and computer science, particularly in the field of machine learning. His most important contributions were his work on classification and regression trees and ensembles of trees fit to bootstrap samples. Bootstrap aggregation was given the name bagging by Breiman. Another of Breiman's ensemble approaches is the random forest. 
\end{quote}

\end{block}

\end{frame}

\begin{frame}{Bagging}
\protect\hypertarget{bagging}{}

\vspace{2mm}

\begin{itemize}
\item
  Decision trees often suffer from high variance. By this we mean that
  the trees are sensitive to small changes in the predictors: If we
  change the observation set, we may get a very different tree.
\item
  Another way to understand ``high variance'' is that, if we split our
  training data into two parts and fit a tree on each, we might get
  rather different decision trees.
\end{itemize}

\begin{itemize}
\tightlist
\item
  To reduce the variance of decision trees we can apply \emph{bootstrap
  aggregating} (\emph{bagging}), invented by Leo Breiman in 1996
  (Breiman 1996).
\end{itemize}

\end{frame}

\begin{frame}[fragile]

\begin{block}{High variance in decision trees -- Illustration}

\(~\)

\begin{itemize}
\item
  Let's draw a new training set (\texttt{train2}) and use the deviance
  criterion to grow the tree.
\item
  The two classification trees contructed from \(850\) different random
  subsets each:
\end{itemize}

\(~\)

\begin{center}\includegraphics[width=0.9\linewidth]{8Trees_files/figure-beamer/headInjury7-1} \end{center}

\(~\)

\begin{itemize}
\tightlist
\item
  We get two rather different trees.
\end{itemize}

\vspace{2mm}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Recall: Variance for independent datasets}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  Assume we have \(B\) \emph{i.i.d.} observations of a random variable
  \(X\) each with the same mean and with variance \(\sigma^2\). We
  calculate the mean \(\bar{X} = \frac{1}{B} \sum_{b=1}^B X_b\). The
  variance of the mean is
  \[\text{Var}(\bar{X}) = \text{Var}\Big(\frac{1}{B}\sum_{b=1}^B X_b \Big) = \frac{1}{B^2} \sum_{b=1}^B \text{Var}(X_b) = \frac{\sigma^2}{B}.\]
  By averaging we get reduced variance. This is the basic idea!
\end{itemize}

\(~\)

\begin{itemize}
\tightlist
\item
  For decision trees, if we have \(B\) training sets, we could estimate
  \(\hat{f}_1({\boldsymbol x}),\hat{f}_2({\boldsymbol x}),\ldots, \hat{f}_B({\boldsymbol x})\)
  and average them as
  \[ \hat{f}_{avg}({\boldsymbol x})=\frac{1}{B}\sum_{b=1}^B \hat{f}_b({\boldsymbol x}) \ .\]
\end{itemize}

However, we do not have many independent data set - so we use
\emph{bootstrapping} to construct \(B\) data sets.

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Bagging = Bootstrap aggregating}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  Bootsrapping: Draw \emph{\textcolor{red}{with replacement}} \(n\)
  observations from our sample - and that is our first \emph{bootstrap
  sample}.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  We repeat this \(B\) times and get \(B\) bootstrap samples - that we
  use as our \(B\) data sets.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  For each bootstrap sample \(b=1,\ldots, B\) we construct a decision
  tree, \(\hat{f}^{*b}(x)\).
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  For a \emph{regression tree}, we take the average of all of the
  predictions and use this as the final result: \[
  \hat{f}_{bag}(x)=\frac{1}{B}\sum_{b=1}^B \hat{f}^{*b}(x).
  \]
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  For a \emph{classification tree} we record the predicted class (for a
  given observation \(x\)) for each of the \(B\) trees and use the most
  occurring classification (\emph{majority vote}) as the final
  prediction.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Out-of-bag error estimation}

\(~\)

\begin{itemize}
\tightlist
\item
  Recall (module 5) that the probability that an observation is in the
  bootstrap sample is approximately \(1-e^{-1}\)=0.63 (\(\approx 2/3\)).
\end{itemize}

\vspace{1mm}

\begin{itemize}
\tightlist
\item
  When an observation is left out of the bootstrap sample it is not used
  to build the tree, and we can use this observation as a part of a
  ``test set'' to measure the predictive performance and error of the
  fitted model, \(\hat{f}^{*b}(x)\).
\end{itemize}

\vspace{1mm}

\begin{itemize}
\tightlist
\item
  For \(B\) bootstrap samples, observation \(i\) will be outside the
  bootstrap sample in approximately \(B/3\) of the fitted trees. Obtain
  a single prediction by averaging (regression problem) or taking the
  majority vote/mean probability (classification problem) of the \(B/3\)
  predictions.
\end{itemize}

\(~\)

\textbf{Terminology:}

\begin{itemize}
\item
  The observations left out are referred to as the
  \emph{\textcolor{red}{out-of-bag}} (OOB) observations.
\item
  The measured error of the \(B/3\) predictions is called the
  \emph{\textcolor{red}{out-of-bag error}}.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Example}

\(~\)

We can do bagging by using the function \texttt{randomForest()} in the
\texttt{randomForest} library.

\(~\)

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(randomForest)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{r.brain.bag <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(brain.injury }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ headInjury2, }\DataTypeTok{subset =}\NormalTok{ train, }
    \DataTypeTok{mtry =} \DecValTok{10}\NormalTok{, }\DataTypeTok{ntree =} \DecValTok{500}\NormalTok{, }\DataTypeTok{importance =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{r.brain.bag}\OperatorTok{$}\NormalTok{confusion}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     0  1 class.error
## 0 637 54   0.0781476
## 1  76 83   0.4779874
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \OperatorTok{-}\StringTok{ }\KeywordTok{sum}\NormalTok{(}\KeywordTok{diag}\NormalTok{(r.brain.bag}\OperatorTok{$}\NormalTok{confusion))}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(r.brain.bag}\OperatorTok{$}\NormalTok{confusion[}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.152941
\end{verbatim}

\normalsize

\(~\)

The variable \texttt{mtry=10} because we want to consider all \(10\)
predictors in each split of the tree. The variable \texttt{ntree=500}
because we want to average over \(500\) trees.

\end{block}

\end{frame}

\begin{frame}[fragile]

Predictive performance of the bagged tree on unseen test data:

\vspace{2mm}

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{yhat.bag =}\StringTok{ }\KeywordTok{predict}\NormalTok{(r.brain.bag, }\DataTypeTok{newdata =}\NormalTok{ headInjury2[test, ])}
\NormalTok{misclass.bag =}\StringTok{ }\KeywordTok{table}\NormalTok{(yhat.bag, headInjury2[test, ]}\OperatorTok{$}\NormalTok{brain.injury)}
\KeywordTok{print}\NormalTok{(misclass.bag)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         
## yhat.bag   0   1
##        0 346  39
##        1  34  52
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \OperatorTok{-}\StringTok{ }\KeywordTok{sum}\NormalTok{(}\KeywordTok{diag}\NormalTok{(misclass.bag))}\OperatorTok{/}\NormalTok{(}\KeywordTok{sum}\NormalTok{(misclass.bag))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.154989
\end{verbatim}

\normalsize

Note: The misclassification rate has increased slightly for the bagged
tree (as compared to our previous full and pruned tree).
\textbf{Typically, we would expect an improvement!}

\end{frame}

\begin{frame}

\begin{block}{Variable importance plots}

\(~\)

\begin{itemize}
\tightlist
\item
  Drawback of bagging: It becomes difficult to interpret the results.
  Instead of having one tree, the resulting model consists of many trees
  (it is an \emph{ensemble method}).
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  \emph{\textcolor{red}{Variable importance plots}} show \emph{the
  relative importance of the predictors:} the predictors are sorted
  according to their importance, such that the top variables have a
  higher importance than the bottom variables.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\item
  There are in general two types of variable importance plots:

  \begin{itemize}
  \tightlist
  \item
    variable importance based on decrease in node impurity.
  \item
    variable importance based on randomization.
  \end{itemize}
\end{itemize}

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Variable importance based on node impurity}

\vspace{1mm}

\emph{Importance} relates to \emph{total decrease in the node impurity,
over splits for a predictor}.

\vspace{4mm}

\textbf{Regression trees:}

\begin{itemize}
\tightlist
\item
  The importance of each predictor is calculated using the RSS.
\item
  The algorithm records the total amount that the RSS is decreased due
  to splits for each predictor (there may be many splits for one
  predictor for each tree).
\item
  This decrease in RSS is then averaged over the \(B\) trees.
\end{itemize}

\vspace{2mm}

\textbf{Classification trees:}

\begin{itemize}
\tightlist
\item
  The importance of each predictor is calculated using the Gini index.
\item
  The importance is the mean decrease (over all \(B\) trees) in the Gini
  index by splits of a predictor.
\end{itemize}

\vspace{3mm}

\textbf{R-hint}: \texttt{varImpPlot()} function (or \texttt{importance})
in \texttt{randomForest} with \texttt{type=2}.

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Variable importance based on randomization}

\vspace{2mm}

Variable importance based on randomization is calculated using the OOB
sample. \vspace{1mm}

\begin{itemize}
\tightlist
\item
  Computations are carried out for one bootstrap sample at a time.
\item
  Each time a tree is grown the OOB sample is used to test the
  predictive power of the tree.
\item
  Then for one predictor at a time, repeat the following:

  \begin{itemize}
  \tightlist
  \item
    permute the OOB observations for the \(j\)th variable \(x_j\) and
    calculate the new OOB error.
  \item
    If \(x_j\) is important, permuting its observations will decrease
    the predictive performance.
  \end{itemize}
\item
  The difference between the two is averaged over all trees (and
  normalized by the standard deviation of the differences).
\end{itemize}

\vspace{4mm}

\textbf{R-hint}: \texttt{varImpPlot()} (or \texttt{importance()})
function with \texttt{type=1}.

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Example 1: Auto data (regression tree)}

\vspace{1mm}

The data relates fuel consumption (\texttt{mpg}) to 10 aspects of
automobile desing and performance.

\vspace{2mm}

\(~\)

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(mtcars)}
\NormalTok{mtcars.rf <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(mpg }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ mtcars, }\DataTypeTok{ntree =} \DecValTok{1000}\NormalTok{, }\DataTypeTok{keep.forest =} \OtherTok{FALSE}\NormalTok{, }
    \DataTypeTok{mtry =} \DecValTok{10}\NormalTok{, }\DataTypeTok{importance =} \OtherTok{TRUE}\NormalTok{)}
\KeywordTok{varImpPlot}\NormalTok{(mtcars.rf, }\DataTypeTok{pch =} \DecValTok{20}\NormalTok{, }\DataTypeTok{main =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{8Trees_files/figure-beamer/autoImp-1} \end{center}

(Randomization: left, node purity: right)

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Example 2: Head injury (classification tree)}

\vspace{1mm}

\(~\)

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{varImpPlot}\NormalTok{(r.brain.bag, }\DataTypeTok{pch =} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{8Trees_files/figure-beamer/brain-1} \end{center}

(Randomization: left, node purity: right)

\end{block}

\end{frame}

\begin{frame}[fragile]{Random Forests}
\protect\hypertarget{random-forests}{}

\(~\)

\begin{itemize}
\tightlist
\item
  If there is a strong predictor in the dataset, the decision trees
  produced by each of the bootstrap samples in the bagging algorithm
  becomes very similar: Most of the trees will use the same strong
  predictor in the top split.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  We have seen this for our example trees for the minor head injury
  example, the predictor \texttt{GCS.15} was chosen in the top split
  every time. This is probably the case for a large amount of the bagged
  trees as well.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  Not optimal, as we get \(B\) trees that are highly correlated.
  \(\rightarrow\) No large reduction in variance by averaging
  \(\hat{f}^{*b}(x)\) when the correlation between the trees is high.
\end{itemize}

\end{frame}

\begin{frame}

\begin{block}{The effect of correlation on the variance of the mean}

\vspace{3mm}

\begin{itemize}
\item
  The variance of the average of \(B\) observations of \emph{i.i.d.}
  random variables \(X\), each with variance \(\sigma^2\) is
  \(\frac{\sigma^2}{B}\).
\item
  But if we have \(B\) \emph{i.i.d.} observations of a random variable
  \(X\) with a positive correlation \(\rho\) such that
  \(\text{Cov}(X_i, X_j) = \rho \sigma^2, \quad i \neq j,\) then
\end{itemize}

\vspace{-3mm}

\[ \begin{aligned} \text{Var}(\bar{X}) &= \text{Var}\Big( \frac{1}{B}\sum_{i=1}^B X_i \Big) =\\
\end{aligned}\]

\vspace{35mm}

\vspace{1mm}

Check: \(\rho=0\) and \(\rho=1\)?

\end{block}

\end{frame}

\begin{frame}

\begin{itemize}
\item
  \emph{Random forests} provide an improvement over bagged trees by a
  small tweak that \emph{\textcolor{red}{decorrelates the trees}}. This
  reduces the variance when we average the trees.
\item
  As in bagging, we build a number of decision trees on bootstrapped
  training samples.
\item
  But each time a split in a tree is considered, a \emph{random
  selection} of \(m\) predictors is chosen as split candidates from the
  full set of \(p\) predictors. The split is allowed to use only one of
  those m predictors.
\item
  A fresh selection of \(m\) predictors is taken at each split,
  typically

  \begin{itemize}
  \tightlist
  \item
    \(m\approx \sqrt p\) (classification),\\
  \item
    \(m=p/3\) (regression).
  \end{itemize}
\end{itemize}

\vspace{2mm}

The general idea is that for very correlated predictors \(m\) is chosen
to be small.

\end{frame}

\begin{frame}

\begin{block}{How many trees?}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  The number of trees, \(B\), is not a tuning parameter, and the best is
  to choose it large enough (as large as ``necessary''). An increase in
  \(B\) will not lead to overfitting.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  Increasing \(B\) will not change the goodness of fit measure.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  To find out which number \(B\) is sufficient, we do \emph{not} need to
  run cross-validation, but can again use the OOB error (works best for
  bagging and random forests).
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

ISLR Figure 8.10, gene expression data set with 15 classes and 500
predictors: \vspace{-3mm}

\centering

\includegraphics[width=0.8\textwidth,height=\textheight]{../../ISLR/Figures/Chapter8/8.10.png}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Example}

\vspace{2mm}

We decorrelate the brain injury classification trees by using the
\texttt{randomForest()} function again, but this time we set
\texttt{mtry=3} (instead for \texttt{mtry=10}). This means that the
algorithm only considers three of the predictors in each split. We
choose \(3\) because we have \(10\) predictors in total and
\(\sqrt{10}\approx 3\).

\vspace{2mm}

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\NormalTok{r.brain.rf =}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(brain.injury }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ headInjury2, }\DataTypeTok{subset =}\NormalTok{ train, }
    \DataTypeTok{mtry =} \DecValTok{3}\NormalTok{, }\DataTypeTok{ntree =} \DecValTok{500}\NormalTok{, }\DataTypeTok{importance =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{block}

\end{frame}

\begin{frame}[fragile]

We check the predictive performance as before, using the test set:

\vspace{2mm}

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{yhat.rf =}\StringTok{ }\KeywordTok{predict}\NormalTok{(r.brain.rf, }\DataTypeTok{newdata =}\NormalTok{ headInjury2[test, ])}

\NormalTok{misclass.rf =}\StringTok{ }\KeywordTok{table}\NormalTok{(yhat.rf, headInjury2[test, ]}\OperatorTok{$}\NormalTok{brain.injury)}
\KeywordTok{print}\NormalTok{(misclass.rf)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        
## yhat.rf   0   1
##       0 363  47
##       1  17  44
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \OperatorTok{-}\StringTok{ }\KeywordTok{sum}\NormalTok{(}\KeywordTok{diag}\NormalTok{(misclass.rf))}\OperatorTok{/}\NormalTok{(}\KeywordTok{sum}\NormalTok{(misclass.rf))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1358811
\end{verbatim}

\normalsize

The misclassification rate is slightly decreased compared to the bagged
tree (and to the pruned tree).

\end{frame}

\begin{frame}[fragile]

By using the \texttt{varImpPlot()} function we can study the importance
of each predictor.

\(~\)

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{varImpPlot}\NormalTok{(r.brain.rf, }\DataTypeTok{pch =} \DecValTok{20}\NormalTok{, }\DataTypeTok{main =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{8Trees_files/figure-beamer/brainImp-1} \end{center}

\normalsize

\(~\)

As expected \texttt{GCS.15} (GCS=15 at 2h) is a strong predictor along
with \texttt{bskullf} (basal skull facture) and \texttt{age}. This means
that most of the trees will have these predictors in the top split.

\end{frame}

\begin{frame}{Boosting}
\protect\hypertarget{boosting}{}

\emph{Boosting} is an alternative approach for improving the predictions
resulting from a decision tree. We will only consider the description of
boosting regression trees (and not classification trees) in this course.

The trees are grown \emph{sequentially} so that each tree is grown using
information from the previous tree.

\begin{enumerate}
\tightlist
\item
  Build a decision tree with \(d\) splits (and \(d+1\) terminal notes).
\item
  Improve the model in areas where the model didn't perform well. This
  is done by fitting a decision tree to the \emph{residuals of the
  model}. This procedure is called \emph{learning slowly}.
\item
  The first decision tree is then updated based on the residual tree,
  but with a weight.
\end{enumerate}

The procedure is repeated until some stopping criterion is reached. Each
of the trees can be very small, with just a few terminal nodes (or just
one split).

\end{frame}

\begin{frame}

\textbf{Algorithm 8.2: Boosting for regression trees}

\begin{enumerate}
\tightlist
\item
  Set \(\hat{f}(x) = 0\) and \(r_i = y_i\) for all \(i\) in the training
  set.
\item
  For \(b=1,2,...,B\), repeat:

  \begin{enumerate}
  [a)]
  \tightlist
  \item
    Fit a tree \(\hat{f}^b\) with \(d\) splits (\(d+1\) terminal nodes)
    to the training data.\\
  \item
    Update \(\hat{f}\) by adding in a shrunken version of the new tree:
    \[\hat{f}(x) \leftarrow \hat{f}(x)+\lambda \hat{f}^b(x).\]\\
  \item
    Update the residuals,
    \[r_i \leftarrow r_i - \lambda \hat{f}^b(x_i).\]\\
  \end{enumerate}
\item
  The boosted model is
  \(\hat{f}(x) = \sum_{b=1}^B \lambda \hat{f}^b(x).\)
\end{enumerate}

Boosting has three tuning parameters which need to be set (\(B\),
\(\lambda\), \(d\)), and can be found using cross-validation.

\end{frame}

\begin{frame}

\textbf{Tuning parameters}

\begin{itemize}
\item
  \textbf{Number of trees \(B\)}. Could be chosen using
  cross-validation. A too small value of \(B\) would imply that much
  information is unused (remember that boosting is a slow learner),
  whereas a too large value of \(B\) may lead to overfitting.
\item
  \textbf{Shrinkage parameter \(\lambda\)}. Controls the rate at which
  boosting learns. \(\lambda\) scales the new information from the
  \(b\)-th tree, when added to the existing tree \(\hat{f}\). Choosing a
  small value for \(\lambda\) ensures that the algorithm learns slowly,
  but will require a larger \(B\). Typical values of \(\lambda\) is 0.1
  or 0.01.
\item
  \textbf{Interaction depth \(d\)}: The number of splits in each tree.
  This parameter controls the complexity of the boosted tree ensemble
  (the level of interaction between variables that we may estimate). By
  choosing \(d=1\) a tree stump will be fitted at each step and this
  gives an additive model.
\end{itemize}

\end{frame}

\begin{frame}

\begin{block}{Short quizz poll}

www.menti.com, Code 3776 8997

\end{block}

\end{frame}

\begin{frame}[fragile]{Example: Boston data set}
\protect\hypertarget{example-boston-data-set}{}

\tiny

(ISLR book, Sections 8.3.2 to 8.3.4.)

\normalsize

Remember the data set: The aim is to predict the median value of
owner-occupied homes (in 1000\$)

We first run through trees, bagging and random forests - before arriving
at boosting.

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(MASS)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{train =}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(Boston), }\KeywordTok{nrow}\NormalTok{(Boston)}\OperatorTok{/}\DecValTok{2}\NormalTok{)}
\KeywordTok{head}\NormalTok{(Boston)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      crim zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat
## 1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98
## 2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14
## 3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03
## 4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94
## 5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33
## 6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21
##   medv
## 1 24.0
## 2 21.6
## 3 34.7
## 4 33.4
## 5 36.2
## 6 28.7
\end{verbatim}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Regression tree}

\(~\)

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tree.boston =}\StringTok{ }\KeywordTok{tree}\NormalTok{(medv }\OperatorTok{~}\StringTok{ }\NormalTok{., Boston, }\DataTypeTok{subset =}\NormalTok{ train, }\DataTypeTok{control =} \KeywordTok{tree.control}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(Boston), }
    \DataTypeTok{mindev =} \FloatTok{0.005}\NormalTok{))}
\KeywordTok{summary}\NormalTok{(tree.boston)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Regression tree:
## tree(formula = medv ~ ., data = Boston, subset = train, control = tree.control(nrow(Boston), 
##     mindev = 0.005))
## Variables actually used in tree construction:
## [1] "rm"      "lstat"   "age"     "crim"    "black"   "nox"     "ptratio"
## Number of terminal nodes:  13 
## Residual mean deviance:  7.35 = 1760 / 240 
## Distribution of residuals:
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  -8.140  -1.440  -0.162   0.000   1.460  12.900
\end{verbatim}

\end{block}

\end{frame}

\begin{frame}[fragile]

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(tree.boston)}
\KeywordTok{text}\NormalTok{(tree.boston, }\DataTypeTok{pretty =} \DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{8Trees_files/figure-beamer/boston1-1} \end{center}

\normalsize

Remember:

\begin{itemize}
\tightlist
\item
  The \texttt{tree()} function has a built-in default stopping
  criterion.
\item
  You can change this with the \texttt{control} option, for example by
  setting
  \texttt{control\ =\ tree.control(mincut\ =\ 2,\ minsize\ =\ 4,\ mindev\ =\ 0.001)}.
  Here we used \texttt{mindev=0.005}.
\end{itemize}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Need to prune?}

\(~\)

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cv.boston =}\StringTok{ }\KeywordTok{cv.tree}\NormalTok{(tree.boston)}
\KeywordTok{plot}\NormalTok{(cv.boston}\OperatorTok{$}\NormalTok{size, cv.boston}\OperatorTok{$}\NormalTok{dev, }\DataTypeTok{type =} \StringTok{"b"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{8Trees_files/figure-beamer/boston2-1} \end{center}

\normalsize

\vspace{2mm}

It looks like a tree with 6 leaves would work well.

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Pruning}

\vspace{1mm}

So we are pruning to a 6-node tree here:

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prune.boston =}\StringTok{ }\KeywordTok{prune.tree}\NormalTok{(tree.boston, }\DataTypeTok{best =} \DecValTok{6}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(prune.boston)}
\KeywordTok{text}\NormalTok{(prune.boston, }\DataTypeTok{pretty =} \DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{8Trees_files/figure-beamer/boston3-1} \end{center}

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Test error for full tree}

\(~\)

We calculate the test error for the pruned tree:

\vspace{2mm}

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{yhat =}\StringTok{ }\KeywordTok{predict}\NormalTok{(prune.boston, }\DataTypeTok{newdata =}\NormalTok{ Boston[}\OperatorTok{-}\NormalTok{train, ])}
\NormalTok{boston.test =}\StringTok{ }\NormalTok{Boston[}\OperatorTok{-}\NormalTok{train, }\StringTok{"medv"}\NormalTok{]}
\KeywordTok{plot}\NormalTok{(yhat, boston.test, }\DataTypeTok{pch =} \DecValTok{20}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.5\linewidth]{8Trees_files/figure-beamer/boston4-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{((yhat }\OperatorTok{-}\StringTok{ }\NormalTok{boston.test)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 35.1644
\end{verbatim}

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Bagging}

\(~\)

\small

Remember: For bagging you can use the \texttt{randomForest()} function,
but include all variables (here \texttt{mtry=13}).

\vspace{2mm}

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(randomForest)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{bag.boston =}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(medv }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ Boston, }\DataTypeTok{subset =}\NormalTok{ train, }\DataTypeTok{mtry =} \DecValTok{13}\NormalTok{, }
    \DataTypeTok{importance =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{bag.boston}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
##  randomForest(formula = medv ~ ., data = Boston, mtry = 13, importance = TRUE,      subset = train) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 13
## 
##           Mean of squared residuals: 11.396
##                     % Var explained: 85.17
\end{verbatim}

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Test error for bagged tree}

\(~\)

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{yhat.bag =}\StringTok{ }\KeywordTok{predict}\NormalTok{(bag.boston, }\DataTypeTok{newdata =}\NormalTok{ Boston[}\OperatorTok{-}\NormalTok{train, ])}
\KeywordTok{plot}\NormalTok{(yhat.bag, boston.test, }\DataTypeTok{pch =} \DecValTok{20}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.5\linewidth]{8Trees_files/figure-beamer/boston5-1} \end{center}

\vspace{2mm}

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bag.boston =}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(medv }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ Boston, }\DataTypeTok{subset =}\NormalTok{ train, }\DataTypeTok{mtry =} \DecValTok{13}\NormalTok{, }
    \DataTypeTok{ntree =} \DecValTok{25}\NormalTok{)}
\NormalTok{yhat.bag =}\StringTok{ }\KeywordTok{predict}\NormalTok{(bag.boston, }\DataTypeTok{newdata =}\NormalTok{ Boston[}\OperatorTok{-}\NormalTok{train, ])}
\KeywordTok{mean}\NormalTok{((yhat.bag }\OperatorTok{-}\StringTok{ }\NormalTok{boston.test)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 23.6672
\end{verbatim}

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Random forest}

\(~\)

Let's go from bagging to a random
forest\footnote{n.b., why are we now speaking of a forest and no longer of a tree?},
using 6 randomly selected predictors for each tree:

\vspace{2mm}

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{rf.boston =}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(medv }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ Boston, }\DataTypeTok{subset =}\NormalTok{ train, }\DataTypeTok{mtry =} \DecValTok{6}\NormalTok{, }
    \DataTypeTok{importance =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{yhat.rf =}\StringTok{ }\KeywordTok{predict}\NormalTok{(rf.boston, }\DataTypeTok{newdata =}\NormalTok{ Boston[}\OperatorTok{-}\NormalTok{train, ])}
\KeywordTok{mean}\NormalTok{((yhat.rf }\OperatorTok{-}\StringTok{ }\NormalTok{boston.test)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 19.6202
\end{verbatim}

\vspace{2mm}
\normalsize

It's interesting to see how the prediction error further decreased with
respect to simple bagging.

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Variable importance}

\(~\)

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{importance}\NormalTok{(rf.boston)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          %IncMSE IncNodePurity
## crim    16.69702     1076.0879
## zn       3.62578       88.3534
## indus    4.96862      609.5336
## chas     1.06143       52.2179
## nox     13.51818      709.8734
## rm      32.34330     7857.6545
## age     13.27250      612.2142
## dis      9.03248      714.9467
## rad      2.87843       95.8060
## tax      9.11880      364.9248
## ptratio  8.46706      823.9334
## black    7.57948      275.6227
## lstat   27.12982     6027.6374
\end{verbatim}

\normalsize

Interpretation?

\end{block}

\end{frame}

\begin{frame}[fragile]

And the variable importance plots

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{varImpPlot}\NormalTok{(rf.boston, }\DataTypeTok{main =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{8Trees_files/figure-beamer/boston6-1} \end{center}

\normalsize

To understand what this means, please check again the meaning of the
variables by typing \texttt{?Boston}.

\end{frame}

\begin{frame}[fragile]

\begin{block}{Boosting}

\vspace{2mm}

And finally, we are boosing the Boston trees! We boost with 5000 trees
and allow the interaction depth (number of splits per tree) to be of
degree 4:

\vspace{2mm}

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(gbm)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{boost.boston =}\StringTok{ }\KeywordTok{gbm}\NormalTok{(medv }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ Boston[train, ], }\DataTypeTok{distribution =} \StringTok{"gaussian"}\NormalTok{, }
    \DataTypeTok{n.trees =} \DecValTok{5000}\NormalTok{, }\DataTypeTok{interaction.depth =} \DecValTok{4}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(boost.boston, }\DataTypeTok{plotit =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             var   rel.inf
## rm           rm 43.991933
## lstat     lstat 33.121694
## crim       crim  4.260417
## dis         dis  4.011109
## nox         nox  3.435302
## black     black  2.826755
## age         age  2.611394
## ptratio ptratio  2.540303
## tax         tax  1.456565
## indus     indus  0.800874
## rad         rad  0.654640
## zn           zn  0.144615
## chas       chas  0.144399
\end{verbatim}

\end{block}

\end{frame}

\begin{frame}[fragile]

\textbf{Partial dependency plots - integrating out other variables }

\small

\texttt{rm} (number of rooms) and \texttt{lstat} (\% of lower status
population) are the most important predictors. Partial dependency plots
show the effect of individual predictors, integrated over the other
predictors see Hastie, Tibshirani, and Friedman (2009), Section 10.13.2.

\vspace{2mm}

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(boost.boston, }\DataTypeTok{i =} \StringTok{"rm"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"medv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.4\linewidth]{8Trees_files/figure-beamer/boston7-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(boost.boston, }\DataTypeTok{i =} \StringTok{"lstat"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"medv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.4\linewidth]{8Trees_files/figure-beamer/boston7-2} \end{center}

\end{frame}

\begin{frame}[fragile]

\textbf{Prediction on test set}

\begin{itemize}
\item
  Calculate the MSE on the test set, first for the model with
  \(\lambda=0.001\) (default), then with \(\lambda=0.2\).
\item
  We could have done cross-validation to find the best \(\lambda\) over
  a grid, but it seems not to make a big difference.
\end{itemize}

\(~\)

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{yhat.boost =}\StringTok{ }\KeywordTok{predict}\NormalTok{(boost.boston, }\DataTypeTok{newdata =}\NormalTok{ Boston[}\OperatorTok{-}\NormalTok{train, ], }\DataTypeTok{n.trees =} \DecValTok{5000}\NormalTok{)}
\KeywordTok{mean}\NormalTok{((yhat.boost }\OperatorTok{-}\StringTok{ }\NormalTok{boston.test)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 18.8471
\end{verbatim}

\vspace{2mm}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boost.boston =}\StringTok{ }\KeywordTok{gbm}\NormalTok{(medv }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ Boston[train, ], }\DataTypeTok{distribution =} \StringTok{"gaussian"}\NormalTok{, }
    \DataTypeTok{n.trees =} \DecValTok{5000}\NormalTok{, }\DataTypeTok{interaction.depth =} \DecValTok{4}\NormalTok{, }\DataTypeTok{shrinkage =} \FloatTok{0.2}\NormalTok{, }\DataTypeTok{verbose =}\NormalTok{ F)}
\NormalTok{yhat.boost =}\StringTok{ }\KeywordTok{predict}\NormalTok{(boost.boston, }\DataTypeTok{newdata =}\NormalTok{ Boston[}\OperatorTok{-}\NormalTok{train, ], }\DataTypeTok{n.trees =} \DecValTok{5000}\NormalTok{)}
\KeywordTok{mean}\NormalTok{((yhat.boost }\OperatorTok{-}\StringTok{ }\NormalTok{boston.test)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 18.3345
\end{verbatim}

\end{frame}

\begin{frame}[fragile]

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(yhat.boost, boston.test, }\DataTypeTok{pch =} \DecValTok{20}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{8Trees_files/figure-beamer/boston8-1} \end{center}

\end{frame}

\begin{frame}{Further reading}
\protect\hypertarget{further-reading}{}

\begin{itemize}
\item
  \href{https://www.youtube.com/playlist?list=PL5-da3qGB5IB23TLuA8ZgVGC8hV8ZAdGh}{Videoes
  on YouTube by the authors of ISL, Chapter 8}, and corresponding
  \href{https://lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/trees.pdf}{slides}.
\item
  \href{https://rstudio-pubs-static.s3.amazonaws.com/65564_925dfde884e14ef9b5735eddd16c263e.html}{Solutions
  to exercises in the book, chapter 8}
\end{itemize}

\end{frame}

\begin{frame}{References}
\protect\hypertarget{references}{}

\tiny

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-Breiman1996}{}%
Breiman, Leo. 1996. ``Bagging Predictors.'' \emph{Machine Learning} 24:
123--40.

\leavevmode\hypertarget{ref-hastie_etal2009}{}%
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. \emph{The
Elements of Statistical Learning}. Vol. 2. Springer series in statistics
New York.

\leavevmode\hypertarget{ref-james.etal}{}%
James, G., D. Witten, T. Hastie, and R. Tibshirani. 2013. \emph{An
Introduction to Statistical Learning with Applications in R}. New York:
Springer.

\leavevmode\hypertarget{ref-tree2019}{}%
Ripley, Brian. 2019. \emph{Tree: Classification and Regression Trees}.
\url{https://CRAN.R-project.org/package=tree}.

\leavevmode\hypertarget{ref-Ripley}{}%
Ripley, Brian D. 1996. \emph{Pattern Recognicion and Neural Networks}.
Cambridge University Press.

\end{frame}

\end{document}
