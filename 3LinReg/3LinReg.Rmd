---
subtitle: "TMA4268 Statistical Learning V2020"
title: "Module 3: Linear Regression"
author: "Stefanie Muff, Department of Mathematical Sciences, NTNU"
date: "January 17 and 20, 2020"
fontsize: 10pt
output:
  # beamer_presentation:
  #   keep_tex: yes
  #   fig_caption: false
  #   latex_engine: xelatex
  #   theme: "Singapore"
  #   colortheme: "default"
  #   font: "serif"
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
urlcolor: blue
bibliography: refs.bib
#header-includes: \usepackage{xcolor}

---

```{r setup, include=FALSE}
showsolA<-TRUE
showsolB<-TRUE
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=68),tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize")

```

---

Last update: January 19, 2020

---

# Acknowledgements

* A lot of this material stems from Mette Langaas and her TAs (especiall Julia Debik). I would like to thank Mette for the permission to use her material!

* Some of the figures and slides in this presentation are taken (or are inspired) from @ISL.

--- 

# Introduction

## Learning material for this module

* @ISL: An Introduction to Statistical Learning, Chapter 3 (skip 3.5).  

We need more statistical theory than is presented in the textbook, which you find in this module page.

<!-- Remark: there are no classnotes for this module since I wrote on the slides in class. Screencast from the lectures are available under Course Modules (column with "Dates") on Blackboard. -->

---

## What will you learn?

* Simple linear regression: 
    + Model and assumptions 
    + Least squares
    + Testing and confidence intervals

* Multiple linear regression:
    + The use of matrix algebra 
    + Distribution of estimators
    + Assessing model fit, model selection
    + Confidence and prediction ranges
    
* Assessing model fit / residual analysis
* Qualitative predictors
* Interactions
    
 
---

# Linear regression

* Very simple approach for _supervised learning_.

* Parametric.

* Quantitative response vs. one or several explanatory variables.

* Aims:
    + **Prediction** - "black box"
    + **Explanation** - understanding the relationship between _explanatory variables_ and the response

* Is linear regression too simple? Maybe, but very useful. Important to _understand_ because many learning methods can be seen as generalization of linear regression.


---


## Motivating example: Prognostic factors for body fat
\tiny(From Theo Gasser \& Burkhardt Seifert _Grundbegriffe der Biostatistik_)

\vspace{2mm}
\normalsize
Body fat is an important indicator for overweight, but difficult to measure. 

\vspace{2mm}
**Question:**  Which factors allow for precise estimation (prediction) of body fat? 


\vspace{2mm}
Study with 243 male participants, where body fat (%) and BMI and other predictors were measured. Some scatterplots\footnote{The data to reproduce these plots and analyses can be found here: https://github.com/stefaniemuff/statlearning/tree/master/3LinReg/data}:


```{r motivating, echo=FALSE, fig.width=8, fig.height=3,fig.align = "center",out.width='100%'}
library(dplyr)
library(ggplot2)
path <- "data/"
d.bodyfat <- read.table(paste(path,"bodyfat.clean.txt",sep=""),header=T)
d.bodyfat <- d.bodyfat[,c("bodyfat","age","gewicht","hoehe","bmi","neck","abdomen","hip")]
names(d.bodyfat) <- c("bodyfat","age","weight","height","bmi","neck","abdomen","hip")

par(mfrow=c(1,4))
plot(bodyfat ~ bmi,d.bodyfat,xlab="bmi", ylab="bodyfat (y)")
plot(bodyfat ~ age,d.bodyfat,xlab="age", ylab="bodyfat (y)")
plot(bodyfat ~ neck,d.bodyfat,xlab="neck", ylab="bodyfat (y)")
plot(bodyfat ~ hip,d.bodyfat,xlab="hip", ylab="bodyfat (y)")
```


---

For a good predictive model we need to dive into _multiple linear regression_. However, wer start with the simple case of _only one predictor variable_:
 


```{r motivating2, echo=FALSE, fig.width=6, fig.height=5,fig.align = "center",out.width='70%'}
ggplot(d.bodyfat,aes(x=bmi,y=bodyfat)) + geom_point() + theme_bw() + ylab("body fat (%)")
```


---

**Interesting questions**

1. How good is BMI as a predictor for body fat? 
2. How strong is this relationship?
3. Is the relationship linear?
4. Are also other variables associated with `bodyfat`? 
5. How well can we predict the bodyfat of a person?

---

# Simple Linear Regression

* One quantitative response $Y$ is modelled

* from _one covariate_ $x$ (=simple), 

* and the relationship between $Y$ and $x$ is assumed to be _linear_. 

\vspace{6mm}
If the relation between $Y$ and $x$ is perfectly linear, all instances of $(x,Y)$, given by $(x_i,y_i)$, $i= 1,\ldots, n$, lie on a  straight line and fulfill 
$$y_i = \beta_0 + \beta_1 x_i\ .$$

---


But which is the "true" or "best" line, if the relationship is not exact?


```{r motivating3, echo=FALSE, fig.width=5, fig.height=4,fig.align = "center",out.width='60%'}
ggplot(d.bodyfat,aes(x=bmi,y=bodyfat)) + geom_point() + theme_bw() + ylab("body fat (%)")  + 
  geom_abline(intercept = -25, slope = 1.7, color="red",   size=0.6) +
  geom_abline(intercept = -35, slope = 2.1, color="green",    size=0.6) +
geom_abline(intercept = -36, slope = 2.25, color="blue",    size=0.6) 
``` 

**Task:** Estimate the intercept and slope parameters (by "eye") and write it down (we will look at the "best" answer later).

---

It is obvious that 
 
* the linear relationship does not describe the data perfectly.
* another realization of the data (other 243 males) would lead to a slightly different picture.

\vspace{4mm}
$\Rightarrow$ We need a  **model** that describes the relationship between BMI and bodyfat.  

---

## The simple linear regression model

\vspace{3mm}

In the linear regression model the dependent variable $Y$ is related to the independent variable $x$ as

$$Y = \beta_0 + \beta_1 x + \varepsilon \ , \qquad \varepsilon \sim N(0,\sigma^2) \ .$$
\vspace{2mm}

In this formulation $Y$ is a random variable $Y \sim N(\beta_0 + \beta_1 x, \sigma^2$) where
$$Y \quad= \quad \underbrace{\text{ expected value }}_{\text{E}(Y) = \beta_0 + \beta_1 x} \quad + \quad \underbrace{\text{ error}}_{\varepsilon}  \ .$$

Note:
 
* The model for $Y$ given $x$ has _\textcolor{red}{three parameters}_: $\beta_0$ (intercept), $\beta_1$ (slope coefficient) and $\sigma^2$ .
* $x$ is the _\textcolor{red}{independent}_/ _\textcolor{red}{explanatory}_ / _\textcolor{red}{regressor}_ variable.
* $Y$ is the _\textcolor{red}{dependent}_ / _\textcolor{red}{outcome}_ / _\textcolor{red}{response}_ variable.


---

## Modeling assumptions

\vspace{4mm}
The central assumption in linear regression is that for any pairs ($x_i,Y_i$), the error $\varepsilon_i \sim N(0,\sigma^2)$. This implies
\vspace{2mm}

a)  The expected value of $\varepsilon_i$ is 0: $\text{E}(\varepsilon_i)=0$.

b) All $\varepsilon_i$ have the same variance: $\text{Var}(\varepsilon_i)=\sigma^2$.  

c) All $\varepsilon_i$ are normally distributed. 

d) $\varepsilon$ is independent of any variable, observation number etc.

e) $\varepsilon_1, \varepsilon_2, \ldots, \varepsilon_n$ are independent of each other.

 
---


## Visualization of the regression assumptions 
 
The assumptions about the linear regression model lie in the error term $$\varepsilon \sim N(0,\sigma^2) \ . $$ 

\vspace{-2mm}
\includegraphics[width=11cm]{pictures/regrAssumptions.jpg}

Note: The true regression line goes through $\text{E}(Y)$.


---

## Parameter estimation ("model fitting")
\vspace{2mm}

In a regression analysis, the task is to estimate the **regression coefficients** $\beta_0$, $\beta_1$ and the **residual variance** $\sigma^2$ for a given set of $(x,y)$ data.


\vspace{4mm}

* **Problem:** For more than two points $(x_i,y_i)$, $i=1,\ldots, n$, there is generally no perfectly fitting line.
\vspace{2mm}

* **Aim**: We want to find the parameters $(a,b)$ of the best fitting line $Y = a + b x$. 

\vspace{2mm}

* **Idea:** Minimize the deviations between the data points $(x_i,y_i)$ and the regression line. 

\vspace{4mm}

But what are we actually going to minimize?

---

## Least squares
\vspace{2mm}

Remember the **Least Squared Method**. Graphically, we are minimizing the sum of the squared distances over all points:

```{r squares, echo=FALSE, fig.width=6.1, fig.height=5,fig.align = "center",out.width='80%'}
set.seed(9670)
n <- 10
x <- rnorm(n)
y <- 4 - 2*x + rnorm(n,0,sd=1)
x[11]  <- -0.5
y[11] <- 6.2
dd <- 0.38
from_x <- c(x[11],x[11],x[11]+dd,x[11] + dd) 
from_y <- c(y[11],(4-2*x[11]),(4-2*x[11]),y[11])

to_x <- c(x[11],x[11] + dd,x[11]+ dd,x[11])
to_y <- c(4-2*x[11],4-2*x[11],y[11], y[11])

plot(x,y)
abline(c(4,-2),lwd=2)
polygon(from_x,from_y,to_x,to_y,col=2,lwd=2)
```

---

* Mathematically, $a$ and $b$ are estimated such that the sum of _\textcolor{red}{squared vertical distances}_ (residual sum of squares)

$$\text{RSS} = \sum_{i=1}^n e_i^2 \ , \qquad \text{where} \quad e_i = y_i - (a + b x_i) $$

is being minimized.

* The respective "best" estimates are called $\hat{\beta_0}$ and $\hat{\beta_1}$.

* We can predict the value of the response for a (new) observation of the covariate at $x$.
$$\hat{y} = \hat{\beta}_0 + \hat{\beta_1}x.$$

* The $i$-th *residual* of the model is the difference between the $i$-th *observed* response value and the $i$-th *predicted* value, and is written as: 
$$e_i = Y_i - \hat{y}_i.$$

* We may regard the residuals as _predictions_ (not estimates) of the error terms $\varepsilon_i$.

\tiny
(The error terms are random variables and can not be estimated - they can be predicted. It is only for parameters that we speak about estimates.)


---


## Least squares estimators:
\vspace{2mm}

Using $n$ observed independent data points 
$$(x_1,y_1), (x_2,y_2), \ldots, (x_n,y_n)\ ,$$ 

the least squares estiamtes for simple linear regression are given as

\begin{equation}\label{eq:beta0}
\hat{\beta}_0 = \bar{y}-\hat{\beta}_1 \bar{x}
\end{equation}
and
\begin{equation}\label{eq:beta1}
\hat{\beta}_1 = \frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2} = \frac{Cov(\boldsymbol{x},\boldsymbol{y})}{Var(\boldsymbol{x})}\ ,
\end{equation}

\vspace{2mm}
where 
$\bar{y} = \frac{1}{n} \sum_{i=1}^n y_i$ and $\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$ are the sample means.

\vspace{6mm}
\scriptsize
This is something you should have proven in your previous statistics classes; if you forgot how to get there, please check again, e.g. in chapter 11 of the book by @walepole.etal, see [here](https://github.com/stefaniemuff/statlearning/blob/master/literature/Walepole_book.pdf).

---

## Do-it-yourself "by hand"

\vspace{6mm}

Go to the Shiny gallery and try to ``estimate'' the correct parameters.
\vspace{2mm}

You can do this here:
\vspace{2mm}

<https://gallery.shinyapps.io/simple_regression/>


---

### Example continued: Body fat
\vspace{2mm}

Assume a linear relationship between the % bodyfat (`bodyfat`) and the BMI (`bmi`), we can get the LS estimates using R as follows: 

\scriptsize
```{r,eval=TRUE,echo=TRUE,size="tiny"}
r.bodyfat = lm(bodyfat~bmi, data=d.bodyfat)
```

\normalsize
The estimates (and more information) can be obtained as follows:

\scriptsize
```{r,eval=TRUE,echo=TRUE,size="tiny"}
summary(r.bodyfat)$coef
```


---

We see that the model fits the data quite well. It captures the essence. It looks that a linear relationship between `bodyfat` and `bmi` is a good approximation. 

```{r, echo=FALSE, fig.width=5, fig.height=4,fig.align = "center",out.width='80%'}
ggplot(d.bodyfat,aes(bmi,bodyfat)) + geom_point() + geom_smooth(method='lm',se=F) + theme_bw()
```

---

**Questions:** 

* The blue line gives the estimated model. Explain what the line means in practice. Is this result plausible?

* Compare the estimates for $\beta_0$ and $\beta_1$ to the estimates you gave at the beginning - were you close?  

* How does this relate to the _true_ (population) model? 

* By looking at the spread of the points around the line, can you detect any violations of the modelling assumptions?

* Finally: **What could the regression line look like if another set of 243 males were used for estimation?**


---

### Uncertainty in the estimates $\hat\beta_0$ and $\hat\beta_1$
\vspace{2mm}

Note: $\hat\beta_0$ and $\hat\beta_1$ are themselves _\textcolor{red}{random variables}_ and as such contain _\textcolor{red}{uncertainty}_!

\vspace{4mm}

Let us look again at the regression output, this time only for the coefficients. The second column shows the standard error of the estimate:
\vspace{2mm}

\scriptsize
```{r lmbodyfat.uncertainty,echo=T,eval=T}
summary(r.bodyfat)$coef
```

\normalsize
$~$
$\rightarrow$ The logical next question is: what is the distribution of the estimates?

---


## Distribution of the estimators for $\hat\beta_0$ and $\hat\beta_1$
 
\vspace{2mm}
To obtain an intuition, we generate data points according to model

$$y_i = 4 - 2x_i + \varepsilon_i \ , \quad \varepsilon_i\sim N(0,0.5^2). $$
In each round, we estimate the parameters and store them:
\tiny
```{r simulation,echo=T}
set.seed(1)
niter <- 1000
pars <- matrix(NA,nrow=niter,ncol=2)
for (ii in 1:niter){
  x <- rnorm(100)
  y <- 4 - 2*x + rnorm(100,0,sd=0.5)
  pars[ii,] <- lm(y~x)$coef
}
```
 
 \normalsize
Doing it 1000 times, we obtain the following distributions for $\hat\beta_0$ and $\hat\beta_1$:

```{r sim_fig, echo=FALSE, fig.width=6, fig.height=3,fig.align = "center",out.width='40%'}
library(cowplot)
pars <- data.frame(pars)
names(pars) <- c("beta0","beta1")
 
p1 <-  ggplot(pars,aes(x=beta0)) + geom_histogram() +  theme_bw()
p2 <-  ggplot(pars,aes(x=beta1)) + geom_histogram() +  theme_bw()
p <- plot_grid(p1,p2,  ncol = 2, rel_heights = c(1, .2))
p
```

---

## Accuracy of the parameter estimates

\vspace{2mm}



* The standard errors of the estimates are given by the following formulas:
$$\text{Var}(\hat{\beta}_0)=\text{SE}(\hat{\beta}_0)^2 = \sigma^2 \Big [ \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n (x_i -\bar{x})^2} \Big]$$
and
$$\text{Var}(\hat{\beta}_1)=\text{SE}(\hat{\beta}_1)^2 = \frac{\sigma^2}{\sum_{i=1}^n (x_i-\bar{x})^2}.$$

* $\text{Cov}(\hat{\beta_0},\hat{\beta_1})$ is in general different from zero.

$~$
$~$


**Note**: We will _derive a general version_ of these formulas for multiple linear regression, because without matrix notation this is very cumbersome.

---

Under the assumption that $\varepsilon \sim N(0,\sigma^2)$, and assuming $\hat\beta_0$ and $\hat\beta_1$ are estimated as in formulas (1) and (2), we have in addition that 

$$
 \hat\beta_0 \sim N(\beta_0,{\sigma^{2}_{\beta_0}}) \quad \text{and} \quad \hat\beta_1 \sim N(\beta_1,{\sigma^{2}_{\beta_1}}) \ .
$$

\vspace{6mm}

**Again**: We will derive this in the multiple linear regression version in more generality.


---

## Design issue with data collection

\vspace{2mm}
Recall that

$$\text{SE}(\hat{\beta}_1)^2 = \frac{\sigma^2}{\sum_{i=1}^n (x_i-\bar{x})^2} \ ,$$ 

thus for a given $\sigma^2$, the standard error is only dependent on the _design_ of the $x_i$'s!

* Would we like the $\text{SE}(\hat{\beta}_1)^2$ large or small? Why?
* If it is possible for us to choose the $x_i$'s, which strategy should we use to choose them? 
* Assume $x$ can take values from 1 to 10 and we choose $n=10$ values. Which is the best design?
    + evenly in a grid: $[1,2,3,4,5,6,7,8,9,10]$.
    + only lower and upper value: $[1,1,1,1,1,10,10,10,10,10]$.
    + randomly drawn from a uniform distribution on $[1,10]$.

---

\scriptsize
```{r,echo=TRUE,eval=TRUE}
x1=seq(1:10)
x2=c(rep(1,5),rep(10,5))
x3=runif(10,1,10)

ss1 = sum((x1-mean(x1))^2)
ss2 = sum((x2-mean(x2))^2)
ss3 = sum((x3-mean(x3))^2)

print(c(ss1,ss2,ss3))
```

\normalsize
$\rightarrow$ The second design - all observations at extremes - is best!

---

## Residual standard error (RSE)
\vspace{2mm}

* **Problem**: $\sigma$ is usually not known, but needs to be estimated\footnote{$\sigma^2$ is the \emph{irreducible error} variance.}.
* Remember: The residual sum of squares is $\text{RSS}=\sum_{i=1}^n (y_i-\hat{\beta}_0-\hat{\beta_1}x_{i})^2$.

* An estimate of $\sigma$, the residual standard error, RSE, is given by
$$\hat\sigma = \text{RSE}  =\sqrt{\frac{1}{n-2} \text{RSS}} = \sqrt{\frac{1}{n-2}\sum_{i=1}^n (y_i -\hat{y}_i)^2} \ .$$


* So actually we have

$$\hat{\text{SE}}(\hat{\beta}_1)^2 = \frac{{\hat\sigma}^2}{\sum_{i=1}^n (x_i-\bar{x})^2} \ ,$$ 
but we usually just write ${\text{SE}}(\hat{\beta}_1)^2$ (without the extra hat).


<!-- --- -->


<!-- If the simple linear regression assumptions are fulfilled, that is, $\varepsilon_i\sim N(0,\sigma^2)$ and all $\varepsilon_i$ independent, then it can be shown that -->

<!-- $$\frac{\text{RSE}^2(n-2)}{\sigma^2}= \frac{\sum_{i=1}^n (y_i -\hat{y}_i)^2}{\sigma^2}\sim \chi^2_{n-2} \ .$$ -->

---

The estimated standard errors can be seen using the `summary()` function:

\scriptsize
```{r}
summary(r.bodyfat)$coef
```


---

To illustrate this point further, again fit the bodyfat example, but each time with only half of the data (randomly selected points each time). See how the model fit varies:

```{r, echo=FALSE, fig.width=5, fig.height=4,fig.align = "center",out.width='80%'}
set.seed(200)

bodyfat.n1 <- nrow(d.bodyfat)

lm1 <- lm(bodyfat~bmi, d.bodyfat[sample(bodyfat.n1,bodyfat.n1/2),])
lm2 <- lm(bodyfat~bmi, d.bodyfat[sample(bodyfat.n1,bodyfat.n1/2),])
lm3 <- lm(bodyfat~bmi, d.bodyfat[sample(bodyfat.n1,bodyfat.n1/2),])

ggplot(d.bodyfat, aes(x=bmi, y=bodyfat))+geom_point(alpha=0.8, size=0.5)+
  geom_line(aes(x=bmi, y=coef(lm1)[1]+bmi*coef(lm1)[2]), 
            col="navy") +
   geom_line(aes(x=bmi, y=coef(lm2)[1]+bmi*coef(lm2)[2]), 
            col="red") +
  geom_line(aes(x=bmi, y=coef(lm3)[1]+bmi*coef(lm3)[2]), 
            col="green") +
  theme_bw()
```

---

# Testing and Confidence Intervals 
After the regression parameters and their uncertainties have been estimated, there are typically two fundamental questions:

\vspace{4mm}

 
1. **"Are the parameters compatible with some specific value?"** 
 Typically, the question is whether the slope $\beta_1$ might be 0 or not, that is: "Is $x$ an informative predictor or not?"
 
 \hspace{4mm} $\rightarrow$ This leads to a **statistical test**. 

\vspace{6mm}

2. "Which values of the parameters are compatible with the data?"

 \hspace{4mm} $\rightarrow$ This leads us to determine  **confidence intervals**.



---

Let's first go back to the output from the bodyfat example:

\scriptsize
```{r,eval=TRUE,echo=TRUE}
summary(r.bodyfat)$coef
```

\normalsize
Besides the estimate and the standard error (which we discussed before), there is a `t value` and a probability `Pr(>|t|` that we need to understand.

How do these things help us to answer the two questions above?

---


## Testing the effect of a covariate 

$~$

Remember: in a statistical test you first need to specify the _null hypothesis_. Here, typically, the null hypothesis is


 
$$H_0: \quad \beta_1 =   0  \ .$$
 
In words: $H_0$ =   ``There is no relationship between $X$ and $Y$.'' 

\vspace{2mm}
 
* Note 1: However, you might want to test against another null hypothesis, like $\beta_1=c$.

* Note 2: Included in $H_0$ is the assumption that the data follow the simple linear regression model!

\vspace{6mm}

Here, the \emph{alternative hypothesis} is given by
$~$


$$H_A: \quad \beta_1 \neq  0  $$


---
 

Remember: To carry out a statistical test, we need a _test statistic_. This is some type of **summary statistic** that follows a known distribution under $H_0$. For our purpose, we use the so-called **$T$-statistic**



\begin{equation*}\label{eq:beta}
T=\frac{\hat\beta_1 - 0}{SE(\hat\beta_1)}\ . 
\end{equation*}



\vspace{4mm}
*Note*: If you want to test against another value than $\beta_1=0$, the formula is

\begin{equation*}
T=\frac{\hat\beta_1 - c}{SE(\hat\beta_1)} \ .
\end{equation*}
 


---

## Distribution of parameter estimators
\vspace{2mm}

We will _derive a general version_ for multiple linear regression!

Brief recap:

$~$

Under $H_0$, $T$ has a $t$-distribution with $n-2$ degrees of freedom ($n=$ number of data points; compare to Chapter 8.6 in @walepole.etal).


---

## Recap: The $t$-distribution

$~$

```{r,echo=FALSE, fig.width=5, fig.height=4,fig.align = "center",out.width='80%'}

xx <- data.frame(seq(-4,4,0.01))
names(xx) <- "xx"
xx$t1 <- dt(xx$xx,1)
xx$t3 <- dt(xx$xx,3)
xx$t10 <- dt(xx$xx,10)
xx$t30 <- dt(xx$xx,30)
xx$norm <- dnorm(xx$xx)

plot(xx$xx,xx$t1,lwd=2,type="l",xlab="x",ylab="density",ylim=c(0,0.4))
lines(xx$xx,xx$t3,add=TRUE,lwd=2,col=2)
lines(xx$xx,xx$t10,add=TRUE,lwd=2,col=3)
lines(xx$xx,xx$t30,add=TRUE,lwd=2,col=4)
lines(xx$xx,xx$norm,add=TRUE,lwd=2,col=5)
legend("topright",legend=c("df=1","df=3","df=10","df=30","normal"),col=1:5,lwd=2)

``` 

\normalsize

* The $t$-distribution has heavier tails than the normal distribution. 
* For df $\geq 30$ the $t$ and Normal distribution are pretty similar.

---

### Hypothesis tests for bodyfat example
\vspace{2mm}

So let's again go back to the bodyfat regression output:


\scriptsize
```{r}
summary(r.bodyfat)$coef
```

\normalsize

**Task**: Use the above formulas to derive the $T$-statistics.

$~$

* The last column contains the *$p$-values* of the tests with $H_0$: $\beta_0=0$ and $\beta_1=0$, respectively.

* The $p$-value for `bmi` is very small ($p<0.0001$). **What does this mean?**

<!-- There **very strong evidence** that the BMI is associated with bodyfat.  -->

<!-- It is very unlikely that such a slope $\hat\beta$ would be seen if there was no effect of BMI on body fat. -->

<!-- * Our test statistic for testing $\beta_1=0$ is  -->

<!-- $$t = \frac{\hat\beta_1}{SE(\hat\beta)} = \frac{1.82}{0.11} = 16.79$$ -->


<!-- We would in a two-sided setting reject $H_0$ for large values of $\text{abs}(t_0)$. We may rely on calculating a $p$-value.  -->

---

## Recap: Formal definition of the $p$-value

$~$

**The $p$-value is** the probability to observe a data summary (e.g., an average) that is at least as extreme as the one observed, given that the Null Hypothesis is correct.

\vspace{3mm}

**Example** (normal distribution): Assume the observed test-statistic leads to a $z$-value = -1.96 
$\Rightarrow$ $\text{P}(|z|\geq 1.96)=0.05$ and $\text{P}(z\leq-1.96)=0.025$ .

\vspace{1mm}


```{r pValFig, echo=FALSE, fig.width=8, fig.height=4,fig.align = "center",out.width='100%'}
par(mfrow=c(1,2))

zz1 <- qnorm(0.025)
zz2 <- qnorm(0.975)
zz3 <- qnorm(0.025)

cord.x1 <- c(-4,seq(-4,zz1,0.01),zz1)
cord.y1 <- c(0,dnorm(seq(-4,zz1,0.01)),0)

cord.x2 <- c(zz2,seq(zz2,4,0.01),4)
cord.y2 <- c(0,dnorm(seq(zz2,4,0.01)),0)

curve(dnorm(x,0,1),-4,4,ylab="density",main="Two-sided p-value (0.05)",xlab="")
polygon(cord.x1,cord.y1,col='gray')
polygon(cord.x2,cord.y2,col='gray')
text(-3,0.05,labels="2.5%")
text(3,0.05,labels="2.5%")

cord.x3 <- c(-4,seq(-4,zz3,0.01),zz3)
cord.y3 <- c(0,dnorm(seq(-4,zz3,0.01)),0)

curve(dnorm(x,0,1),-4,4,ylab="density",main="One-sided p-value (0.025)",xlab="")
polygon(cord.x3,cord.y3,col='gray')
text(-3,0.05,labels="2.5%")
```

---

## Recap: Two types of errors

\vspace{2mm}

In the testing setup, we typically _reject the null hypothesis_ if the $p$-value is small enough. Typical cutoffs for the _significance level_ ($\alpha$) are $5\%$ or $1\%$.

\vspace{2mm}

However, this means we can make two types of errors:
\vspace{2mm}

* Type I error: 

* Type II error:

\vspace{2mm}

---

### Cautionary notes regarding $p$-values:

$~$

* The (mis)use of $p$-values is heavily under critique in the scientific world!



* Simple yes/no decisions do often stand on very wiggly scientific ground.

\vspace{2mm}
$~$

We will discuss this a bit in the final module 12. The topic is connected to good/bad research practice, problems with ``reproducibility'' and scientific progress in general. See here:

* The $p$-value statement by ASA: [https://amstat.tandfonline.com/doi/full/10.1080/00031305.2016.1154108#.Xh16iuExnhM](https://amstat.tandfonline.com/doi/full/10.1080/00031305.2016.1154108#.Xh16iuExnhM)
* Ideas to redefine what ``statistical significane'' means: [https://www.nature.com/articles/s41562-017-0189-z](https://www.nature.com/articles/s41562-017-0189-z)
* A blog by the Scientific American: [https://blogs.scientificamerican.com/observations/to-fix-the-reproducibility-crisis-rethink-how-we-do-experiments/](https://blogs.scientificamerican.com/observations/to-fix-the-reproducibility-crisis-rethink-how-we-do-experiments/)

<!-- * "Reject $H_0$ when $H_0$ is true"="false positives" = "type I error" ="miscarriage of justice". -->
<!-- These are our _fake news_, which are very important for us to avoid. -->

<!-- * "Fail to reject $H_0$ when $H_1$ is true (and $H_0$ is false)"="false negatives" = "type II error"= "guilty criminal go free". -->


---

## Confidence intervals

\vspace{2mm}

* Confidence intervals (CIs) are a much more informative way to report results than $p$-values!

* The $t$-distribution\footnote{If $n$ is large, the normal approximation to the $t$-distribution can be used (and is used in the textbook).} can be used to create confidence intervals for the regression parameters. The lower and upper limits of a 95\% confidence interval for $\beta_j$ are
$$\hat{\beta}_j \pm t_{(1-\alpha/2),n-2} \cdot\text{SE} (\hat{\beta}_j) \quad j=0, 1.$$
* Interpretation of this confidence interval: 

    * There is a 95\% probability that the interval will contain the *true* value of $\beta_j$.

    * **It is the range of parameter estimates that are *compatible with the data* **.


---

Doing this for the bodfat example ``by hand'' is not hard. We have $241 (=243-2)$ degrees of freedom:

\scriptsize
```{r}
coefs <- summary(r.bodyfat)$coef
beta <- coefs[2,1]
sdbeta <- coefs[2,2] 
beta + c(-1,1) * qt(0.975,241) * sdbeta 
```

---

Even easier: directly ask R to give you the CIs.

\scriptsize
```{r }
confint(r.bodyfat,level=c(0.95))
```

\normalsize

**Interpretation:** for an increase in the bmi by one index point, roughly 1.82 percentage points more bodyfat are expected, and all true values for $\beta_1$ between 1.61 and 2.03 are compatible with the observed data.

---

## Confidence and prediction ranges

$~$ 

* Based on the joint distribution of the intercept and slope it is possible to find the distribution for the linear predictor $\hat{\beta}_0+\hat{\beta}_1 x$, and then confidence intervals for $\beta_0+\beta_1 x$. 

\hspace{6mm} $\rightarrow$ **Confidence range**

$~$


* Accounting for the fact that we also have an error in the equation $\varepsilon$, we can also find the distribution of future observations.

\hspace{6mm} $\rightarrow$ **Prediction range**

\vspace{10mm}
We will discuss confidence and prediction ranges in the (more general) multiple linear regression setup.

---

# Model accuracy

Measured by


1. The **residual standard error (RSE)**, which provides an **absolute measure** of _lack of fit_ (see above).

\vspace{2mm}

2. The **coefficient of determination $R^2$**, which measures the proportion of $y$'s variance explained by the model (between 0 and 1), is a **relative measure** of _lack of fit_:

$$R^2 = \frac{\text{TSS}-\text{RSS}}{\text{TSS}}= 1-\frac{\text{RSS}}{\text{TSS}}=1-\frac{\sum_{i=1}^n(y_i-\hat{y}_i)^2}{\sum_{i=1}^n(y_i-\bar{y}_i)^2}, $$ 

where $$\text{TSS} = \sum_{i=1}^n (y_i - \bar{y})^2$$ is the _total sum of squares_, a measure for the total variability in $Y$. 


---

### $R^2$ in simple linear regression

\vspace{3mm}

**Note**: In simple linear regression, $R^2$ is the squared correlation between the independent and the dependent variable.


\vspace{6mm}

Verify this by comparing $R^2$ from the bodyfat output to the squared correlation between the two variables: 

\scriptsize
```{r}
summary(r.bodyfat)$r.squared
cor(d.bodyfat$bodyfat,d.bodyfat$bmi)^2
```




---

# Multiple Linear Regression

Remember that the bodyfat dataset contained much more information than only bmi and bodyfat:

* `bodyfat`: % of body fat.
* `age`: age of the person.
* `weight`: body weighth.
* `height`: body height.
* `bmi`: bmi.
* `abdomen`:  circumference of abdomen.
* `hip`: circumference of hip.

---


## Model
$~$

We assume 
\begin{equation}
Y = \beta_0 + \beta_{1}  X_1 + \beta_2 X_1 + ... + \beta_p X_p + \varepsilon \ ,
\end{equation}

where $X_j$ is the $j$th predictor and $\beta_j$ the respective regression coeffficient.

$~$

Assume we have $n$ sampling units $(x_{1i},\ldots,x_{pi}, y_i)$, $1\leq i \leq n$, such that each represent an instance of equation (3), we can use the data matrix 

$${\bf{X}} = \left[\begin{matrix} 1 & x_{11} & ... & x_{1p} \\
1 & x_{21} & ... & x_{2p} \\
\vdots  & ... & ... & \vdots \\
1 & x_{n1} & ... & x_{np} \\
\end{matrix}\right]$$

to write the model in matrix form:
$${\bf Y}={\bf {X}} \boldsymbol{\beta}+{\boldsymbol{\varepsilon}} $$ 

---

## Notation
$~$

* ${\bf Y}: (n \times 1)$ vector of responses [e.g. one of the following: rent, weight of baby, pH of a lake, volume of a tree]

* ${\bf X}: (n \times (p+1))$ design matrix, and ${\boldsymbol x}_i^T$ is a $(p+1)$-dimensional row row vector for observation $i$.

* ${\boldsymbol \beta}: ((p+1) \times 1)$ vector of regression parameters $(\beta_0,\beta_1,\ldots,\beta_p)^\top$.

* ${\boldsymbol \varepsilon}: (n\times 1)$ vector of random errors.

* We assume that pairs $({\boldsymbol x}_i^T,y_i)$ $(i=1,...,n)$ are measured from _independent_ sampling units. 

$~$

Remark: other books, including the book in TMA4267 and TMA4315 define $p$ to include the intercept. This may lead to some confusion about $p$ or $p+1$ in formulas...


---


## Classical linear model

$${\bf Y=X \boldsymbol\beta}+{\boldsymbol \varepsilon}$$

Assumptions:

1. $\text{E}(\boldsymbol{\varepsilon})=\boldsymbol{0}$.
2. $\text{Cov}(\boldsymbol{\varepsilon})=\text{E}(\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}^T)=\sigma^2\boldsymbol{I}$.
3. The design matrix has full rank, $\text{rank}({\bf X})=p+1$. (We assume $n>>(p+1)$.)

The classical _normal_ linear regression model is obtained if additionally

4. $\boldsymbol\varepsilon\sim N_n({\boldsymbol 0},\sigma^2 {\bf I})$ holds. Here $N_n$ denotes the $n$-dimensional multivarate normal distribution.



<!-- The interpretation of the coefficients $\beta_j$ is now as following: holding all other covariates fixed, what is the average effect on $Y$ of a one-unit increase in the $j$th covariate. -->


---

## Design matrix: Getting it in R

\scriptsize
```{r}
r.bodyfat=lm(bodyfat ~  bmi + age ,data=d.bodyfat)
head(model.matrix(r.bodyfat))
head(d.bodyfat$bmi)
head(d.bodyfat$age)
```
\normalsize

---

## Distribution of the response vector

$~$

Assume that 

$${\bf Y=X \boldsymbol\beta}+{\boldsymbol \varepsilon} \ , \quad \boldsymbol\varepsilon\sim N_n({\bf 0},\sigma^2 {\bf I}) \ . $$

\vspace{4mm}

**Q:** 

* What is the mean $\text{E}(\bf Y)$?

* The covariance matrix $\text{Cov}(\bf Y)$ given $\bf{X}$?

* Thus what is the distribution of $\bf Y$?

---

**A**:
$$ {\bf Y} \sim N_{n}({\bf X} {\boldsymbol\beta},\sigma^2 {\bf I})$$




---


## Parameter estimation for $\boldsymbol{\beta}$

$~$

In multiple linear regression, the parameter vector $\boldsymbol\beta$ is estimated with _\textcolor{red}{maximum likelihood}_ and _\textcolor{red}{least squares}_. These two methods give the same estimator when we assume the normal linear regression model. 

$~$

With least suqres, we minimize the RSS for a multiple linear regression model:
$$\begin{aligned} \text{RSS} &=\sum_{i=1}^n (y_i - \hat y_i)^2 = \sum_{i=1}^n (y_i - \hat \beta_0 - \hat \beta_1 x_{i1} - \hat \beta_2 x_{i2} -...-\hat \beta_p x_{ip} )^2 \\
&= \sum_{i=1}^n (y_i-{\boldsymbol x}_i^T \hat{\boldsymbol\beta})^2=({\bf Y}-{\bf X}\hat{\boldsymbol{\beta}})^T({\bf Y}-{\bf X}\hat{\boldsymbol{\beta}})\end{aligned}$$
The estimator is found by solving the system of $(p+1)$ equations

$$\frac{\partial \text{RSS}}{\partial \boldsymbol \beta}={\bf 0} \ .$$ 

$\rightarrow$ Derivation on the board. See also [here](https://www.math.ntnu.no/emner/TMA4268/2018v/notes/LeastSquaresMLR.pdf) (from Mette in TMA4267).

---

Summing up:
 
\textbf{The least squares and maximum likelihood estimator for ${\boldsymbol \beta}$:} is given like
$$ \hat{\boldsymbol\beta}=({\bf X}^T{\bf X})^{-1} {\bf X}^T {\bf Y} \ .$$
 

---

## Example continued

\tiny
```{r,echo=showsolB}
r.bodyfat3  <- lm(bodyfat ~  bmi + age + neck + hip +abdomen,data=d.bodyfat)
summary(r.bodyfat3)
```
\normalsize

---

Reproduce the values under `Estimate` by calculating without the use of `lm`.

\scriptsize
```{r}
X=model.matrix(r.bodyfat3 )
Y=d.bodyfat$bodyfat
betahat=solve(t(X)%*%X)%*%t(X)%*%Y
print(betahat)
```

---

## Distribution of the regression parameter estimator

\vspace{2mm}
Given
$$ \hat{\boldsymbol\beta}=({\bf X}^T{\bf X})^{-1} {\bf X}^T {\bf Y} \ ,$$
<!-- 3. From Module 2: Part B: $\mathbf{Y}_{(n\times 1)}$ with mean vector -->
<!--     $\mathbf{\mu}$ and variance-covariance matrix $\Sigma$, then $\mathbf{Z}=\mathbf{C}\mathbf{Y}$ -->
<!-- has $\text{E}(\mathbf{Z})=\mathbf{C}\mathbf{\mu}$ and $\text{Cov}(\mathbf{Z})= -->
<!--    \mathbf{C}\Sigma\mathbf{C}^T$. -->
<!-- 4. Also Module 2: Part B: If ${\bf Y}$ is multivariate normal, then also $\mathbf{C}\mathbf{Y}$ is multivariate normal.  -->


**what are**

* The mean $\text{E}(\hat{\boldsymbol\beta})$?
* The covariance matrix $\text{Cov}(\hat{\boldsymbol\beta})$?
* The distribution of $\hat{\boldsymbol\beta}$?

\vspace{4mm}
**Hint:** Use that 

  * $\hat{\boldsymbol\beta}={\bf C}{\bf Y}$ with ${\bf C}=({\bf X}^T{\bf X})^{-1} {\bf X}^T$.
  * ${\bf Y} \sim N_{n}({\bf X} {\boldsymbol\beta},\sigma^2 {\bf I})$. 


---

**Answers:**

See Problem 3 of recommended exercise 3 (and the respective solutions) to derive that

$$\hat{\boldsymbol\beta}\sim N_{p+1}(\boldsymbol{\beta},\underbrace{\sigma^2({\bf X}^T{\bf X})^{-1}}_{\text{covariance matrix}}) \ . $$

<!-- * $\text{E}(\hat{\boldsymbol\beta})={\bf C}\text{E}({\bf Y})=({\bf X}^T{\bf X})^{-1} {\bf X}^T{\bf X} {\boldsymbol\beta}={\boldsymbol\beta}$. -->
<!-- * $\text{Cov}(\hat{\boldsymbol\beta})={\bf C}\text{Cov}({\bf Y}){\bf C}^T=({\bf X}^T{\bf X})^{-1} {\bf X}^T \sigma^2 {\bf I}(({\bf X}^T{\bf X})^{-1} {\bf X}^T)^T =({\bf X}^T{\bf X})^{-1} \sigma^2$. -->
<!-- * $\hat{\boldsymbol\beta}$ is multivariate normal $(p+1)$ dimensions. -->

<!-- So: $\hat{\beta}\sim N_{p+1}(\boldsymbol{\beta},\sigma^2({\bf X}^T{\bf X})^{-1})$. -->

<!-- [](https://www.math.ntnu.no/emner/TMA4268/2018v/notes/PropertiesBetahatMLR.pdf) for a derivation. -->
 
 
 

---

### The covariance matrix of $\boldsymbol{\hat\beta}$ in R

\vspace{2mm}

The covariance matrix for the $\boldsymbol{\hat\beta}$ can be obtained as follows:

\scriptsize
```{r}
vcov(r.bodyfat3)
```
 

---

How does this compare to simple linear regression? Not so easy to see a connection!

$$\hat{\beta}_0 = \bar{Y}-\hat{\beta}_1 \bar{x} \text{ and } \hat{\beta}_1 = \frac{\sum_{i=1}^n(x_i-\bar{x})(Y_i-\bar{Y})}{\sum_{i=1}^n(x_i-\bar{x})^2},$$

$$ \hat{\boldsymbol\beta}=({\bf X}^T{\bf X})^{-1} {\bf X}^T {\bf Y} \ .$$

$~$ 

**Exercise:** Verify the connection using $\boldsymbol\beta=(\beta_0,\beta_1)^\top$ and ${\bf X}= \left[\begin{matrix} 1 & x_{11} \\
1 & x_{21}  \\
1  & \vdots  \\
1 & x_{n1}  \\
\end{matrix}\right]$.

 

<!-- ## Another example: Ozone -->

<!-- $~$ -->

<!-- New York, 1973: 111 observations of -->

<!-- * `ozone` : ozone concentration (ppm); **response variable** -->
<!-- * `radiation` : solar radiation (langleys) -->
<!-- * `temperature` : daily maximum temperature (F) -->
<!-- * `wind` : wind speed (mph) -->

<!-- **Question:** What is the expected ozone concentration, given we know solar radiation, temperature and wind speed? -->

<!-- \scriptsize -->
<!-- ```{r,echo=TRUE,eval=TRUE,messages=FALSE,warnings=FALSE,error=FALSE,results="hold"} -->
<!-- library(ElemStatLearn) -->
<!-- data(ozone) -->
<!-- head(ozone) -->
<!-- ``` -->

<!-- \scriptsize -->

<!-- ```{r,echo=TRUE,eval=TRUE,messages=FALSE,warnings=FALSE,error=FALSE,results="hold"} -->
<!-- ozone.lm = lm(ozone~temperature+wind+radiation, data=ozone) -->
<!-- summary(ozone.lm) -->
<!-- ``` -->


# Four important questions

1. Is at least one of the predictors $X_1, \ldots, X_p$ useful in predicting the response?

2. Do all the predictors help to explain $Y$, or is only a subset of predictors useful?

3. How well does the model fit the data?

4. Given a set of predictor variables, what response value should we predict, and how accurate is our prediction?

---

## 1. Relationship between predictors and response?

$~$

Question is whether we could as well omit all predictor variables at the same time, that is
\begin{center}
$$H_0: \beta_1=\beta_2=\ldots=\beta_p=0 $$

vs. 
$$H_1: \text{at least one } \beta_j \text{ is non-zero.}$$
\end{center}

---

\vspace{4mm}
To answer this, we need the $F$-statistic

$$F = \frac{(\text{TSS}-\text{RSS})/p}{\text{RSS}/(n-p-1)} \sim F_{p,(n-p-1)}\ ,$$

where total sum of squares $\text{TSS}=\sum_i(y_i-\bar{y})^2$, and residual sum of squares $\text{RSS}=\sum_i(y_i-\hat{y}_i)^2$. Under the Normal regression assumptions, $F$ follows an $F_{p,(n-p-1)}$ distribution (see @walepole.etal, Chapter 8.7).

* If $H_0$ is true, $F$ is expected to be 1.

* Otherwise, we expect that the numerator is larger than the denominator (because the regression then explains a lot of variation) and thus $F$ is greater than 1. For an observed value $f_0$, the $p$-value is given as
$$p =P(F_{p,n-p-1}> f_0) \ .$$

---

Checking the $F$-value in the `R` output:

\scriptsize
```{r}
summary(r.bodyfat)
```

\normalsize
Conclusion?

---

## More complex hypotheses

$~$

Sometimes we don't want to test if all $\beta$'s are zero at the same time, but only a subset $1,\ldots , q$:


$$ H_0: \beta_1=\beta_2=\cdots= \beta_q =0$$
\text{  vs.  } 
$$H_1: \text{at least one different from zero}.$$

\vspace{2mm}
Again, the $F$-test can be used, but now $F$ is calculated like

$$F=\frac{(\text{RSS$_0$-RSS})/(q)}{\text{RSS}/(n-p-1)} \sim F_{q,n-p-1} \ ,$$

where
 
* Large model: RSS with $p+1$ regression parameters
* Small model: RSS$_0$ with $q+1$ regression parameters



---

## Example in R
\vspace{3mm}

* **Question:** Do `weight` and `height` explain something of `bodyfat`, on top of the variables `bmi` and `age`?

* Fit both models and use the `anova()` function to carry out the $F$-test:

\footnotesize
```{r}
r.bodyfat.large=lm(bodyfat ~  bmi + age ,data=d.bodyfat)
r.bodyfat.small=lm(bodyfat ~  bmi + age + weight + height, data=d.bodyfat)
anova(r.bodyfat.large,r.bodyfat.small)
```
\normalsize

---

## Inference about a single predictor $\beta_j$ 

$~$

A special case is


$$H_0: \beta_j=0 \; \text{ vs. } \; H_1: \beta_j\neq 0$$
\vspace{2mm}

* Nothing new: We did it for simple linear regression!

* This makes sense: is known that (or you can try to show it yourself)
$$F_{1,n-p-1} = t^2_{n-p-1} \ ,$$
thus we can use a $T$-statistics with $(n-p-1)$ degrees of freedom to get the $p$-value.

<!-- $$ T_j=\frac{\hat{\beta}_j-\beta_j}{\sqrt{c_{jj}}\hat{\sigma}}\sim t_{n-p-1}$$ -->

<!-- Again, $c_{jj}$ is diagonal element corresponding to $\hat{\beta}_j$ of $({\bf X}^T{\bf X})^{-1}$. -->

---

Going back again:

\footnotesize
```{r}
summary(r.bodyfat)$coef
```
\normalsize

However:

* Only checking the individual $p$-values is dangerous. **Why?**  
<!-- $\rightarrow$ e.g., multiple testing problem. -->

* Not possible if $n>p$ $\rightarrow$ need other approaches (see e.g., Module 6).


---

## Inference about $\beta_j$: confidence interval

$~$

* Using that 
$$ T_j=\frac{\hat{\beta}_j}{\text{SE}(\hat\beta_j)}\sim t_{n-p-1} \ ,$$
we can create confidence intervals for $\beta_j$ in the same manner as we did for simple linear regression (see slide 43). For example, when using the typical confidence level $\alpha=0.05$ we have

$$\hat{\beta}_j \pm t_{0.975,n-p-2} \cdot\text{SE} (\hat{\beta}_j)  \ .$$

* Using R, this is very easy:

\footnotesize
```{r}
confint(r.bodyfat)
```


---

## 2. Deciding on important variables

$~$

Overarching question: 

\begin{center}
 \emph{\textcolor{blue}{\bf Which model is the best?}}
\end{center}

But:

* Not clear what _best_ means $\rightarrow$ we need an objective criterion, like AIC, BIC, Mallows $C_p$, adjusted $R^2$.

* There are usually **many** possible models. For $p$ predictors, we can build $2^p$ different models.

* **Cautionary note**: Model selection can also lead to biased parameters estimates. 

$~$

$\rightarrow$ This topic is the focus of Module 6.

---

## 3. Model Fit

$~$

We can again look at the two measures from simple linear regression:

* An absolute measure of lack of fit is again given by the estimate of $\sigma$, the residual standard error (RSE)

$$\hat\sigma = \text{RSE}= \sqrt{ \frac{ \text{RSS}}{n-p-1}} \ . $$


* $R^2$ is again the fraction of variance explained (no change from simple linear regression)
$$R^2 = \frac{\text{TSS}-\text{RSS}}{\text{TSS}}= 1-\frac{\text{RSS}}{\text{TSS}}=1-\frac{\sum_{i=1}^n(y_i-\hat{y}_i)^2}{\sum_{i=1}^n(y_i-\bar{y}_i)^2} \ .$$ 
Simply speaking: "The higher $R^2$, the better."

---

### However: Caveat with $R^2$

$~$

Let us look at the $R^2$s from the three bodyfat models

(model 1: $y\sim bmi$; model 2: $y\sim bmi + age$; 

model 3: $y\sim bmi + age + neck + hip + abdomen$): 

\tiny
```{r, echo=F}
r.bodyfatM1 <- lm(bodyfat ~ bmi , d.bodyfat)
r.bodyfatM2 <- lm(bodyfat ~ bmi  + age, d.bodyfat)
r.bodyfatM3 <- lm(bodyfat ~ bmi  + age + neck + hip + abdomen, d.bodyfat)
```

```{r}
summary(r.bodyfatM1)$r.squared
summary(r.bodyfatM2)$r.squared
summary(r.bodyfatM3)$r.squared
```

\normalsize
The models explain 54%, 58% and 72% of the total variability of $y$. 

It thus _seems_ that larger models are ``better''. However, $R^2$ does always increase when new variables are included, but this does not mean that the model is more reasonable. 


---

## Adjusted $R^2$

$~$

When the sample size $n$ is small with respect to the number of variables $m$ included in the model, an _adjusted_ $R^2$ gives a better ("fairer") estimation of the actual variability that is explained by the covariates:

\begin{equation*}
R^2_a = 1-(1-R^2 )\frac{n-1}{n-m-1}
\end{equation*}

$~$

$R^2_a$ **penalizes for adding more variables** if they do not really improve the model!

$~$

$\rightarrow$ $R_a$ may decrease when a new variable is added.


---

##  Model fit -- in a broader sense

$~$

We will look at model validation / model checking later.


---

## 4. Predictions: Two questions
 
 
1. **Which other regression lines are compatible with the observed data?**

We can use $\hat\beta_0, \ldots , \hat\beta_p$ to estimate the _least squares plane_
$$\hat{Y} = \hat\beta_0 + \hat\beta_1 X_1 + \ldots + \hat\beta_p X_p $$
 as an approximation of $f(X) = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p.$
This leads to the _\textcolor{red}{confidence interval}_. 

\vspace{4mm}

2. **Where do future observations with a given $x$ coordinate lie?**


Even if we could predict $\hat{Y}=f(X)$, the _true_ value $Y$ varies around $\hat{Y}$. We can compute a _\textcolor{red}{prediction interval}_ for new observations $Y$.


---


```{r, echo=F}
r.bodyfat=lm(bodyfat ~  bmi ,data=d.bodyfat)
```

```{r,echo=FALSE, fig.width=5, fig.height=5,fig.align = "center",out.width='50%'}
t.range <- range(d.bodyfat$bmi)
t.xwerte <- seq(t.range[1]-1,t.range[2]+1,by=1)
t.vert <- predict(r.bodyfat,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="confidence")$fit
t.vorh <- predict(r.bodyfat,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="prediction")$fit
plot(d.bodyfat$bmi,d.bodyfat$bodyfat,main="",xlab="BMI",ylab="bodyfat",xlim=range(t.xwerte),ylim=c(-5,50),cex=0.8)
abline(r.bodyfat,lwd=2)
lines(x=t.xwerte,y=t.vert[,2],lty=8,lwd=2,col=2)
lines(x=t.xwerte,y=t.vert[,3],lty=8,lwd=2,col=2)
lines(x=t.xwerte,y=t.vorh[,2],lty=8,lwd=2,col=4)
lines(x=t.xwerte,y=t.vorh[,3],lty=8,lwd=2,col=4)
legend("bottomright", c("confidence range (95%)", "prediction range (95%)"),
lty=8, cex=1,col=c(2,4),lwd=2)
```

\colorbox{lightgray}{\begin{minipage}{10cm}

Plotting the confidence and prediction intervals around all predicted values $\hat Y_0$ one obtains the {\bf confidence range} or {\bf confidence band} for the expected values of $Y$.
\end{minipage}} 

Note: The prediction range is much broader than the confidence range. Why?

---

### Calculation of the confidence intervals/range

$~$ 

Given a realization of $X_1, \ldots ,X_p$, say $x_1^{(0)}, \ldots x_p^{(0)}$. The question is:  

$~$ 

 
**Where does $\hat y_0 = \hat\beta_0 + \hat\beta_1 x_1^{(0)} + \ldots \hat\beta_p x_p^{(0)}$ lie with a certain confidence (i.e., 95\%)?**


\vspace{6mm} 

This question is not trivial, because $\hat\beta_0, \ldots \hat\beta_p$ are estimates from the data and contain uncertainty. 

$~$ 

<!-- The details of the calculation are given in Stahel 2.4b.  -->

<!-- %(Idea: $\hat y_0 \pm q\cdot se^{(\hat{y}_0)}$.) \\[6mm] -->



\vspace{4mm}

$\rightarrow$ For the confidence range, only the uncertainty in the estimates $\hat\beta_0, \ldots \hat\beta_p$ matters.



---


### Calculation of the prediction intervals/range

$~$ 

Given a new value of $X_1, \ldots, X_p$, say $x_1^{(0)}, \ldots x_p^{(0)}$. The question is:  

$~$ 

**Where does a future observation lie with a certain confidence (i.e., 95\%)?**

\vspace{6mm} 

To answer this question, we have to sum uncertainty over two components:

1. the _\textcolor{red}{uncertainty in the predicted value}_ $\hat y_0 =  \hat\beta_0 + \hat\beta_1 x_1^{(0)} + \ldots \hat\beta_p x_p^{(0)}$ (due to uncertainty in $\hat{\boldsymbol\beta}$).

2. the _\textcolor{red}{irreducible error}_ $\varepsilon_i \sim N(0,\sigma^2)$.  


\vspace{4mm}

$\rightarrow$ The *prediction intervals and range are always wider than the confidence intervals and range*.


---

Confidence and prediction intervals for given data points can be found in R using `predict` on an `lm` object (make sure that `newdata` is a `data.frame` with the same names as the original data). 


\scriptsize
```{r}
fit=lm(bodyfat ~ bmi + age + abdomen,data=d.bodyfat)
newobs=d.bodyfat[1,]
predict(fit,newdata=newobs,interval="confidence",type="response")
predict(fit,newdata=newobs,interval="prediction",type="response")
```


---


Finally, we need to keep in mind that the model we work with is only an _approximation of the reality_. In fact,

\vspace{4mm}

In 2014, David Hand wrote:

\vspace{4mm}

\begin{quote}
In general, when building statistical models, we must
not forget that the aim is to understand something about
the real world. Or predict, choose an action, make
a decision, summarize evidence, and so on, but always
about the real world, not an abstract mathematical
world: our models are not the reality -- a point well
made by George Box in his often-cited remark that
``all models are wrong, but some are useful''.
\end{quote}

[@box1979]




---

## Challenges - for model fit


1. Non-linearity of data
2. Correlation of error terms
3. Non-constant variance of error terms
4. Non-Normality of error terms
5. Outliers
6. High leverage points
7. Collinearity

---

### Recap of modelling assumptions in linear regression

$~$

To make valid inference from our model, we must check if our model assumptions are fulfilled!\footnote{What is the problem if the assumptions are violated?}

$~$

The assumption in linear regression is that the residuals follow a $N(0,\sigma^2)$ distribution, implying that :

$~$

1. The expected value of $\varepsilon_i$ is 0: $\text{E}(\varepsilon_i)=0$. 
\vspace{1mm}

2. All $\varepsilon_i$ have the same variance: $\text{Var}(\varepsilon_i)=\sigma^2$.  
\vspace{1mm}

3. The $\varepsilon_i$ are normally distributed. 
\vspace{1mm}

4. The $\varepsilon_i$ are independent of each other.


---

## Model checking tool I: Tukey-Anscombe diagram

$~$

The _\textcolor{red}{Tukey-Anscombe}_ diagram plots the residuals against the fitted values. For the bodyfat data it looks like this:

```{r,echo=FALSE, fig.width=5, fig.height=4,fig.align = "center",out.width='50%'}
library(ggfortify)
autoplot(r.bodyfat,smooth.colour=NA)[1] 
```
This plot is ideal to check if assumptions 1. and 2. (and partially 4.) are met. Here, this seems fine.


---

## Model checking tool II: The QQ-diagram
$~$

To check assumption 3., the quantiles of the observed distribution are plotted against the quantiles of the respective theoretical (normal) distribution:

 
```{r,echo=FALSE, fig.width=5, fig.height=4,fig.align = "center",out.width='50%'}
autoplot(r.bodyfat,smooth.colour=NA)[2] 
```
 
If the points lie approximately on a straight line, the data is fairly normally distributed. This is often ``tested'' by eye, and needs some experience.


<!-- --- -->

<!-- # Todo: Move this to exercises! -->
<!-- ## How do I know if a QQ-plot looks ``good''? -->

<!-- There is **no quantitative rule** to answer this question, experience is needed. However, you can gain this experience from simulations. To this end, generate the same number of data points of a normally distributed variable and compare to your plot.  -->

<!-- Example: Generate 59 points $\varepsilon_i \sim N(0,1)$ each time: -->


<!-- ```{r,echo=FALSE, fig.width=6, fig.height=3,fig.align = "center",out.width='80%'} -->
<!-- set.seed(390457) -->
<!-- par(mfrow=c(2,3),mar=c(4,4,1,1)) -->
<!-- for (ii in 1:6){ -->
<!--   ss <- rnorm(59) -->
<!--   qqnorm(ss,main="") -->
<!--   qqline(ss,xlab="") -->
<!-- } -->
<!-- ``` -->


---

## Model checking tool III: The scale-location plot

$~$

The scale-location plot is particularly suited to check the assumption of equal variances (homoscedasticity; assumption 2.).

$~$

The idea is to plot the square root of the (standardized) residuals $\sqrt{|\tilde{r}_i|}$ against the fitted values $\hat{y_i}$. There should be *no trend*:
 
```{r,echo=FALSE, fig.width=4, fig.height=4,fig.align = "center",out.width='50%'}
autoplot(r.bodyfat,which=3) + theme_bw()
```



---

## Model checking tool IV: The leverage plot

$~$

* Mainly useful to determine outliers.

$~$

* To understand the leverage plot, we need to introduce the idea of the **leverage**.

$~$

* In simple regression, the leverage of individual $i$ is defined as 
\begin{equation}\label{eq:leverage}
H_{ii} = \frac{1}{n} + \frac{(x_i-\overline{x})^2}{\sum_{i'}(x_{i'}-\overline{x})^2} \ . 
\end{equation}

\vspace{7mm}

**Q:** When are leverages expected to be large/small?

$~$

---

**Illustration**:
Data points with $x_i$ values far from the mean have a stronger leverage effect than when $x_i\approx \overline{x}$:

```{r,echo=FALSE, fig.width=6, fig.height=2,fig.align = "center",out.width='90%'}
set.seed(37489)
par(mfrow=c(1,3),mar=c(4,4,1,1))
x <- sort(rnorm(18))
y <- 2*x + rnorm(18,0,0.4)
plot(y~x)
abline(lm(y~x))
y1 <- y
y1[18] <- y[18] -5
plot(y1~x,col=c(rep(1,17),2))
abline(lm(y~x))
abline(lm(y1~x),col=2,lty=2)
y2 <- y
y2[9] <- y2[9] + 4
plot(y2~x,col=c(rep(1,8),2,rep(1,9)))
abline(lm(y~x))
abline(lm(y2~x),col=2,lty=2)
```

The outlier in the middle plot ``pulls'' the regression line in its direction and biases the slope.

$~$

\href{http://students.brown.edu/seeing-theory/regression-analysis/index.html}{Click here} to do it manually!

---


In the leverage plot, (standardized) residuals $\tilde{r_i}$ are plotted against the leverage $H_{ii}$ (still for the bodyfat):
 
 
```{r,echo=FALSE, fig.width=4, fig.height=4,fig.align = "center",out.width='50%'}
autoplot(r.bodyfat,which=5)
```
 



**Critical ranges** are the top and bottom right corners! Why?

---

### Leverages in multiple regression

\vspace{3mm}

* Leverage is defined as the diagonal elements of the so-called _hat matrix_ $\mathbf{H}$\footnote{Do you remember why $\mathbf{H}$ is called \emph{hat matrix}?}, i.e., the leverage of the $i$-th data point is $H_{ii}$ on the diagonal of $\mathbf{H = X(X^TX)^{-1}X^T}$. 

\vspace{1mm}

* Exercise: Verify that formula (4) comes out in the special case of simple linear regression.

\vspace{1mm}

* A large leverage indicates that the observation ($i$) has a large influence on the estimation results, and that the covariate values (${\boldsymbol x}_i$) are unusual.



<!-- ### Cook's distance -->

<!-- Cook's distance is the Euclidean distance between the ${\hat{y}}$ (the fitted values) and ${\hat{y}}_{(i)}$ (the fitted values calculated when the $i$-th observation is omitted from the regression). This is then a measure on how much the model is influences by observation $i$. The distance is scaled, and a rule of thumb is to examine observations with Cook's distance larger than 1, and give some attention to those with Cook's distance above 0.5. -->

<!-- Cook's distance is shown with dashed lines when you use the `plot.lm()` function on a `lm` object. -->


<!-- --- -->

<!-- ```{r,echo=FALSE, fig.width=4, fig.height=4,fig.align = "center",out.width='50%'} -->
<!-- plot(r.bodyfat) -->
<!-- ``` -->

---

### Different types of residuals?

$~$

It can be shown that the vector of residuals, ${\bf e}=(e_1,e_2,\ldots,e_n)$ have a normal (singular) distribution with 

* $\text{E}({\bf e})={\bf 0}$ 
* $\text{Cov}({\bf e})=\sigma^2({\bf I}-{\bf H})$, 

where ${\bf H}={\bf X}({\bf X}^T{\bf X})^{-1}{\bf X}^T$.

$~$

This means that the residuals (possibly) have different variance, and may also be correlated.

$~$

**Q:** Why is that a problem?

---

**A**:

* To check the model assumptions we want to look at the distribution of the error terms $\varepsilon_i$, to check that our errors are independent, homoscedastic (same variance for each observation), and not dependent on our covariates.

* However, we only have the residuals $e_i$, the "predictions" for $\epsilon_i$.

* It would have been great if the $e_i$ have the same properties as $\epsilon_i$. 

\vspace{2mm}
$\rightarrow$ To make the $e_i$ more "like $\epsilon_i$", we use _\textcolor{red}{standardized}_ or _\textcolor{red}{studentized residuals}_.

---

**Standardized residuals:**

$$r_i=\frac{e_i}{\hat{\sigma}\sqrt{1-H_{ii}}}$$
where $H_{ii}$ is the $i$th diagonal element of the hat matrix ${\bf H}$.

In R you can get the standardized residuals from an `lm`-object (named `fit`) by `rstandard(fit)`.

\vspace{2mm}

**Studentized residuals:**

$$r^*_i=\frac{e_i}{\hat{\sigma}_{(i)}\sqrt{1-H_{ii}}}$$
where $\hat{\sigma}_{(i)}$ is the estimated error variance in a model with observation number $i$ omitted. It can be shown that it is possible to calculated the studentized residuals directly from the standardized residuals.

In R you can get the studentized residuals from an `lm`-object (named `fit`) by `rstudent(fit)`.

---

### Diagnostic plots in `R`

\vspace{2mm}

See exercises: We use `autoplot()` from the `ggfortify` package in R to plot the diagnostic plots.  

 


---

### Collinearity

In brief, collinearity refers to the situation when two or more predictors are correlated, thus encode (partially) for the same information.

$~$

**Problems:**

* Reduces the accuracy of the estimated coefficients $\hat\beta_j$ (large SE!).

* Consequently, reduces power in finding effects ($p$-values become larger).

**Solutions:**

* Detect it by calculating the _variance inflation factor_ (VIF).

* Remove the problematic variable.

* Or combine the collinear variables into a single new one.

$~$

**Todo:** Read in the course book p.99-102 (self-study).


---

# Other considerations in the regression model

1. Qualitative predictors ($X_j$): 
    + Binary covariate (e.g., male/female, smoker/non-smoker) 
    + Categorical covariate (e.g., black/white/green)?

\vspace{4mm}

2. Extensions of the linear model
    + Interactions
    + Non-linear terms



---


## Binary predictors

$~$

So far, the covariates $X$ were always continuous.  
\vspace{2mm}

In reality, there are no restrictions assumed with respect to the $X$ variables. 
\vspace{2mm}

One very frequent data type are **binary** variables, that is, variables that can only attain values 0 or 1. 
\vspace{4mm}

If the binary variable $x$ is the only variable in the model $Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$, the model has only two predicted outcomes (plus error):

\begin{equation*}
Y_i = \left\{ 
\begin{array}{ll}
 \beta_0  + \varepsilon_i \quad &\text{if } x_i=0 \ , \\
 \beta_0 + \beta_1 + \varepsilon_i \quad &\text{if } x_i =1 \ .\\
\end{array}
\right .
\end{equation*}

\vspace{4mm}

**Example**: Credit card data analysis in Section 3.3.1 in the ISLR book.


---

## Qualitative predictors with more than 2 levels

$~$
 
More generally, a covariate may indicate a **category**, for instance the species of an animal or a plant. This type of covariate is called a **factor**. The trick: convert a factor variable $X$ with $k$ levels (for instance 3 species) into $k$ dummy variables $X_j$ with 
\vspace{2mm}

\begin{equation*}
x_{ij} = \left\{ 
\begin{array} {ll}
1, & \text{if the $i$th observation belongs to group $j$}.\\
0, & \text{otherwise.}
\end{array}\right.
\end{equation*}


\vspace{4mm}

Each of the covariates $x_1,\ldots, x_k$ can then be included as a binary variable in the model
\begin{equation*}
y_i = \beta_0 + \beta_1 x_1 + \ldots + \beta_k x_k + \varepsilon_i \ .
\end{equation*}

\vspace{6mm}
However:  this model is _\textcolor{red}{not identifiable}_.\footnote{What does that mean? I could add a constant to $\beta_1, \beta_2, ...\beta_k$ and subtract it from $\beta_0$, and the model would fit equally well to the data, so it cannot be decided which set of the parameters is best.}

---

**Solution:** One of the $k$ categories must be selected as a _reference category_ and is _not included in the model_. Typically: the first category is the reference, thus $\beta_1=0$.

\vspace{2mm}


The model thus discriminates between the factor levels, such that (assuming $\beta_1=0$)

\begin{equation*}
y_i = \left\{
\begin{array}{ll}
\beta_0 + \varepsilon, & \text{if $x_{i1}=1$ }\\
\beta_0 + \beta_2 + \varepsilon, & \text{if $x_{i2}=1$ }\\
...\\
\beta_0 + \beta_k + \varepsilon, & \text{if $x_{ik}=1$ } \ .
\end{array}\right.
\end{equation*} 

---

## !Important to remember!
\textcolor{gray}{(Common aspect that leads to confusion!)}

\vspace{10mm}

 _\textcolor{red}{A factor covariate with $k$ factor levels requires $k-1$ parameters!}_
\vspace{2mm}

$\rightarrow$ The _degrees of freedom_ of the fitted model are therefore reduced by $k-1$.


---

## Example

$~$

We are now using the `Credit` dataset from the `ISLR` library.

\scriptsize
```{r, echo=T}
library(ISLR)
data(Credit)
head(Credit)
```

\normalsize
Question: Do the Balances differ for different Ethnicities?


---

In R, a factor covariate can be used in the same way as a continuous predictor:

\scriptsize
```{r, echo=T}
r.lm <- lm(Balance ~ Ethnicity, data=Credit)
summary(r.lm)
```

\normalsize
Interpretation? Do the ethnicities really differ? Check also the $F$-test in the last line of the summary output.

---

## The "reference category"
\vspace{2mm}
In the above example we do not see a result for the `EthnicityAfrican American`. Why?

* `African American` is chosen to be the reference category.
* The results for `EthnicityAsian` and `EthnicityCaucasian` are **differences** with respect to the reference cateogry.
* R chooses the reference category in alphabetic order! This is sometimes not a relevant category. 
* You can change the reference category:

\scriptsize
```{r, echo=T}
library(dplyr)
Credit <-  mutate(Credit,Ethnicity = relevel(Ethnicity,ref="Caucasian"))
r.lm <- lm(Balance ~ Ethnicity, data=Credit)
summary(r.lm)$coef
```

\small
Note: The differences are now with respect to the Caucasian category -- the model is however exactly the same!

---

## Testing for a categorical predictor

\vspace{2mm}

**Question**: Is a qualitative predictor needed in the model?

\vspace{2mm}

For a predictor with more than two levels (like Ethnicity above), the Null Hypothesis is whether

$$\beta_1 = \ldots = \beta_{k-1}=0$$
at the same time. 

\vspace{2mm}

$\rightarrow$ We again need the $F$-test\footnote{remember that the $F$-test is a generalization of the $t$-test!}, as **always when we test for more than one $\beta_j=0$ _simultaneously_**!

\vspace{2mm}
In R, this is done by the `anova()` function:

\scriptsize
```{r, echo=T}
anova(r.lm)
```

---

## Interactions: Removing the additivity assumption
\vspace{2mm}

We again look at the `Credit` dataset. We want to model the `Balance` as a function of `Income` and wheter the person is a student or not.

\vspace{2mm}

The model is given as 
$$\text{Balance}_i = \beta_0 + \beta_1 \cdot \text{Income}_i + \beta_2 \cdot \text{Student}_i + \varepsilon_i \ ,$$
where `Student` is a binary variable. Thus we have a model that looks like
\begin{equation*}
\text{Balance}_i = \left\{ 
\begin{array}{ll}
\beta_0 + \beta_2 + \beta_1 \cdot \text{Income}_i  + \varepsilon_i \ ,  & \text{if $i$ is a student,}\\
\beta_0 + \qquad \; \beta_1 \cdot \text{Income}_i  + \varepsilon_i  & \text{otherwise.}
\end{array}
\right.
\end{equation*}


In R, we simply add `Student` to the model:
\scriptsize
```{r, echo=T}
r.lm <- lm(Balance ~ Income + Student, Credit)
```

\normalsize

**Caveat:** This model assumes that students and non-students have the same slope for `Income`. Realistic?

---

Let's look at the graphs:

![ISLR Figure 3.7](../../ISLR/Figures/Chapter3/3.7.png)

$\rightarrow$ We want a model that allows for different slopes!

---

## Interaction terms
\vspace{2mm}

We formulate a new model that includes the interaction term $(\text{Income}\cdot \text{Student})$:

$$\text{Balance}_i = \beta_0 + \beta_1 \cdot \text{Income}_i + \beta_2 \cdot \text{Student}_i + \beta_3 \cdot \text{Income}_i \cdot \text{Student}_i  + \varepsilon_i \ ,$$

Thus we have a model that allows for different intercept _and_ slope for the two groups:

\begin{equation*}
\text{Balance}_i = \left\{ 
\begin{array}{ll}
\beta_0 + \beta_2 + (\beta_1 + \beta_3) \cdot \text{Income}_i  + \varepsilon_i \ ,  & \text{if $i$ is a student,}\\
\beta_0 + \qquad \; \beta_1 \cdot \text{Income}_i  + \varepsilon_i  & \text{otherwise.}
\end{array}
\right.
\end{equation*}


---

In R, this is again quite simple:

\scriptsize
```{r, echo=T}
r.lm <- lm(Balance ~ Income * Student, Credit)
summary(r.lm)$coef
```

\normalsize

**Interpretation:**

We allow the model to depend on the binary variable `Student`, such that

For a student: $\hat{y} =$ `r round(summary(r.lm)$coef[1,1],1)` +  `r round(summary(r.lm)$coef[3,1],1)` + (`r round(summary(r.lm)$coef[2,1],1)` + `r format(summary(r.lm)$coef[4,1],1,1,1)`) $\cdot$ Income 

For a non-Student: $\hat{y} =$ `r round(summary(r.lm)$coef[1,1],1)`  + (`r round(summary(r.lm)$coef[2,1],1)`) $\cdot$ Income 

\vspace{2mm}

**Question:** Is the interaction relevant here?


---

## The hierarchical principle

$~$

If we include an interaction in a model, we
should also include the main effects, even if the $p$-values associated with
the coefficients of the main effects are large (see p.89 in ISLR book).

---

## More interactions

\vspace{4mm}

We can include interactions also between

* two continuous variables.
* a categorical variable with more than 2 levels and a continuous variable.

\vspace{4mm}

$\rightarrow$ See exercises!

\vspace{4mm}


---


## Non-linear terms
\vspace{2mm}

**Linear regression is even more powerful!**
\vspace{2mm}

* We have seen that it is possible to include continuous, binary or factorial covariates in a regression model.

\vspace{2mm}

* Even \emph{\textcolor{red}{transformations}} of covariates can be included in (almost) any form. For instance the square of a variable $X^2$
\begin{equation*}
y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \varepsilon_i \ , 
\end{equation*}
which leads to a **quadratic** or **polynomial** regression (if higher order terms are used).

\vspace{4mm}

* Other common transformations are: 
    * $\log$ 
    * $\sqrt{..}$
    * $\sin$, $\cos$,


--- 

How can a \emph{quadratic} regression be a \emph{linear regression}??

\vspace{4mm}

**Note**:

The word _linear_ refers to the _\textcolor{red}{linearity in the coefficients}_, and not on a linear relationship between $Y$ and $X_1, \ldots , X_p$!


\vspace{4mm}

**Question**: When would we need such a regression? Well, sometimes the world is not linear. In particular, if 

* there is a theoretical/biological/medical reason to believe in a non-linear relationship, or
* the residual analysis indicates that there are non-linear associations in the
data,

it can sometimes help to use transformations of a variable $X$. 

\vspace{2mm}

\scriptsize
$\rightarrow$ In the later modules, we will discuss other more advanced non-linear
approaches for addressing this issue.



---

# <a id="further"> Further reading </a>


* [Videoes on YouTube by the authors of ISL, Chapter 3](https://www.youtube.com/playlist?list=PL5-da3qGB5IBSSCPANhTgrw82ws7w_or9)

<!-- --- -->

<!-- # <a id="Rpackages"> R packages</a> -->

<!-- If you want to look at the .Rmd file and `knit` it, you need to first install the following packages (only once). -->

<!-- ```{r, eval=FALSE} -->
<!-- # packages to install before knitting this R Markdown file -->
<!-- # to knit the Rmd -->
<!-- install.packages("knitr") -->
<!-- install.packages("rmarkdown") -->

<!-- # nice tables in Rmd -->
<!-- install.packages("kableExtra") -->

<!-- # cool layout for the Rmd -->
<!-- install.packages("prettydoc") # alternative to github -->

<!-- #plotting -->
<!-- install.packages("ggplot2") # cool plotting -->
<!-- install.packages("ggpubr") # for many ggplots -->
<!-- install.packages("GGally") # for ggpairs -->
<!-- #datasets -->
<!-- install.packages("ElemStatLearn") # for ozone data set -->
<!-- install.packages("gamlss.data")#rent index data set here -->
<!-- #methods -->
<!-- install.packages("nortest")#test for normality - e.g. Anderson-Darling -->

<!-- install.packages("car") # vif -->
<!-- library(Matrix) -->
<!-- install.packages("reshape") -->
<!-- install.packages("corrplot") -->
<!-- install.packages("tidyverse") -->
<!-- ``` -->



---

# References

