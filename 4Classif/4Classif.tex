\documentclass[10pt,ignorenonframetext,]{beamer}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
\usetheme[]{Singapore}
\usefonttheme{serif}
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\newif\ifbibliography
\hypersetup{
            pdftitle={Module 4: Classification},
            pdfauthor={Stefanie Muff, Department of Mathematical Sciences, NTNU},
            colorlinks=true,
            linkcolor=Maroon,
            citecolor=Blue,
            urlcolor=blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight0.8\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}

% Prevent slide breaks in the middle of a paragraph:
\widowpenalties 1 10000
\raggedbottom

\AtBeginPart{
  \let\insertpartnumber\relax
  \let\partname\relax
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \let\insertsectionnumber\relax
    \let\sectionname\relax
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \let\insertsubsectionnumber\relax
  \let\subsectionname\relax
  \frame{\subsectionpage}
}

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
\usepackage{multirow}

\title{Module 4: Classification}
\subtitle{TMA4268 Statistical Learning V2020}
\author{Stefanie Muff, Department of Mathematical Sciences, NTNU}
\date{February xx, 2020}

\begin{document}
\frame{\titlepage}

\begin{frame}{Introduction}

\begin{block}{Learning material for this module}

\begin{itemize}
\tightlist
\item
  James et al (2013): An Introduction to Statistical Learning. Chapter
  4.
\end{itemize}

We need more statistical theory than is presented in the textbook, which
you find in this module page.

\vspace{2mm}

Some of the figures and slides in this presentation are taken (or are
inspired) from James et al. (2013).

\end{block}

\end{frame}

\begin{frame}

\begin{block}{What will you learn? (todo)}

\begin{itemize}
\tightlist
\item
  What is classification and discrimination?
\item
  What is the Bayes classifier and the Bayes risk?
\item
  What is the sampling paradigm and what is modelled then?
\item
  Linear discriminant analysis: model, method, results.
\item
  Quadratic discriminant analysis: model, method, results.
\item
  Naive Bayes - when and why?
\item
  That is the diagnostic paradigm and what is modelled then?
\item
  KNN - majority vote or estimate posterior class probability?
\end{itemize}

\end{block}

\end{frame}

\begin{frame}[fragile]{What is classification?}

\vspace{2mm}

\begin{itemize}
\item
  By now our responses \(Y\) was assumed \emph{continuous}, while
  covariates were allowed to be \emph{categorical}.
\item
  Now we allow the response to be \emph{categorical}.
\item
  This is even more common than continuous responses. Examples:

  \begin{itemize}
  \tightlist
  \item
    Spam filters \texttt{email} \(\in\) \{spam, ham\},
  \item
    \texttt{Eye\ color} \(\in\) \{blue, brown, green\}.
  \item
    \texttt{Medical\ condition} \(\in\) \{disease1, disease2,
    disease3\}.
  \end{itemize}
\end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}
\item
  Suppose we have a qualititative response value that can be a member in
  one of \(K\) classes \(\mathcal{C} = \{c_1, c_2, \ldots , c_K\}\).
\item
  In classification we build a function \(f(X)\) that takes a vector of
  input variables \(X\) and predicts its class membership, such that
  \(Y \in \mathcal{C}\).
\item
  We would also assess the \emph{uncertainty} in this classification.
  Sometimes the role of the different predictors may be of main
  interest.
\item
  We often build models that \textbf{predict probabilities of
  categories}, \emph{given} certain covariates \(X\).
\end{itemize}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Example: The credit card data}

The \texttt{Default} dataset is available from the ISLR package.

\vspace{2mm}
{\bf Aim}: to predic whether an individual will default on his or her
credit card payment, given the annual income and credit card balance.

\textcolor{orange}{Orange: default=yes},
\textcolor{blue}{blue: default=no}.

\vspace{0mm} \centering
\includegraphics[width=0.50000\textwidth]{../../ISLR/Figures/Chapter4/4.1a.png}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{General classification setup}

\(~\)

\textbf{Set-up:} Training observations
\(\{(x_1, y_1), ..., (x_n, y_n)\}\) where the response variable \(Y\) is
categorical, e.g \(Y \in \mathcal{C} = \{0, 1, ..., 9\}\) or
\(Y \in \mathcal{C} = \{dog, cat,... ,horse\}\).

\(~\)

\textbf{Aim: } To \emph{build} a classifier \(f(X)\) that assigns a
class label from \(\mathcal{C}\) to a future unlabelled observation
\(x\) and to asses the \emph{uncertainty} in this classification.

\(~\)

\textbf{Performance measure:} Most popular is the misclassification
error rate (training and test version).

\end{block}

\end{frame}

\begin{frame}

\begin{block}{What are the methods?}

\(~\)

\textbf{Three methods for classification} are discussed here:

\begin{itemize}
\item
  Logistic regression
\item
  \(K\)-nearest neighbours
\item
  Linear and quadratic discriminant analysis
\end{itemize}

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Linear regression for binary classification?}

Suppose we have a binary outcome, for example whether a credit card user
defaults \(Y =\) \texttt{yes} or \texttt{no}, given covariates \(X\) to
predict \(Y\). We could use \emph{dummy encoding} for \(Y\) like

\[Y = \left\{ \begin{array}{ll}
0 & \text{if } \texttt{no} \ , \\
1 & \text{if } \texttt{yes} \ .
\end{array} \right.\] Can we simply perform a linear regression of \(Y\)
on \(X\) and classify as \texttt{yes} if \(\hat{Y}> 0.5\)?

\begin{itemize}
\item
  In this case of a binary outcome, linear regression does a good job as
  a classifier, and is equivalent to linear discriminant analysis which
  we discuss later.
\item
  Since in the population
  \(\text{E}(Y \mid X = x) = \text{Pr}(Y = 1 \mid X = x)\), we might
  think that regression is perfect for this task.
\item
  However, linear regression might produce probabilities less than zero
  or bigger than one.
\end{itemize}

\(\rightarrow\) We need to use \textbf{logistic regression}.

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Linear vs.~logistic regression}

\(~\)

Let's anticipate a bit, to see why linear regression does not so well.
We estimate the probability that someone defaults, given the credit card
balance as predictor: \vspace{2mm}

\includegraphics{../../ISLR/Figures/Chapter4/4.2.png}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Linear regression for categorical classification?}

\(~\)

What when there are more than two possible outcomes? For example, a
medical diagnosis \(Y\), given predictors \(X\) can be categorized as

\(~\)

\[Y = \left\{ \begin{array}{ll}
1 & \text{if } \texttt{stroke} \ , \\
2 & \text{if } \texttt{drug overdose} \ , \\
3 & \text{if } \texttt{epileptic seizure} \ .
\end{array} \right.\]

\(~\)

This suggests an ordering, but this is artificial.

\(~\)

\begin{itemize}
\tightlist
\item
  Linear and logistic regression are not appropriate here.
\item
  We need \emph{Multiclass logistic regression} and \emph{Discriminant
  Analysis}.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{However:}

\begin{itemize}
\item
  It is still possible to use linear regression for classification
  problems with two classes. It is actually not even a bad idea, and
  works well under some conditions. If the conditional class densities
  are (multivariate) normal with equal covariance matrices then this
  linear regression (with 0 and 1 response) will in fact give the same
  classification as linear discriminatn analysis (LDA).
\item
  For categorical outcomes with more than two level, it requires some
  extra work (multivariate \(Y\) due to the dummy variable coding).
\item
  We leave linear regression for now.
\item
  For two classes \emph{binary regression}, in particular \emph{logistic
  regression}, is very popular - and is up next.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}{Logistic regression}

\begin{itemize}
\item
  In logistic regression we consider a classification problem with two
  classes.
\item
  Assume that \(Y\) is coded (\(\mathcal{C} = \{1, 0\}\) or \{success,
  failure\}), and we focus on success \((Y=1)\).
\item
  We may assume that \(Y_i\) follows a \textbf{Bernoulli distribution}
  with probability of success \(p_i\).
\end{itemize}

\[Y_i = \begin{cases} 1 \text{ with probability } p_i, \\ 0 \text{ with probability } 1-p_i. \end{cases}\]

\begin{itemize}
\tightlist
\item
  \textbf{Aim}: For covariates \((X_1,\ldots,X_p)\), we want to estimate
  \(p_i = \text{Pr}(Y_i=1 \mid X_1,\ldots,X_p)\).
\end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}
\item
  We need a clever way to \emph{link} our covariates
  \(X_1, \ldots, X_p\) with this probability \(p_i\). Aim: want to
  relate the \emph{linear predictor}
  \[\eta_i = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}\] to
  \(p_i\). How?
\item
  The idea is to use a so-called \emph{link-function} to link \(p_i\) to
  the linear predictor.
\item
  In logistic regression, we use the \emph{logistic link function}
\end{itemize}

\begin{center}
\colorbox{lightgray}{\begin{minipage}{10cm}
\begin{equation}
\log\left( \frac{p_i}{1-p_i} \right) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_p x_{ip} \ .
\end{equation}
\end{minipage}}
\end{center}

\begin{itemize}
\tightlist
\item
  \textbf{Q:} What is the rationale behind this? Other transformations
  that bring the linear predictor into a \(0\) to \(1\) range?
\end{itemize}

\end{frame}

\begin{frame}

\begin{block}{Logistic regression with one covariate}

\begin{itemize}
\tightlist
\item
  Equation (1) can be rearranged and solved for \(p_i\). Let's look at
  this for only one covariate:
\end{itemize}

\[ p_i= \frac{e^{\beta_0+\beta_1 x_i}}{1 + e^{\beta_0 + \beta_1 x_i}}.\]

\begin{itemize}
\item
  \textbf{Important:} These values \(p_i\) will always lie in the
  interval between 0 and 1, with an S-shaped curve.
\item
  The parameter \(\beta_1\) determines the rate of increase or decrease
  of the S-shaped curve, and the sign indicates whether the curve
  ascends or descends.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Example: \texttt{Default} credit card data}

\vspace{2mm}

\begin{itemize}
\item
  The parameters are estimated using the method of maximum likelihood -
  we will look at that soon.
\item
  Let's first do it! In R, this works with the \texttt{glm()} function,
  where we specify \texttt{family="binomial"}.
\end{itemize}

\vspace{2mm}

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ISLR)}
\KeywordTok{data}\NormalTok{(Default)}
\NormalTok{Default}\OperatorTok{$}\NormalTok{default <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(Default}\OperatorTok{$}\NormalTok{default) }\OperatorTok{-}\StringTok{ }\DecValTok{1}
\NormalTok{glm_default =}\StringTok{ }\KeywordTok{glm}\NormalTok{(default }\OperatorTok{~}\StringTok{ }\NormalTok{balance, }\DataTypeTok{data =}\NormalTok{ Default, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{)}

\KeywordTok{summary}\NormalTok{(glm_default)}\OperatorTok{$}\NormalTok{coef}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                  Estimate   Std. Error   z value      Pr(>|z|)
## (Intercept) -10.651330614 0.3611573721 -29.49221 3.623124e-191
## balance       0.005498917 0.0002203702  24.95309 1.976602e-137
\end{verbatim}

\end{block}

\end{frame}

\begin{frame}

Plotting the fitted line (in blue):

\begin{center}\includegraphics[width=0.7\linewidth]{4Classif_files/figure-beamer/defaultglm-1} \end{center}

Default data: here \(\hat{\beta}_0=\) -10.65 and \(\hat{\beta}_1=\)
0.005.

\end{frame}

\begin{frame}

\begin{block}{Estimating the regression coefficients with ML}

\begin{itemize}
\item
  The coefficients \(\beta_0, \beta_1, \ldots\) are estimated with
  \emph{maximum likelihood} (ML).
\item
  We assume that pairs of covariates and responses
  \(\{{\bf x}_i, y_i\}\) are measured independently of each other. Given
  \(n\) such observation pairs, the likelihood function of a logistic
  regression model can be written as:
  \[L(\boldsymbol{\beta}) = \prod_{i=1}^n L_i(\boldsymbol{\beta}) = \prod_{i=1}^n f(y_i; \boldsymbol{\beta}) = \prod_{i=1}^n (p_i)^{y_i}(1-p_i)^{1-y_i},\]
  where
  \(\boldsymbol{\beta} = (\beta_0, \beta_1, \beta_2, \ldots, \beta_p)^T\)
  enters into \(p_i\)
\end{itemize}

\[p_i= \frac{\exp(\beta_0+\beta_1 x_{i1}+\cdots + \beta_p x_{ip})}{1 + \exp(\beta_0 + \beta_1 x_{i1}+\cdots+\beta_p x_{ip})} \ .\]

\end{block}

\end{frame}

\begin{frame}

\begin{itemize}
\item
  The maximum likelihood estimates are found by maximizing the
  likelihood.
\item
  To make the math easier, we usually work with the log-likelihood (the
  log is a monotone transform, thus it will give the same result as
  maximizing the likelihood).
\end{itemize}

\vspace{-4mm}

\begin{align*} \log(L(\boldsymbol{\beta}))&=l(\boldsymbol{\beta}) =\sum_{i=1}^n \Big ( y_i \log p_i + (1-y_i) \log(1 - p_i )\Big ) \\ &= \sum_{i=1}^n \Big ( y_i \log \Big (\frac{p_i}{1-p_i} \Big) + \log(1-p_i) \Big ) \\
&= \sum_{i=1}^n \Big (y_i (\beta_0 + \beta_1 x_{i1}+\cdots + \beta_p x_{ip}) - \log(1 + e^{\beta_0 + \beta_1 x_{i1}+\cdots + \beta_p x_{ip}} \Big ).\end{align*}

\end{frame}

\begin{frame}

\begin{itemize}
\item
  To maximize the log-likelihood function we find the \(r+1\) partial
  derivatives, and set equal to 0.
\item
  This gives us a set of \(p+1\) non-linear equations in the \(\beta\)s.
\item
  This set of equations does not have a closed form solution.
\item
  The equation system is therefore solved numerically using the
  \emph{Newton-Raphson algorithm} (or Fisher Scoring).
\end{itemize}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Qualitative interpretation of the coefficients}

\(~\)

Let's again look at the regression output:

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(glm_default)}\OperatorTok{$}\NormalTok{coef}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                  Estimate   Std. Error   z value      Pr(>|z|)
## (Intercept) -10.651330614 0.3611573721 -29.49221 3.623124e-191
## balance       0.005498917 0.0002203702  24.95309 1.976602e-137
\end{verbatim}

\normalsize

\begin{itemize}
\item
  The \(z\)-statistic is equal to \(\frac{\hat\beta}{SE(\hat\beta)}\),
  and is approximately \(N(0,1)\)
  distributed\footnote{With this knowledge we can construct confidence intervals and test hypotheses about the $\beta$s, with the aim to understand which covariate(s) contribute to our posterior probabilites and classification.}
\item
  The \(p\)-value is \(\text{Pr}(|Z| > |z|)\) for a \(Z\sim N(0,1)\)
  random variable
\item
  \texttt{Balance} seems an important predictor for \texttt{Default}
  (yes/no), with a higher balance leading to a higher probability of
  defaulting (\(\beta_1>0\)).
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Quantitative interpretation of the coefficients}

\vspace{2mm} Remember from equation (1) that

\begin{equation*}
\log\left( \frac{p_i}{1-p_i} \right) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_p x_{ip} \ ,
\end{equation*}

thus

\begin{equation*}
 \frac{p_i}{1-p_i} = e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_p x_{ip}} = e^{\eta_i} \ .
\end{equation*}

\vspace{2mm}

The quantity \(p_i/(1-p_i)\) is called the \emph{\textcolor{red}{odds}}.
Odds represent \emph{\textcolor{red}{chances}} (e.g.~in betting).

\(~\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  You think your football team will win tonight with a probability
  \(p=80\%\). What is the odds that it will win?
\item
  The odds for the best horse in a race is \(9:1\). What is the
  probability that this horse will win?
\end{enumerate}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Why is the odds relevant?}

\vspace{2mm} Let's again rearrange the \emph{odds} in the logistic
regression model:

\begin{align*}
 \frac{p_i}{1-p_i} = \text{odds}(Y_i=1 \mid X=x)  & = \frac{\text{P}(Y_i=1 \mid X=x)}{\text{P}(Y_i=0 \mid X = x)} \\
 &= \exp(\beta_0) \cdot \exp(\beta_1 x_{i1}) \cdot \ldots \cdot  \exp(\beta_p x_{ip}) \ .
 \end{align*}

\(~\)

\(\rightarrow\) We have a \emph{multiplicative model} for the odds -
which can help us to interpret our \(\beta\)s.

\end{block}

\end{frame}

\begin{frame}

\begin{block}{The odds ratio}

\(~\)

To understand the effect of a regression coefficient \(\beta_j\), let's
see what happens if we increase \(x_{ij}\) to \(x_{ij}+1\), while all
other covariates are kept fixed.

\vspace{2mm} Using simple algebra and the formula on the previous slide,
you will see that

\begin{equation}
\frac{\text{odds}(Y_i=1 \mid X_{j} = x_{ij} + 1)}{\text{odds}(Y_i=1 \mid X_j = x_{ij})} = \exp(\beta_j)  \ .
\end{equation}

\vspace{2mm}

\begin{center}
\colorbox{lightgray}{\begin{minipage}{11cm}
{\bf Interpretation}: 
By increasing covariate $x_{ij}$ by one unit, we change the odds ratio for $Y_i=1$ by a factor $\exp(\beta_j)$.
\end{minipage}}
\end{center}

\begin{center}
\colorbox{lightgray}{\begin{minipage}{11cm}
{\bf Moreover}:
Taking the log on equation (2), it follows that $\beta_j$ can be interpreted as a {\bf log odds-ratio}.
\end{minipage}}
\end{center}

\end{block}

\end{frame}

\begin{frame}[fragile]

Let's now fit the logistic regression model for \texttt{default}, given
\texttt{balance}, \texttt{income} and the binary variable
\texttt{student} as predictors:

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glm_default2 =}\StringTok{ }\KeywordTok{glm}\NormalTok{(default }\OperatorTok{~}\StringTok{ }\NormalTok{balance }\OperatorTok{+}\StringTok{ }\NormalTok{income }\OperatorTok{+}\StringTok{ }\NormalTok{student, }\DataTypeTok{data =}\NormalTok{ Default, }
    \DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{)}

\KeywordTok{summary}\NormalTok{(glm_default2)}\OperatorTok{$}\NormalTok{coef}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                  Estimate   Std. Error    z value      Pr(>|z|)
## (Intercept) -1.086905e+01 4.922555e-01 -22.080088 4.911280e-108
## balance      5.736505e-03 2.318945e-04  24.737563 4.219578e-135
## income       3.033450e-06 8.202615e-06   0.369815  7.115203e-01
## studentYes  -6.467758e-01 2.362525e-01  -2.737646  6.188063e-03
\end{verbatim}

\normalsize

\textbf{Questions:}

\begin{itemize}
\tightlist
\item
  What happens with the odds-ratio when \texttt{income} increases by
  10'000 dollars?
\end{itemize}

\begin{itemize}
\tightlist
\item
  What happens with the odds-ratio when \texttt{balance} increases by
  100 dollars?
\end{itemize}

\end{frame}

\begin{frame}

\begin{block}{Predictions}

\vspace{2mm}

Let's catch up. Why were we doing all this in the first place?

\vspace{2mm}

\textbf{Answer:} We wanted to build a model that \textbf{predict
probabilities of categories} \(Y\), \emph{given} certain covariates
\(X_1, \ldots, X_p\).

\vspace{4mm}

\begin{itemize}
\tightlist
\item
  For given parameter estimates
  \(\hat\beta_0, \hat\beta_1, \ldots \hat\beta_p\) and a new observation
  \({\bf x}_0\), we can estimate the probability \(\hat{p}({\bf x}_0)\)
  that the new observation belongs to the class defined by \(Y=1\)
\end{itemize}

\[\hat{p}({\bf x}_0) = \frac{e^{\hat{\eta}_0}}{1+e^{\hat{\eta}_0}} \ , \]

with linear predictor
\[\hat\eta_0 = \hat\beta_0 + \hat\beta_1 x_{01} + \ldots \hat\beta_p x_{0p} \ .\]

In the case of qualitative covariates, a dummy variable needs to be
introduced. This is done as for linear regression.

\end{block}

\end{frame}

\begin{frame}[fragile]

So in the \texttt{Default} example, we can predict the probability that
someone defaults.

For example: ``What is the estimated probability that a student defaults
with a balance of 2000, and an income of 40000?''

\[\hat{p}(X) = \frac{e^{\beta_0 + 2000 \cdot \beta_1 + 40000 \cdot \beta_2 + 1 \cdot \beta_3}}{ 1+  e^{\beta_0 + 2000 \cdot \beta_1 + 40000 \cdot \beta_2 + 1 \cdot \beta_3}} = 0.5196\]
\vspace{6mm}

Using R: \tiny

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eta <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(glm_default2)}\OperatorTok{$}\NormalTok{coef[, }\DecValTok{1}\NormalTok{] }\OperatorTok{%*%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2000}\NormalTok{, }\DecValTok{40000}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\KeywordTok{exp}\NormalTok{(eta)}\OperatorTok{/}\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\KeywordTok{exp}\NormalTok{(eta))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           [,1]
## [1,] 0.5196218
\end{verbatim}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Example: South African heart disease data set}

\vspace{2mm}

\begin{itemize}
\item
  \texttt{SAhert} data set from the \texttt{ElemStatLearn} package, a
  retrospective sample of males in a heart-disease high-risk region in
  South Africa.
\item
  462 observations on 10 variables.
\item
  All subjects are male in the age range 15-64.
\item
  160 cases (individuals who have suffered from a conorary heart
  disease) and 302 controls (individuals who have not suffered from a
  conorary heart disease).
\end{itemize}

\end{block}

\end{frame}

\begin{frame}[fragile]

The response value (\texttt{chd}) and covariates

\begin{itemize}
\tightlist
\item
  \texttt{chd} : conorary heart disease \{yes, no\} coded by the numbers
  \{1, 0\}
\item
  \texttt{sbp} : systolic blood pressure\\
\item
  \texttt{tobacco} : cumulative tobacco (kg)\\
\item
  \texttt{ldl} : low density lipoprotein cholesterol
\item
  \texttt{famhist} : family history of heart disease. Categorical
  variable with two levels: \{Absent, Present\}.
\item
  \texttt{obesity} : a numerical value
\item
  \texttt{alcohol} : current alcohol consumption
\item
  \texttt{age} : age at onset
\end{itemize}

The goal is to identify important risk factors. We start by loading and
looking at the data:

\end{frame}

\begin{frame}[fragile]

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ElemStatLearn)}

\NormalTok{d.heart <-}\StringTok{ }\NormalTok{SAheart}
\NormalTok{d.heart}\OperatorTok{$}\NormalTok{chd <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(d.heart}\OperatorTok{$}\NormalTok{chd)}
\NormalTok{d.heart <-}\StringTok{ }\NormalTok{d.heart[, }\KeywordTok{c}\NormalTok{(}\StringTok{"sbp"}\NormalTok{, }\StringTok{"tobacco"}\NormalTok{, }\StringTok{"ldl"}\NormalTok{, }\StringTok{"famhist"}\NormalTok{, }\StringTok{"obesity"}\NormalTok{, }
    \StringTok{"alcohol"}\NormalTok{, }\StringTok{"age"}\NormalTok{, }\StringTok{"chd"}\NormalTok{)]}
\KeywordTok{head}\NormalTok{(d.heart)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   sbp tobacco  ldl famhist obesity alcohol age chd
## 1 160   12.00 5.73 Present   25.30   97.20  52   1
## 2 144    0.01 4.41  Absent   28.87    2.06  63   1
## 3 118    0.08 3.48 Present   29.14    3.81  46   0
## 4 170    7.50 6.41 Present   31.99   24.26  58   1
## 5 134   13.60 3.50 Present   25.99   57.34  49   1
## 6 132    6.20 6.47 Present   30.77   14.14  45   0
\end{verbatim}

\normalsize

\end{frame}

\begin{frame}[fragile]

\texttt{pairs()} plot with \(Y=1\) (case) in green and \(Y=0\) (control)
in blue:

\begin{center}\includegraphics[width=1\linewidth]{4Classif_files/figure-beamer/ggpairs_heart-1} \end{center}

\end{frame}

\begin{frame}[fragile]

Fitting the model using all predictors in R:

\tiny

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glm_heart =}\StringTok{ }\KeywordTok{glm}\NormalTok{(chd }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ d.heart, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(glm_heart)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = chd ~ ., family = "binomial", data = d.heart)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.7517  -0.8378  -0.4552   0.9292   2.4434  
## 
## Coefficients:
##                  Estimate Std. Error z value Pr(>|z|)    
## (Intercept)    -4.1295997  0.9641558  -4.283 1.84e-05 ***
## sbp             0.0057607  0.0056326   1.023  0.30643    
## tobacco         0.0795256  0.0262150   3.034  0.00242 ** 
## ldl             0.1847793  0.0574115   3.219  0.00129 ** 
## famhistPresent  0.9391855  0.2248691   4.177 2.96e-05 ***
## obesity        -0.0345434  0.0291053  -1.187  0.23529    
## alcohol         0.0006065  0.0044550   0.136  0.89171    
## age             0.0425412  0.0101749   4.181 2.90e-05 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 596.11  on 461  degrees of freedom
## Residual deviance: 483.17  on 454  degrees of freedom
## AIC: 499.17
## 
## Number of Fisher Scoring iterations: 4
\end{verbatim}

\end{frame}

\begin{frame}

\begin{itemize}
\item
  What are important predictors for CHD?
\item
  How would you now calculate \(\hat{p}(X)\) that someone will develop
  CHD, given covariates \(X\)?
\end{itemize}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Multinomial logistic regression}

\begin{itemize}
\item
  Generalization of the logistic regression model to a response variable
  with more than two (\(K\)) possible classes.
\item
  The probability that \(Y\) belongs to class \(k\), given an
  observation vector \(\mathbf{x} = (x_1, x_2, \dots, x_p)^T\) is
  (usually) modelled by:
  \[\log \frac{\text{Pr}(Y = k | \mathbf{x})}{\text{Pr}(Y = K | \mathbf{x})}= \beta_{0k} + \beta_{1k} x_1 + \cdots + \beta_{pk} x_p ,\]
  where \(K\) is chosen as the \emph{reference category} (arbitrary). We
  would thus (jointly) estimate \(K-1\) sets of regression parameters.
\item
  The multinomial logistic regression model is implemented in the
  \texttt{glmnet} or \texttt{VGAM} package in \texttt{R}.
\item
  We will not discuss this further since LDA is more popular (than
  logistic regression) in the multi-class setting. And, as we shall see
  soon - they are not that different.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Bayes classifier}

\begin{itemize}
\tightlist
\item
  We were going for classification, but now estimated
  \(\hat{p}(X)=\text{Pr}(Y \mid X)\) using logistic regression. Idea:
  probability can be used for classification.
\end{itemize}

\vspace{1mm}

\begin{itemize}
\item
  Assume that we know or can estimate the probability that a new
  observation \(x_0\) belongs to class \(k\), for \(K\) classes
  \(\mathcal{C} = \{c_1, c_2,\ldots, c_K\}\), with elements numbered as
  \(1, 2, ..., K\)
  \[p_k(x_0) = \text{Pr}(Y=k | X=x_0), \quad k = 1, 2, ... K \ .\] This
  is the probability that \(Y=k\) given the observation \(x_0\).
  \vspace{1mm}
\item
  The \emph{Bayes classifier assigns an observation to the most likely
  class}, given its predictor values.
\end{itemize}

\vspace{1mm}

\begin{itemize}
\tightlist
\item
  \textbf{Example} for two groups \(\{A, B\}\). A new observation
  \(x_0\) will be classified to \(A\) if
  \(\text{Pr}(Y=A | X=x_0) > 0.5\) and to class \(B\) otherwise.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Properties of the Bayes classifier}

\begin{itemize}
\item
  It has the \emph{smallest test error rate}.
\item
  The class boundaries using the Bayes classifier is called the
  \emph{Bayes decision boundary}.
\item
  The overall Bayes error rate is given as
  \[1-\text{E}(\max_j \text{Pr}(Y=j\mid X))\] where the expectation is
  over \(X\).
\item
  The Bayes error rate is comparable to the \emph{irreducible error} in
  the regression setting.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{What is test error?}

\vspace{2mm}

To understand what test error means, we need to understand the terms

\begin{itemize}
\tightlist
\item
  \emph{Training set}
\item
  \emph{Test set}
\item
  \emph{Training error}
\item
  \emph{Test error}
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\textbf{Training set:} observations (independent pairs)
\(\{(x_1, y_1), ..., (x_n, y_n)\}\) where the response variable \(Y\) is
\emph{qualitative} and labelled \(1, 2, ..., K\).

The training set is used to construct the classification rule (by
estimating parameters in class densities or posterior probabilites).

\vspace{2mm}

\textbf{Test set:} observations (independent pairs), same format as the
training set.

The test set is used to evaluate the classification rule.

\vspace{2mm}

\textbf{Loss function:} The misclassifications are given the loss 1 and
the correct classifications loss 0 - this is called \emph{0/1-loss}.

\end{frame}

\begin{frame}

\begin{block}{Training error}

\begin{itemize}
\tightlist
\item
  \textbf{Training error rate}: The proportion of mistakes that are made
  if we apply our estimator \(\hat{f}\) to the training observations,
  i.e. \(\hat{y}_i=\hat{f}(x_i)\)
  \[\frac{1}{n}\sum_{i=1}^n \text{I}(y_i \neq \hat{y}_i) \ ,\] with
  indicator function I, which is defined as:
\end{itemize}

\[\text{I}(a\neq\hat{a}) = \begin{cases} 1 \text{ if } a \neq \hat{a} \ , \\ 
0 \text{ else. } \end{cases}\]

\begin{itemize}
\item
  The indicator function counts the number of times our model has made a
  wrong classification. The training error rate is the fraction of
  misclassifications made on our training set.
\item
  A very low training error rate may imply overfitting.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Test error}

\begin{itemize}
\item
  \textbf{Test error rate}: The fraction of misclassifications when our
  model is applied on a test set
  \[\text{Ave}(I(y_0\neq \hat{y}_0)) \ ,\] where the average is over all
  the test observations \((x_0,y_0)\).
\item
  Again, this gives a better indication of the true performance of the
  classifier (than the training error).
\item
  We assume that a \emph{good} classifier is a classifier that has a
  \emph{low} test error.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{The K-nearest neighbour (KNN) classifier}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  Caveat: We usually don't know the conditional distribution of \(Y\)
  given \(X\) for real data.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  We have discussed how to do it with logistic regression (for two
  categories).
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  The \(K\)-nearest neighbor (KNN) classifier estimates this conditional
  distribution and chooses the most likely cateogry (Bayes
  classifier).\footnote{Attention!! $K$ refers to the number of neighbours used for the classifier, and \emph{not} to the number of classes!! The latter is assumed known.}
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\vspace{2mm} The \(K\)-nearest neighbour classifier (KNN) works in the
following way:

\begin{itemize}
\tightlist
\item
  Given a new observation \(x_0\) it searches for the \(K\) points in
  our training data that are closest to it (Euclidean distance).
\item
  These points make up the neighborhood of \(x_0\), \(\mathcal{N}_0\).
\item
  The point \(x_0\) is classified by taking a majority vote of the
  neighbors.
\item
  That means that \(x_0\) is classified to the most occurring class
  among its neighbors (``majority vote'')
  \[\text{Pr}(Y=j | X = x_0) = \frac{1}{K} \sum_{i \in \mathcal{N}_0} I(y_i = j)\ .\]
\end{itemize}

\end{frame}

\begin{frame}

\begin{block}{A synthetic example}

\begin{itemize}
\item
  Simulate \(2\times 100\) observations from a bivariate normal
  distribution with mean vectors \(\mu_A = (1, 1)^T\),
  \(\mu_B = (3, 3)^T\), and covariance matrix
  \(\Sigma_A = \Sigma_B = \begin{pmatrix} 2\hspace{2mm} 0 \\ 0 \hspace{2mm} 2 \end{pmatrix}\).
\item
  Aim: Find a rule to classify a new observation to \(A\) or \(B\).
\end{itemize}

\begin{center}\includegraphics[width=0.55\linewidth]{4Classif_files/figure-beamer/knn-1} \end{center}

\end{block}

\end{frame}

\begin{frame}

\begin{itemize}
\tightlist
\item
  Assume we have a new observation \(X_0 = (x_{01}, x_{02})^T\) which we
  want to classify as belonging to the class \(A\) or \(B\).
\item
  To illustrate this problem we fit the \(K\)-nearest neighbor
  classifier to our simulated data set with \(K = 1, 3, 10\) and \(150\)
  (next slide).
\end{itemize}

\vspace{4mm}

Interpretation:

\begin{itemize}
\tightlist
\item
  The small colored dots show the predicted classes for an evenly-spaced
  grid.
\item
  The lines show the decision boundaries.
\item
  If our new observation falls into the region within the red decision
  boundary, it will be classified as \(A\). If it falls into the region
  within the green decision boundary, it will be classified as \(B\).
\end{itemize}

\end{frame}

\begin{frame}

\includegraphics{4Classif_files/figure-beamer/unnamed-chunk-7-1.pdf}

\end{frame}

\begin{frame}

\begin{itemize}
\item
  The choice of \(K\) has a big influence on the result of our
  classification. For \(K=1\) the classification is made to the same
  class as the one nearest neighbor. As \(K\) gets very large, the
  decision boundary tends towards a straight line (which is the Bayes
  boundary in this set-up).
\item
  To find the optimal value of \(K\) the typical procedure is to try
  different values of \(K\) and then test the predictive power of the
  different classifiers, for example by cross-validation, which will be
  discussed in M5.
\item
  After trying all choices for \(K\) between 1 and 50, a few choices of
  \(K\) gave the smallest misclassification error rate, estimating by
  leave-one out cross-validation (see M5). The smallest error rate is
  equal to 0.165 (16.5\% misclassification).
\end{itemize}

\end{frame}

\begin{frame}

\begin{center}\includegraphics[width=0.9\linewidth]{4Classif_files/figure-beamer/knnerror1-1} \end{center}

\end{frame}

\begin{frame}

\begin{block}{Bias-variance trade-off in a classification setting}

\vspace{2mm} Remember the bias-variance trade-off that we discussed in
Module 2. \vspace{2mm}

Considerations: \vspace{2mm}

\begin{itemize}
\item
  A too low value of \(K\) will give a very flexible classifier (with
  high variance and low bias) which will fit the training set too well
  (it will overfit) and make poor predictions for new observations.
\item
  Choosing a high value for \(K\) makes the classifier loose its
  flexibility and the classifier will have low variance but high bias.
\end{itemize}

\vspace{2mm}

\(\rightarrow\) Critical to the success of the classifier to choose the
correct level of flexibility (\(K\)).

\end{block}

\end{frame}

\begin{frame}

\begin{block}{The curse of dimensionality}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  The nearest neighbor classifier can be quite good if the number of
  predictors \(p\) is small and the number of observations \(n\) is
  large. We need enough close neighbors to make a good classification.
\end{itemize}

\vspace{1mm}

\begin{itemize}
\tightlist
\item
  The effectiveness of the KNN classifier falls quickly when the
  dimension of the preditor space is high.
\end{itemize}

\vspace{1mm}

\begin{itemize}
\tightlist
\item
  Why? Because the nearest neighbors tend to be far away in high
  dimensions and the method is no longer local. This is referred to as
  the \emph{curse of dimensionality}.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}{Bayes decision rule - two paradigms}

\vspace{1mm}

Two approaches to estimate \(\text{Pr}(Y=k \mid X=x)\): \vspace{1mm}

\begin{block}{\textbf{Diagnostic paradigm}}

This is what we did so far: The focus was \emph{directly} estimating the
posterior distribution for the classes \[\text{Pr}(Y=k \mid X=x)\ .\]
\textbf{Examples:} Logistic regression. KNN classification.

\vspace{2mm}

\end{block}

\begin{block}{\textbf{Sampling paradigm}}

\begin{itemize}
\tightlist
\item
  The focus is on estimating the prior probabilities
  \(\pi_k=\text{Pr}(Y=k)\) for the classes and the class conditional
  distributions \(f_k(x)=\text{Pr}(X=x \mid Y=k)\).
\item
  We classify to the class with the maximal product \(\pi_k f_k(x)\). In
  this paradigm, we need to model the pdf for each class.
\item
  Popular: the multivariate normal distribution!
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

Given a continuous \(X\) and categorical \(Y\), and

\begin{itemize}
\tightlist
\item
  the probability \emph{\textcolor{red}{density}} function
  \(f_k(x) = \text{Pr}(X=x \mid Y=k)\) for \(X\) in class \(k\).
\item
  the \emph{\textcolor{red}{prior}} proability for class \(k\)
  \(\pi_k = \text{Pr}(Y=k)\) is the prior probability.
\end{itemize}

How do we get \(\text{Pr}(Y=k | X=x_0)\)?

\begin{center}
\colorbox{lightgray}{\begin{minipage}{10cm}
{\bf Bayes theorem}
\begin{align*}
p_k(X) = \text{Pr}(Y=k \mid X= x) &= 
\frac{\text{Pr}({\bf X}={\bf x} \cap Y=k)}{f({\bf x})}\\
&= \frac{ f_k(x) \pi_k}{\sum_{l=1}^K  f_l(x) \pi_l}  
\end{align*}
\end{minipage}}
\end{center}

\end{frame}

\begin{frame}{Discriminant Analysis}

\vspace{2mm} Discriminant analysis is relying on the \emph{sampling
paradigm}.

\vspace{2mm}

\begin{itemize}
\item
  The approach is to model the distribution of \(X\) in each of the
  classes separately, and then use Bayes theorem to flip things around
  and obtain \(\text{Pr}(Y \mid X)\).
\item
  When we use normal (Gaussian) distributions for each class, this leads
  to linear or quadratic discriminant analysis.
\item
  However, this approach is quite general, and other distributions can
  be used as well. We will focus on normal distributions.
\end{itemize}

\end{frame}

\begin{frame}

\begin{block}{Classification to the highest density}

\vspace{2mm}

Suppose we have observations coming from two classes:

\{\textcolor{green}{green}, \textcolor{orange}{orange}\}

\[X_{\text{green}}\sim \mathcal{N}(-2, 1.5^2) \text{ and }
X_{\text{orange}}\sim \mathcal{N}(2, 1.5^2) \]

\begin{itemize}
\tightlist
\item
  Assume probabilities to be equal, \(\pi_1 = \pi_2 = 0.5\).
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

We plot \(\pi_k f_k(x)\) for the two classes:

\begin{center}\includegraphics[width=0.7\linewidth]{4Classif_files/figure-beamer/gauss1-1} \end{center}

The decision boundary is where the point of intersection of the two
lines is, because here \(\pi_1 f_1(x)=\pi_2 f_2(x)\). Thus all points to
left of the decision boundary will be classified as green and similarly,
all points to the right of the decision boundary will be classified as
orange.

\end{frame}

\begin{frame}

We now specify different priors, such that \(\pi_1 = 0.3\) and
\(\pi_2 = 0.7\). We see that this has affected the decision boundary,
which now has shifted to the left:

\begin{center}\includegraphics[width=0.7\linewidth]{4Classif_files/figure-beamer/gauss2-1} \end{center}

\end{frame}

\begin{frame}

\begin{block}{Why discriminant analysis?}

\begin{itemize}
\item
  When the classes are well-separated, the parameter estimates for the
  logistic regression model are surprisingly unstable. Linear
  discriminant analysis does not suffer from this problem.
\item
  If \(n\) is small and the distribution of the predictors \(X\) is
  approximately normal in each of the classes, the linear discriminant
  model is again more stable than the logistic regression model.
\item
  Linear discriminant analysis is popular when we have more than two
  response classes, because it also provides low-dimensional views of
  the data.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}{Linear discriminant analysis (LDA) when \(p=1\)}

\begin{itemize}
\item
  Assume there is only one predictor \(X\) (\(p=1\)).
\item
  Class conditional distributions \(f_k(X)\) are assumed normal
  (Gaussian) for \(k=1,\ldots,K\), that is
  \[f_k(x) = \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{1}{2}\big(\frac{x-\mu_k}{\sigma_k}\big)^2} \]
  has parameters \(\mu_k\) (mean) and \(\sigma_k\) (standard deviation).
\item
  With LDA we assume that all of the classes have the \emph{same
  standard deviation} \(\sigma_k = \sigma\).
\item
  In addition we have prior class probabilites \(\pi_k=\text{Pr}(Y=k)\),
  so that \(\sum_{k=1}^K \pi_k=1\).
\end{itemize}

\end{frame}

\begin{frame}

We can insert the expression for each class distribution into Bayes
formula to obtain the posterior probability
\(p_k(x) = \text{Pr}(Y = k | X = x)\)
\[p_k(x) = \frac{f_k({\bf x}) \pi_k}{f({\bf x})}=\frac{\pi_k \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{1}{2}\big(\frac{x-\mu_k}{\sigma}\big)^2}}{\sum_{l=1}^K \pi_l \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{1}{2}\big(\frac{x-\mu_l}{\sigma}\big)^2}} \ .\]

Our rule is to classify to the class for which \(p_k(x)\) is largest.

\end{frame}

\begin{frame}

Taking logs, and discarding terms that do not depend on \(k\), we see
that this is equivalent to assigning \(x\) to the class with the largest
\emph{discriminant score} \(\delta_k(x)\):

\[\delta_k(x) = x\cdot \frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2 \sigma^2}+\log(\pi_k).\]

\begin{itemize}
\item
  This decision boundaries between the classes are \emph{linear} in
  \(x\).
\item
  For \(K=2\) classes and \(\pi_1=\pi_2\), the decision boundary is at
\end{itemize}

\[x = \frac{\mu_1+ \mu_2}{2} \ .\]

(Show this by setting \(\delta_1(x)=\delta_2(x)\) and resolving for
\(x\).)

\end{frame}

\begin{frame}

\begin{block}{Back to our example}

\[X_{\text{green}}\sim \mathcal{N}(-2, 1.5^2) \text{ and }
X_{\text{orange}}\sim \mathcal{N}(2, 1.5^2) \]

\begin{center}\includegraphics[width=0.6\linewidth]{4Classif_files/figure-beamer/gauss3-1} \end{center}

\begin{itemize}
\item
  Where is the decision boundary?
\item
  How can we calculate the error rate?
\item
  Is it possible to construct a classifier that has a lower error rate
  than the Bayes error rate?
\end{itemize}

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{center}\includegraphics[width=0.6\linewidth]{4Classif_files/figure-beamer/gauss4-1} \end{center}

\begin{itemize}
\item
  The Bayes decision boundary is at \(x=0\), and we would make a (Bayes)
  error of \texttt{round(2*0.5*pnorm(0,2,1.5))}=0.09.
\item
  The Bayes classifier has the lowest test error rate.
\end{itemize}

\end{frame}

\begin{frame}

\vspace{2mm}

In the above example we knew the true distributions \(p_k(X)\) and the
priors \(\pi_k\). But typically we don't know these parameters, we only
have the training data.

Idea: we simply estimate the parameters and plug them into the rule.

\end{frame}

\begin{frame}

\begin{block}{Parameter estimators}

\vspace{2mm}

\begin{itemize}
\item
  Prior probability for class \(k\) is (often) estimated by taking the
  fraction of observations \(n_k\) (out of \(n\)) coming from class
  \(k\): \(\hat{\pi}_k = \frac{n_k}{n}.\)
\item
  The mean value for class \(k\) is simply the sample mean of all
  observations from class \(k\):
  \[\hat{\mu}_k = \frac{1}{n_k}\sum_{i:y_i=k} x_i.\]
\item
  The standard deviation: sample standard deviation across all classes:
  \[\hat{\sigma}^2=\frac{1}{n-K}\sum_{k=1}^K \sum_{i: y_i=k} (x_i-\hat{\mu}_k)^2 = \sum_{k=1}^K \frac{n_k - 1}{n - K} \cdot \hat{\sigma}_k^2.\]
  \(\hat{\sigma}_k\): estimated standard deviation of all observations
  from class \(k\).
\end{itemize}

\textbf{How can we then estimate the goodness of our estimator?}

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use the training set to estimate parameters and class boundary:
\item
  Use the test set to estimate misclassification rate.
\end{enumerate}

Simulate data and use training data to estimate \(\mu_k\) and
\(\hat\sigma^2\):

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n =}\StringTok{ }\DecValTok{1000}
\NormalTok{pi1 =}\StringTok{ }\NormalTok{pi2 =}\StringTok{ }\FloatTok{0.5}
\NormalTok{mu1 =}\StringTok{ }\DecValTok{-2}
\NormalTok{mu2 =}\StringTok{ }\DecValTok{2}
\NormalTok{sigma =}\StringTok{ }\FloatTok{1.5}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{n1train =}\StringTok{ }\KeywordTok{rbinom}\NormalTok{(}\DecValTok{1}\NormalTok{, n, pi1)}
\NormalTok{n2train =}\StringTok{ }\NormalTok{n }\OperatorTok{-}\StringTok{ }\NormalTok{n1train}
\NormalTok{n1test =}\StringTok{ }\KeywordTok{rbinom}\NormalTok{(}\DecValTok{1}\NormalTok{, n, pi1)}
\NormalTok{n2test =}\StringTok{ }\NormalTok{n }\OperatorTok{-}\StringTok{ }\NormalTok{n1test}
\NormalTok{train1 =}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n1train, mu1, sigma)}
\NormalTok{train2 =}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n2train, mu2, sigma)}
\NormalTok{test1 =}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n1test, mu1, sigma)}
\NormalTok{test2 =}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n2test, mu2, sigma)}
\NormalTok{var2}\FloatTok{.1}\NormalTok{ =}\StringTok{ }\KeywordTok{var}\NormalTok{(train1)}
\NormalTok{var2}\FloatTok{.2}\NormalTok{ =}\StringTok{ }\KeywordTok{var}\NormalTok{(train2)}
\NormalTok{var.pool =}\StringTok{ }\NormalTok{((n1train }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{var2}\FloatTok{.1} \OperatorTok{+}\StringTok{ }\NormalTok{(n2train }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{var2}\FloatTok{.2}\NormalTok{)}\OperatorTok{/}\NormalTok{(n }\OperatorTok{-}\StringTok{ }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{frame}

\begin{frame}[fragile]

Then set

\[\hat\delta_1(x) = \hat\delta_2(x)\]

and resolve for \(x\) to obtain a decision rule (boundary).

\vspace{2mm} Todo: Verify that the following code will give you the
training and test error rates:

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rule =}\StringTok{ }\FloatTok{0.5} \OperatorTok{*}\StringTok{ }\NormalTok{(}\KeywordTok{mean}\NormalTok{(train1) }\OperatorTok{+}\StringTok{ }\KeywordTok{mean}\NormalTok{(train2)) }\OperatorTok{+}\StringTok{ }\NormalTok{var.pool }\OperatorTok{*}\StringTok{ }\NormalTok{(}\KeywordTok{log}\NormalTok{(n2train}\OperatorTok{/}\NormalTok{n) }\OperatorTok{-}\StringTok{ }
\StringTok{    }\KeywordTok{log}\NormalTok{(n1train}\OperatorTok{/}\NormalTok{n))}\OperatorTok{/}\NormalTok{(}\KeywordTok{mean}\NormalTok{(train1) }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(train2))}

\KeywordTok{c}\NormalTok{((}\KeywordTok{sum}\NormalTok{(train1 }\OperatorTok{>}\StringTok{ }\NormalTok{rule) }\OperatorTok{+}\StringTok{ }\KeywordTok{sum}\NormalTok{(train2 }\OperatorTok{<}\StringTok{ }\NormalTok{rule))}\OperatorTok{/}\NormalTok{n, (}\KeywordTok{sum}\NormalTok{(test1 }\OperatorTok{>}\StringTok{ }\NormalTok{rule) }\OperatorTok{+}\StringTok{ }\KeywordTok{sum}\NormalTok{(test2 }\OperatorTok{<}\StringTok{ }
\StringTok{    }\NormalTok{rule))}\OperatorTok{/}\NormalTok{n)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.105 0.115
\end{verbatim}

\normalsize

This is a rather good performance, compared to the minimal Bayes error
rate. But keep in mind that we LDA classifier relies on the Normal
assumption, and that \(\sigma_k=\sigma\) for all classes is assumed.

\end{frame}

\begin{frame}

\begin{block}{The confusion matrix}

\vspace{2mm}

\begin{itemize}
\item
  The confusion matrix is a table that can show the performance of a
  classifier, given that the true values are known.
\item
  We can make a confusion matrix from the training or test set, but will
  in most cases do so for the test set only.
\item
  The rows represent the true classes, while the columns represent the
  predicted classes. Inside the table we have counts (just labelled
  ``correct'' and ``wrong'' below - but should be numbers).
\item
  The sum of the diagonal is the total number of correct
  classifications. The sum of all elements off the diagonal is the total
  number of misclassifications.
\end{itemize}

\begin{center}

\begin{tabular}{l|l|l|l|l}
\hline
 & Predicted 1 & Predicted 2 & ... & Predicted K\\
\hline
True 1 & correct & wrong & ... & wrong\\
\hline
True 2 & wrong & correct & ... & wrong\\
\hline
... & ... & ... & ... & ...\\
\hline
True K & wrong & wrong & ... & correct\\
\hline
\end{tabular}
\end{center}

\end{block}

\end{frame}

\begin{frame}[fragile]

The confusion matrix can be obtained in \texttt{R} by using the
\texttt{table} function with the predicted classes and the true classes
need to be given as function arguments, or directly using the
\texttt{caret} package.

We will soon come back to the special case of two classes - where we may
think of this as ``-'' (non disease) and ``+'' (disease).

\end{frame}

\begin{frame}

\begin{block}{Multivariate LDA (\(p>1\))}

\vspace{2mm}

\begin{itemize}
\item
  So far: Only one variable \(X\) (\(p=1\)) as predictor for \(Y\).
\item
  LDA can be generalized to situations when \(p>1\) covariates are used.
  The decision boundaries are still linear.
\item
  The multivariate normal distribution function:
  \[f(x) = \frac{1}{(2 \pi)^{p/2}|\boldsymbol{\Sigma}|^{1/2}}\exp({-\frac{1}{2}({\bf x}-\boldsymbol\mu)^T \boldsymbol{\Sigma}^{-1}({\bf x}-\boldsymbol\mu)})\]
\end{itemize}

\centering
\includegraphics[width=0.70000\textwidth]{../../ISLR/Figures/Chapter4/4.5.png}

\end{block}

\end{frame}

\begin{frame}

\begin{itemize}
\item
  This gives the following expression for the discriminant function:
  \[\delta_k(x) = {\bf x}^T \boldsymbol{\Sigma}^{-1}\boldsymbol\mu_k - \frac{1}{2}\boldsymbol\mu_k^T \boldsymbol{\Sigma}^{-1}\boldsymbol\mu_k + \log \pi_k.\]
\item
  Note: \(\delta_k(x) = c_{k0} + c_{k1}x_1 + \ldots + c_{kp}x_p\) is a
  linear function in \((x_1,\ldots ,x_p)\).
\end{itemize}

\end{frame}

\begin{frame}

Some details on the derivation of the discriminant function - when
\(K=2\) classes, and the classes are denoted \(0\) and \(1\)

\[\text{Pr}(Y=0 | X={\bf x}) = \text{Pr}(Y=1 | X={\bf x})\]
\[\frac{\pi_0f_0(x)}{\pi_0f_0(x)+\pi_1f_1(x)} = \frac{\pi_1f_0(1)}{\pi_0f_0(x)+\pi_1f_1(x)} \]
\[\pi_0 f_0({\bf x}) = \pi_1 f_1({\bf x})\]
\[\pi_0\frac{1}{(2 \pi)^{p/2}|\boldsymbol{\Sigma}|^{1/2}}
e^{\frac{1}{2}({\bf x}-\boldsymbol{\mu}_0)^T \boldsymbol{\Sigma}^{-1}({\bf x}-\boldsymbol{\mu}_0)} =  \pi_1\frac{1}{(2 \pi)^{p/2}|\boldsymbol{\Sigma}|^{1/2}}e^{\frac{1}{2}({\bf x}-\boldsymbol{\mu}_1)^T \boldsymbol{\Sigma}^{-1}({\bf x}-\boldsymbol{\mu}_1)} \]
\[\log(\pi_0) -\frac{1}{2}({\bf x}-\boldsymbol{\mu}_0)^T \boldsymbol{\Sigma}^{-1}({\bf x}-\boldsymbol{\mu}_0) = 
\log(\pi_1) -\frac{1}{2}({\bf x}-\boldsymbol{\mu}_1)^T \boldsymbol{\Sigma}^{-1}({\bf x}-\boldsymbol{\mu}_1) \]
\[\log(\pi_0)  -\frac{1}{2}{\bf x}^T\boldsymbol{\Sigma}^{-1}{\bf x} + {\bf x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_0 - \frac{1}{2}\boldsymbol{\mu}_0^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_0 = \log(\pi_1)  -\frac{1}{2}{\bf x}^T\boldsymbol{\Sigma}^{-1}{\bf x} + {\bf x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1 - \frac{1}{2}\boldsymbol{\mu}_1^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1 \]
\[\log(\pi_0) + {\bf x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_0 - \frac{1}{2}\boldsymbol{\mu}_0^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_0 = 
\log(\pi_1) + {\bf x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1 - \frac{1}{2}\mu_1^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1\]
\[\delta_0({\bf x}) = \delta_1({\bf x}) \]

\end{frame}

\begin{frame}

\begin{block}{Back to our synthetic example}

\begin{itemize}
\item
  Consider again our simulateion from a bivariate normal distribution
  with mean vectors \(\mu_A = (1, 1)^T\), \(\mu_B = (3, 3)^T\), and
  covariance matrix
  \(\Sigma_A = \Sigma_B = \begin{pmatrix} 2\hspace{2mm} 0 \\ 0 \hspace{2mm} 2 \end{pmatrix}\).
\item
  Aim: Use LDA to classify a new observation \(x_0\) to class \(A\) or
  \(B\).
\end{itemize}

\begin{center}\includegraphics[width=0.6\linewidth]{4Classif_files/figure-beamer/synthAB2-1} \end{center}

\end{block}

\end{frame}

\begin{frame}

\begin{itemize}
\item
  Since the truth is known here we can calculate the Bayes boundary and
  the Bayes error.
\item
  Since we have bivariate normal class distributions with common
  covariance matrix, the optimal boundary is given by LDA, with boundary
  given at \(\delta_A({\bf x})=\delta_B({\bf x})\).
\end{itemize}

\[{\bf x}^T \boldsymbol{\Sigma}^{-1}\boldsymbol\mu_A - \frac{1}{2}\boldsymbol\mu_A^T \boldsymbol{\Sigma}^{-1}\boldsymbol\mu_A + \log \pi_A={\bf x}^T \boldsymbol{\Sigma}^{-1}\boldsymbol\mu_B - \frac{1}{2}\boldsymbol\mu_B^T \boldsymbol{\Sigma}^{-1}\boldsymbol\mu_B + \log \pi_B\]

\[{\bf x}^T\boldsymbol{\Sigma}^{-1}(\boldsymbol\mu_A -\boldsymbol\mu_B)-\frac{1}{2}\boldsymbol\mu_A^T \boldsymbol{\Sigma}^{-1}\boldsymbol\mu_A +\frac{1}{2}\boldsymbol\mu_B^T \boldsymbol{\Sigma}^{-1}\boldsymbol\mu_B +\log \pi_A-\log \pi_B=0\]

Inserting numerical values gives \(-x_1-x_2+4=0\), thus a boundary with
functional form \[x_2=4-x_1 \ .\]

\end{frame}

\begin{frame}[fragile]

\begin{block}{Confusion matrix for the synthetic example}

\vspace{2mm}

We can use the Bayes boundary

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r.pred <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(df}\OperatorTok{$}\NormalTok{X2 }\OperatorTok{<}\StringTok{ }\DecValTok{4} \OperatorTok{-}\StringTok{ }\NormalTok{df}\OperatorTok{$}\NormalTok{X1, }\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{)}
\KeywordTok{table}\NormalTok{(}\DataTypeTok{real =}\NormalTok{ df}\OperatorTok{$}\NormalTok{class, r.pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     r.pred
## real  A  B
##    A 82 18
##    B 21 79
\end{verbatim}

However, again, the Bayes boundary is usually not known, and we must
estimate it from the data.

\end{block}

\end{frame}

\begin{frame}

\textbf{Estimators for p\textgreater{}1:}

\begin{itemize}
\item
  Prior probability for class \(k\) (unchanged from \(p=1\)):
  \(\hat{\pi}_k = \frac{n_k}{n}.\)
\item
  The mean value for class \(k\) is simply the sample mean of all
  observations from class \(k\) (but now these are vectors):
  \[\hat{\boldsymbol{\mu}}_k = \frac{1}{n_k}\sum_{i:y_i=k} {\bf X}_i.\]
\item
  The covariance matrices for each class:
  \[\hat{\boldsymbol{\Sigma}}_k=\frac{1}{n_k-1}\sum_{i:y_i=k} ({\bf X}_i-\hat{\boldsymbol{\mu}}_k ) ({\bf X}_i-\hat{\boldsymbol{\mu}}_k)^T\]
\item
  Pooled version:
  \[\hat{\boldsymbol{\Sigma}}= \sum_{k=1}^K \frac{n_k - 1}{n - K} \cdot \hat{\boldsymbol{\Sigma}}_k.\]
\end{itemize}

Optional:
\href{https://www.math.ntnu.no/emner/TMA4268/2018v/notes/ProofMeanS.pdf}{Proof
that the estimator \(\hat{\boldsymbol{\Sigma}}_k\) is unbiased for each
class (from TMA4267)}. Proof for pooled version not provided.

\end{frame}

\begin{frame}[fragile]

\begin{block}{Analysing the synthetic example with \texttt{lda()}}

\vspace{2mm}

To obtain parameters estimates and an estimated decision boundary, we
can use the \texttt{lda()} function in R.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r.lda <-}\StringTok{ }\KeywordTok{lda}\NormalTok{(class }\OperatorTok{~}\StringTok{ }\NormalTok{X1 }\OperatorTok{+}\StringTok{ }\NormalTok{X2, df)}
\NormalTok{r.pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(r.lda, df)}\OperatorTok{$}\NormalTok{class}
\KeywordTok{table}\NormalTok{(}\DataTypeTok{real =}\NormalTok{ df}\OperatorTok{$}\NormalTok{class, }\DataTypeTok{predicted =}\NormalTok{ r.pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     predicted
## real  A  B
##    A 87 13
##    B 18 82
\end{verbatim}

Note: The training error is smaller than for the Bayes boundary. Why?

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Comparison of the Bayes with the estimated boundary}

\vspace{2mm}

Solid line: Bayes boundary

Dashed line: Estimated boundary

\begin{center}\includegraphics[width=0.6\linewidth]{4Classif_files/figure-beamer/synthAB3-1} \end{center}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Posterior probabilites}

\vspace{2mm}

Sometimes the probability that an observation comes from a class \(k\)
is more interesting than the actual classification itself. These class
probabilities can be estimated from the priors and class conditional
distributions, or from the discriminant functions:

\begin{align*}\hat{P}(Y=k | X=\boldsymbol{x})&=
\frac{\hat{\pi}_k \cdot \frac{1}{(2 \pi)^{p/2}|\hat{\boldsymbol{\Sigma}}|^{1/2}} \exp(-\frac{1}{2}
(\boldsymbol{x}-\hat{\boldsymbol\mu}_k)^T \hat{\boldsymbol{\Sigma}}^{-1}
(\boldsymbol{x}-\hat{\boldsymbol\mu}_k))}
{\sum_{l=1}^K \hat{\pi}_l 
\frac{1}{(2 \pi)^{p/2}|\hat{\boldsymbol{\Sigma}}|^{1/2}}
\exp(-\frac{1}{2}
(\boldsymbol{x}-\hat{\boldsymbol\mu}_l)^T 
\hat{\boldsymbol{\Sigma}}^{-1}
(\boldsymbol{x}-\hat{\boldsymbol\mu}_l))}\\
&=
\frac{e^{\hat{\delta}_k(\boldsymbol{x})}}{\sum_{l=1}^K e^{\hat{\delta}_l(\boldsymbol{x})}}.\end{align*}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Quadratic Discriminant Analysis (QDA)}

\begin{itemize}
\item
  In LDA we assumed that \(\boldsymbol{\Sigma}_k = \boldsymbol{\Sigma}\)
  for all classes.
\item
  In QDA we allow different covariance matrices
  \(\boldsymbol{\Sigma}_k\) for each class, while the predictors are
  still multivariate Gaussian
\end{itemize}

\[X \sim N(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \ .\]

\begin{itemize}
\tightlist
\item
  The discriminant functions are now given by:

  \begin{align*} \delta_k({\boldsymbol x}) &= -\frac{1}{2}({\boldsymbol x}-\boldsymbol{\mu}_k)^T \boldsymbol{\Sigma}_k^{-1}({\boldsymbol x}-\boldsymbol{\mu}_k)-\frac{1}{2}\log |\boldsymbol{\Sigma}_k| + \log \pi_k \\ 
  &= -\frac{1}{2} {\boldsymbol x}^T \boldsymbol{\Sigma}_k^{-1}{\boldsymbol x} + {\boldsymbol x}^T \boldsymbol{\Sigma}_k^{-1}\boldsymbol{\mu}_k - \frac{1}{2} \boldsymbol{\mu}_k^T \boldsymbol{\Sigma}_k^{-1}\boldsymbol{\mu}_k - \frac{1}{2}\log |\boldsymbol{\Sigma}_k | + \log \pi_k.
  \end{align*}
\item
  These decision boundaries are \emph{quadratic} functions of
  \({\boldsymbol x}\).
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{LDA vs QDA}

\vspace{2mm}

QDA is more flexible than LDA, as it allows for group-specific
covariance matrices.

\vspace{4mm}

\textbf{Q:}

\begin{itemize}
\tightlist
\item
  But, if the covariance matrices in theory are equal - will they not be
  estimated equal?
\item
  Should we not always prefer QDA to LDA?
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\textbf{A:}

Explanation similar to a ``Bias-variance trade-off'':

If the assumption of equal covariance matrices is wrong

\begin{itemize}
\tightlist
\item
  then LDA may suffer from high bias for the parameter estimators.
\item
  and QDA is better off. But, for small sample sizes the covariance
  matrices might be poorly estimated (high variance of estimators).
\end{itemize}

If the number of covariates is high:

\begin{itemize}
\tightlist
\item
  then QDA requires estimating \(K\cdot p \cdot (p+1)/2\) parameters,
\item
  while LDA only requires \(p\cdot(p+1)/2\).
\end{itemize}

Therefore, LDA is less flexible than QDA and might therefore have much
less variance.

\end{frame}

\begin{frame}

\begin{block}{LDA vs QDA -- Illustration}

Bayes (purple dashed), LDA (black dotted) and QDA (green solid) decision
boundaries for the cases where
\(\boldsymbol{\Sigma}_1 = \boldsymbol{\Sigma}_2\) (left) and
\(\boldsymbol{\Sigma}_1 \neq \boldsymbol{\Sigma}_2\) (right).

\centering
\includegraphics[width=1.00000\textwidth]{../../ISLR/Figures/Chapter4/4.9.png}

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Example: Which type of iris species?}

\vspace{2mm}

The \texttt{iris} flower data set was introduced by the British
statistician and biologist Ronald Fisher in 1936.

\begin{itemize}
\tightlist
\item
  Three plant species \{setosa, virginica, versicolor\} (50 observation
  of each), and
\item
  four features: \texttt{Sepal.Length}, \texttt{Sepal.Width},
  \texttt{Petal.Length} and \texttt{Petal.Width}.
\end{itemize}

\begin{figure}
\includegraphics[width=0.3\linewidth]{iris} \caption{Iris plant with sepal and petal leaves}\label{fig:iris_pic}
\end{figure}

\scriptsize
<http://blog.kaggle.com/2015/04/22/scikit-learn-video-3-machine-learning-first-steps-with-the-iris-dataset/>

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Example: Classification of iris plants}

\vspace{2mm}

We will use \texttt{sepal\ width} and \texttt{sepal\ length} to build a
classificator, here 50 observations from each class available.

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{attach}\NormalTok{(iris)}
\KeywordTok{library}\NormalTok{(class)}
\KeywordTok{library}\NormalTok{(MASS)}
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{library}\NormalTok{(dplyr)}
\KeywordTok{head}\NormalTok{(iris)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa
\end{verbatim}

\end{block}

\end{frame}

\begin{frame}

\begin{center}\includegraphics[width=1\linewidth]{4Classif_files/figure-beamer/iris1-1} \end{center}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Iris: LDA}

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris_lda =}\StringTok{ }\KeywordTok{lda}\NormalTok{(Species }\OperatorTok{~}\StringTok{ }\NormalTok{Sepal.Length }\OperatorTok{+}\StringTok{ }\NormalTok{Sepal.Width, }\DataTypeTok{data =}\NormalTok{ iris, }\DataTypeTok{prior =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }
    \DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}\OperatorTok{/}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{4Classif_files/figure-beamer/iris2-1} \end{center}

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Iris: QDA}

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris_qda =}\StringTok{ }\KeywordTok{qda}\NormalTok{(Species }\OperatorTok{~}\StringTok{ }\NormalTok{Sepal.Length }\OperatorTok{+}\StringTok{ }\NormalTok{Sepal.Width, }\DataTypeTok{data =}\NormalTok{ iris, }\DataTypeTok{prior =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }
    \DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}\OperatorTok{/}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{4Classif_files/figure-beamer/iris3-1} \end{center}

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Iris: compare LDA and QDA}

\vspace{2mm}

We now want to compare the predictive performance of our two
classifiers. We do this by dividing the original \texttt{iris} data set,
randomly, into train and test samples of equal size:

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{train =}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{150}\NormalTok{, }\DecValTok{75}\NormalTok{)}

\NormalTok{iris_train =}\StringTok{ }\NormalTok{iris[train, ]}
\NormalTok{iris_test =}\StringTok{ }\NormalTok{iris[}\OperatorTok{-}\NormalTok{train, ]}
\end{Highlighting}
\end{Shaded}

\normalsize

Run LDA and QDA on the same (!!) training set:

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris_lda2 =}\StringTok{ }\KeywordTok{lda}\NormalTok{(Species }\OperatorTok{~}\StringTok{ }\NormalTok{Sepal.Length }\OperatorTok{+}\StringTok{ }\NormalTok{Sepal.Width, }\DataTypeTok{data =}\NormalTok{ iris_train, }
    \DataTypeTok{prior =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}\OperatorTok{/}\DecValTok{3}\NormalTok{)}

\NormalTok{iris_qda2 =}\StringTok{ }\KeywordTok{qda}\NormalTok{(Species }\OperatorTok{~}\StringTok{ }\NormalTok{Sepal.Length }\OperatorTok{+}\StringTok{ }\NormalTok{Sepal.Width, }\DataTypeTok{data =}\NormalTok{ iris_train, }
    \DataTypeTok{prior =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}\OperatorTok{/}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{block}

\end{frame}

\begin{frame}[fragile]

LDA training error: \(\frac{14}{75} =0.19\)

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(}\KeywordTok{predict}\NormalTok{(iris_lda2, }\DataTypeTok{newdata =}\NormalTok{ iris_train)}\OperatorTok{$}\NormalTok{class, iris_train}\OperatorTok{$}\NormalTok{Species)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             
##              setosa versicolor virginica
##   setosa         27          0         0
##   versicolor      1         15         8
##   virginica       0          5        19
\end{verbatim}

\normalsize

\vspace{2mm}

LDA test error: \(\frac{19}{75} =0.26.\)

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris_lda2_predict =}\StringTok{ }\KeywordTok{predict}\NormalTok{(iris_lda2, }\DataTypeTok{newdata =}\NormalTok{ iris_test)}
\KeywordTok{table}\NormalTok{(iris_lda2_predict}\OperatorTok{$}\NormalTok{class, iris}\OperatorTok{$}\NormalTok{Species[}\OperatorTok{-}\NormalTok{train])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             
##              setosa versicolor virginica
##   setosa         22          0         0
##   versicolor      0         22        11
##   virginica       0          8        12
\end{verbatim}

\end{frame}

\begin{frame}[fragile]

QDA training error: \(\frac{13}{75} =0.17\).

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(}\KeywordTok{predict}\NormalTok{(iris_qda2, }\DataTypeTok{newdata =}\NormalTok{ iris_train)}\OperatorTok{$}\NormalTok{class, iris_train}\OperatorTok{$}\NormalTok{Species)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             
##              setosa versicolor virginica
##   setosa         28          0         0
##   versicolor      0         16         9
##   virginica       0          4        18
\end{verbatim}

\normalsize

\vspace{2mm}

QDA test error: \(\frac{24}{75}=0.32\).

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris_qda2_predict =}\StringTok{ }\KeywordTok{predict}\NormalTok{(iris_qda2, }\DataTypeTok{newdata =}\NormalTok{ iris_test)}
\KeywordTok{table}\NormalTok{(iris_qda2_predict}\OperatorTok{$}\NormalTok{class, iris}\OperatorTok{$}\NormalTok{Species[}\OperatorTok{-}\NormalTok{train])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             
##              setosa versicolor virginica
##   setosa         22          0         0
##   versicolor      0         18        12
##   virginica       0         12        11
\end{verbatim}

\end{frame}

\begin{frame}

\textbf{Result}: The LDA classifier has given the smallest test
error\footnote{Note that the training error is of much less interest; it could be low due to \emph{overfitting} only.}
for classifying iris plants based on sepal width and sepal length for
our test set and should be preferred in this case.

\vspace{2mm}

But:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Would another division of the data into training and test set give the
  same conclusion (that LDA is better than QDA for this data set)? (A:
  Not necessarily, but probably.)

  \begin{itemize}
  \tightlist
  \item
    We will look into other choice than dividing into one training and
    one test set in Module 5 (crossvalidation).
  \end{itemize}
\end{enumerate}

\vspace{2mm}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  What about the other two covariates? Would adding them to the model (4
  covariates) give a better classification rule? (A: Probably. Try if
  you want.)
\end{enumerate}

\end{frame}

\begin{frame}

\begin{block}{Different forms of discriminant analysis}

\begin{itemize}
\item
  LDA
\item
  QDA
\item
  Naive Bayes (``Idiot's Bayes''): Assume that each class density is the
  product of marginal densities - i.e.~inputs are conditionally
  independent in each class \[f_k(x)=\prod_{j=1}^p f_{kj}(x_j) \ .\]
  This is generally not true, but it simplifies the estimation
  dramatically.
\item
  Other forms by proposing specific density models for \(f_k(x)\),
  including nonparametric approaches.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Naive Bayes}

\begin{itemize}
\item
  Naive Bayes is method that is popular when \(p\) is large.
\item
  The \emph{original naive Bayes}: univariate normal marginal
  distributions. Consequently
  \[\delta_k (x) \propto \log \left[ \pi_i \prod_{j=1}^p f_{kj}(x_j) \right] = - \frac{1}{2} \sum_{j=1}^p \frac{(x_j - \mu_{kj})^2}{\sigma_{kj}^2} + \log(\pi_k) \ , \]
  thus \(\boldsymbol{\Sigma}_k\) is assumed diagonal, and only the
  diagonal elements are estimated.
\item
  Arbitraty generalizations can be made. For example, mixed features
  (qualitative and quantitative predictors).
\item
  This method often produces good results, even though the joint pdf is
  not the product of the marginal pdf. This might be because we are not
  focussing on estimation of class pdfs, but class boundaries.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Summary of Classification Methods}

\begin{itemize}
\item
  Logistic regression
\item
  Linear discriminant analysis
\item
  Quadratic discriminant analysis
\item
  Naive Bayes
\item
  KNN
\end{itemize}

\vspace{2mm}

\end{block}

\end{frame}

\begin{frame}{Which classification method is the best?}

\begin{block}{Advantages of discriminant analysis}

\begin{itemize}
\tightlist
\item
  Discriminant analysis is more stable than logistic regression when the
  classes are well-separated.
\item
  Discriminant analysis is more stable than logistic regression if the
  number of observations \(n\) is small and the distribution of the
  predictors \(X\) is approximately (multivariate) normal.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Linearity}

\vspace{2mm}

Assume a binary classification problem with one covariate.

\begin{itemize}
\item
  Recall that logistic regression can be written:
  \[\log \Big ( \frac{p(x)}{1-p(x)}\Big ) = \beta_0 + \beta_1 x.\]
\item
  For a two-class problem, one can show that for LDA
  \[\log\left(\frac{p_1(x)}{1-p_1(x)}\right) = \log\left(\frac{p_1(x)}{p_2(x)}\right) = c_0 + c_1 x_1 \ ,\]
  thus the same linear form. The difference is in how the parameters are
  estimated.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{LDA vs logistic regression}

\begin{itemize}
\item
  Logistic regression uses the conditional likelihood based on
  \(\text{Pr}(Y \mid X)\).
\item
  LDA uses the full likelihood based on \(\text{Pr}(X,Y)\).
\item
  Despite these differences, in practice the results are often very
  similar\footnote{logistic regression can also fit quadratic boundaries
  like QDA, by explicitly including quadratic terms in the model.}, but

  \begin{itemize}
  \tightlist
  \item
    LDA is ``more available'' in the multi-class setting
  \item
    if the class conditional distributions are multivariate normal then
    LDA (or QDA) is preferred
  \item
    if the class conditional distributions are far from multivariate
    normal then logistic regression is preferred
  \item
    in medicine for two-class problems logistic regression is often
    preferred (for interpretability) and (always) together with ROC and
    AUC (for model comparison).
  \end{itemize}
\end{itemize}

\end{block}

\begin{block}{and KNN?}

\begin{itemize}
\tightlist
\item
  KNN is used when the class boundaries are non-linear.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}{So: Which classification method is the best?}

\vspace{2mm}

The answer is: \textbf{it depends!}

\begin{itemize}
\item
  Logistic regression is very popular for classification, especially
  when \(K = 2\).
\item
  LDA is useful when \(n\) is small, or the classes are well separated,
  and Gaussian assumptions are reasonable. Also when \(K > 2\).
\item
  Naive Bayes is useful when \(p\) is very large.
\item
  KNN is completely different, as it makes no assumptions about the
  decision boundary nor the distribution of the variables
  (nonparametric). It is expected to work better than LDA and logistic
  regression when boundary is very non-linear.
\end{itemize}

\vspace{2mm} Please read Section 4.5 of our coursebook (James et al.
2013).

\end{frame}

\begin{frame}

\vspace{2mm} Todo: Include exercise(s) that compares LDA, QDA, logreg
and KNN!

Compulsory Exercise 1, Problem 3a.

\end{frame}

\begin{frame}{Two-class problems: sensitivity, specificity}

\begin{itemize}
\item
  Problems with only two classes (binary classifiers) have a special
  status, e.g.~in medicine or biology.
\item
  Assume the classes (\(Y\)) are labelled ``-'' (non disease, or
  \(Y=0\)) and ``+'' (disease, or \(Y=1\)), and that a diagnostic test
  is used to predict \(Y\) given \(X=x\).
\item
  \textbf{Sensitivity} is the proportion of correctly classified
  positive observations:
  \[\frac{\# \text{True Positive}}{\# \text{Condition Positive}}=\frac{\text{TP}}{\text{P}} \ .\]
\item
  \textbf{Specificity} is the proportion of correctly classified
  negative observations:
  \[\frac{\# \text{True Negative}}{\# \text{Condition Negative}}=\frac{\text{TN}}{\text{N}} \ .\]
\end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}
\tightlist
\item
  We would like that a classification rule (or a diagnostic test) have
  both a high sensitivity and a high specificity.
\item
  However, in an imperfect test (i.e., an imperfect predictor) one
  usually comes at the cost of the other.
\end{itemize}

\(2 \times 2\) table shows data from a simple diagnostic study:

\begin{center}
\begin{tabular}{ll|c|c|c}
& & \multicolumn{2}{c}{\emph{Predicted}} \\
& & $Y=0$ & $Y=1$ \\ 
\hline
\multirow{2}{*}{\emph{True}} & $Y=0$ & True negative ($TN$) & False positive ($FN$) & $N$\\
 & $Y=1$ & False negative ($FP$) & True positive ($TP$) & $P$\\
\hline
& & $N^\star$ & $P^\star$ & $Tot$ 
\end{tabular}
\end{center}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Example Continued: South African heart disease}

We want to evaluate our multiple logistic model for the \texttt{SAheart}
data set. In order to investigate the training error and the test error,
we divide the original data set, randomly, into two samples of equal
size.

\small

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{20}\NormalTok{)}
\NormalTok{train_ID =}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(d.heart), }\KeywordTok{nrow}\NormalTok{(d.heart)}\OperatorTok{/}\DecValTok{2}\NormalTok{)}
\NormalTok{train_SA =}\StringTok{ }\NormalTok{d.heart[train_ID, ]}
\NormalTok{test_SA =}\StringTok{ }\NormalTok{d.heart[}\OperatorTok{-}\NormalTok{train_ID, ]}
\end{Highlighting}
\end{Shaded}

\end{block}

\end{frame}

\begin{frame}[fragile]

We now fit a logistic regression model, using the training set only:

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glm_SA =}\StringTok{ }\KeywordTok{glm}\NormalTok{(chd }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ train_SA, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(glm_SA)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = chd ~ ., family = "binomial", data = train_SA)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.0104  -0.8065  -0.4638   0.8892   2.3089  
## 
## Coefficients:
##                  Estimate Std. Error z value Pr(>|z|)   
## (Intercept)    -3.7674748  1.4391193  -2.618  0.00885 **
## sbp             0.0045154  0.0089070   0.507  0.61219   
## tobacco         0.1253784  0.0438283   2.861  0.00423 **
## ldl             0.0484642  0.0804538   0.602  0.54692   
## famhistPresent  0.8186419  0.3380680   2.422  0.01546 * 
## obesity        -0.0226110  0.0404832  -0.559  0.57648   
## alcohol        -0.0007384  0.0080396  -0.092  0.92682   
## age             0.0446074  0.0147819   3.018  0.00255 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 289.73  on 230  degrees of freedom
## Residual deviance: 232.66  on 223  degrees of freedom
## AIC: 248.66
## 
## Number of Fisher Scoring iterations: 4
\end{verbatim}

\normalsize

\end{frame}

\begin{frame}[fragile]

\begin{itemize}
\item
  \textbf{Aim}: To estimate the probability of a \texttt{chd} event for
  the observations in the test set.
\item
  To do this we can insert the estimated coefficient into the logistic
  equation, remembering that \texttt{famhist} is a categorical
  covariate, modeled by a dummy variable:
  \[x_\text{famhist} = \begin{cases} 1, \quad \text{ if Present}, \\ 0, \quad \text{ if Absent}.\end{cases}\]
\item
  The estimated probability of \(Y=1\) if
  \texttt{famhist\ =\ "Present"}, given a vector \(\mathbf{X}\) of
  covariate observations is:
  \[\hat{p}(\mathbf{X}) =\frac{e^\eta}{1+e^\eta} ,\] with
  \(\eta=\beta_0 + \beta_1 x_{\text{sbp}} + \beta_2 x_{\text{tobacco}} + \beta_3 x_\text{ldh} + \beta_4 \cdot x_{famhist} + \beta_5 x_\text{obesity} + \beta_6 x_\text{alcohol} + \beta_7 x_{\text{age}}\).
\end{itemize}

\end{frame}

\begin{frame}[fragile]

The \texttt{predict} function does these calculations for us. When
specifying \texttt{type="response"} the function returns the
probabilities for \(Y=1\).

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{probs_SA =}\StringTok{ }\KeywordTok{predict}\NormalTok{(glm_SA, }\DataTypeTok{newdata =}\NormalTok{ test_SA, }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\normalsize

From these probabilities we can obtain classifications, by specifying a
threshold value. We have here chosen a threshold value of 0.5. By using
the \texttt{ifelse} function we specify that all probabilities larger
than 0.5 are to be classified as 1, while the remaining probabilities
are to be classified as 0.

\end{frame}

\begin{frame}[fragile]

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred_SA =}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(probs_SA }\OperatorTok{>}\StringTok{ }\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}

\NormalTok{predictions_SA =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(probs_SA, pred_SA, test_SA[, }\StringTok{"chd"}\NormalTok{])}
\KeywordTok{colnames}\NormalTok{(predictions_SA) =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Estim. prob. of Y=1"}\NormalTok{, }\StringTok{"Predicted class"}\NormalTok{, }
    \StringTok{"True class"}\NormalTok{)}
\KeywordTok{head}\NormalTok{(predictions_SA)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Estim. prob. of Y=1 Predicted class True class
## 1            0.7741048               1          1
## 4            0.7141807               1          1
## 7            0.2258178               0          0
## 11           0.5216856               1          1
## 12           0.7112496               1          1
## 15           0.6702761               1          0
\end{verbatim}

\end{frame}

\begin{frame}[fragile]

We can now use the confusion matrix to count the number of
misclassifications. The below confusion matrix is calculated using the
test set and comparing the predicted classes with the true classes.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(}\DataTypeTok{predicted =}\NormalTok{ pred_SA, }\DataTypeTok{true =}\NormalTok{ d.heart[}\OperatorTok{-}\NormalTok{train_ID, }\StringTok{"chd"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          true
## predicted   0   1
##         0 118  45
##         1  27  41
\end{verbatim}

The logistic model has correctly classified 118+41 times, and
misclassified 27+45 times. The misclassification test error rate is
thus: \[\text{Test error} = \frac{27+45}{27+45+118+41} \approx 0.31\]

\end{frame}

\begin{frame}[fragile]

The training error can be calculated in a similar fashion, but now we
use the fitted model to make prediction for the training set.

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SA_train_prob =}\StringTok{ }\NormalTok{glm_SA}\OperatorTok{$}\NormalTok{fitted.values}
\NormalTok{SA_train_pred =}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(SA_train_prob }\OperatorTok{>}\StringTok{ }\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{conf_train =}\StringTok{ }\KeywordTok{table}\NormalTok{(SA_train_pred, d.heart[train_ID, }\StringTok{"chd"}\NormalTok{])}
\NormalTok{misclas_train =}\StringTok{ }\NormalTok{(}\DecValTok{231} \OperatorTok{-}\StringTok{ }\KeywordTok{sum}\NormalTok{(}\KeywordTok{diag}\NormalTok{(conf_train)))}\OperatorTok{/}\DecValTok{231}
\NormalTok{misclas_train}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2380952
\end{verbatim}

\normalsize
The train misclassification error rate is \(\approx 23.8\%\).

\end{frame}

\begin{frame}

\begin{block}{ROC curves and AUC}

\vspace{2mm}

\begin{itemize}
\item
  The receiver operating characteristics (ROC) curve gives a graphical
  display of the sensitivity against (1-specificity), as the threshold
  value (cut-off on probability of success or disease) is moved over the
  range of all possible values.
\item
  An ideal classifier will give a ROC curve which hugs the top left
  corner (sensitivity = specificity = 1), while a straight line
  represents a classifier with a random guess of the outcome.
\item
  The \textbf{AUC} score is the area under the AUC curve. It ranges
  between the values 0 and 1, where a higher value indicates a better
  classifier.
\item
  The AUC score is useful for comparing the performance of different
  classifiers, as all possible threshold values are taken into account.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Example Continued: South African heart disease}

In order to see how our model performs for different threshold values,
we can plot a ROC curve:

\scriptsize

\begin{center}\includegraphics[width=0.8\linewidth]{4Classif_files/figure-beamer/roc-1} \end{center}

\end{block}

\end{frame}

\begin{frame}[fragile]

To check where in the plot we find the default cut-off on 0.5, we need
to calculate sensitivity and specificity for this cut-off:

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res =}\StringTok{ }\KeywordTok{table}\NormalTok{(pred_SA, d.heart[}\OperatorTok{-}\NormalTok{train_ID, }\StringTok{"chd"}\NormalTok{])}
\NormalTok{sens =}\StringTok{ }\NormalTok{res[}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{]}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(res[}\DecValTok{2}\NormalTok{, ])}
\NormalTok{spec =}\StringTok{ }\NormalTok{res[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(res[}\DecValTok{1}\NormalTok{, ])}
\NormalTok{sens}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6029412
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spec}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.7239264
\end{verbatim}

\normalsize
Observe that the value 0.603 (on \(y\)-axis) and 0.276 (1-specificity on
\(x\)-axis) is on our ROC curve.

The ROC-curve is made up of all possible cut-offs and their associated
sensitivity and specificity.

\end{frame}

\begin{frame}

\begin{block}{Extensions for classifications}

\begin{itemize}
\tightlist
\item
  Module 5: how to use cross-validation in model evaluation and model
  selection
\item
  (Module 6: model selection - but mainly regression)
\item
  Module 7: maybe a taste of nonlinear methods
\item
  Module 8: classification trees (binary splits for the covariates)
\item
  Module 9: support vector machines
\item
  Module 11: neural nets
\end{itemize}

\end{block}

\end{frame}

\begin{frame}{ Further reading }

\begin{itemize}
\tightlist
\item
  More on logistic regression from TMA4315 Generalized linear models
  H2018:
  \href{https://www.math.ntnu.no/emner/TMA4315/2018h/3BinReg.html}{TMA4315M3:
  Binary regression}
\item
  \href{https://www.youtube.com/playlist?list=PL5-da3qGB5IC4vaDba5ClatUmFppXLAhE}{Videoes
  on YouTube by the authors of ISL, Chapter 4}
\end{itemize}

\end{frame}

\begin{frame}

\begin{block}{Want to learn more (theory) about logistic regression?}

\begin{itemize}
\item
  In TMA4315 Generalized linear models there are 3 weeks with binary
  regression - mainly logistic regression.
\item
  The focus there is on all parts of the regression (not classification)
  with a mathematical focus on estimation, inference, model fit.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}{Acknowledgements}

I thank Mette Langaas and Julia Debik, whose slides I modified and
adapted to get this set of slides.

\end{frame}

\begin{frame}{References}

\hypertarget{refs}{}
\hypertarget{ref-james.etal}{}
James, G., D. Witten, T. Hastie, and R. Tibshirani. 2013. \emph{An
Introduction to Statistical Learning with Applications in R}. New York:
Springer.

\end{frame}

\end{document}
