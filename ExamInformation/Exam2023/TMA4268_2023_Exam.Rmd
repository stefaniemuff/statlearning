---
title: "TMA4268 V2023 Exam"
author: Stefanie Muff, Department of Mathematical
  Sciences, NTNU
date: 'June 1, 2023'
output:
  # html_document:
  #   df_print: paged
  #   toc: no
  #   toc_depth: '2'
  pdf_document:
    toc: no
    toc_depth: '2'
subtitle: TMA4268 Statistical Learning V2023
urlcolor: blue
header-includes: \usepackage{amsmath,bm}
---


```{r setup, include=FALSE}
library(knitr)
library(MASS)
library(caret)
library(pls)
library(glmnet)
library(gam)
library(gbm)
library(randomForest)
library(ggfortify)
library(leaps)
library(pROC)
 
opts_chunk$set(tidy.opts=list(width.cutoff=68),tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=FALSE, size="scriptsize", fig.height=4, fig.width=5,cache=TRUE)

```


Maximum number of points: 55.5

###################################
# Warming up
###################################


# Problem 1 (Fill-in-the-blank text, 4.5P)

Read the whole text and fill in the blanks such that the whole text makes sense (you might only understand which answer is correct after you continued reading):

We have discussed a lot of methods and models in our course, and the methods are broadly divided into supervised and _\textcolor{red}{unsupervised}_ (_parametric_, _non-parametric_, _regression_, _classificiation_) approaches. In the latter case we _\textcolor{red}{do not have a response, }_ (_cannot do inference_, _do not learn from the data_, _tend to overfit_) and therefore are not interested in prediction.

In supervised statistical learning methods there are two main purposes: _\textcolor{red}{prediction}_ ( _inference_, _bias reduction_, _variance reduction_, _supervised learning_, _unsupervised learning_) and _\textcolor{red}{inference}_ ( _prediction_, _bias reduction_, _variance reduction_, _unsupervised learning_, _supervised learning_). In both cases we want to learn from data and build a model that relates a set of variables to an outcome, but in the first case we do not interpret the actual model parameters. Some of the methods we learned about were _\textcolor{red}{parametric}_ ( _non-parameteric_, _supervised_, _unsupervised_) and others were _\textcolor{red}{non-parametric}_ ( _parameteric_, _supervised_, _unsupervised_), whereas the latter tend to be more more flexible -- and thus possibly less biased -- than the former ones, but at the cost of _\textcolor{red}{higher variance}_ (_curse of dimensionality_, _higher test MSE_ , _lower accuracy_). This phenomenon is denoted as _\textcolor{red}{bias-variance trade-off}_ (_overfitting_, _underfitting_, _bias_, _variance_, _model selection bias_, _regularization_). 

In the course we learned about models with different levels of complexity. For complex models, or in the presence of many variables, we have to be careful that we do not over-fit the data. We learned about several techniques that help prevent over-fitting via _\textcolor{red}{regularization}_ (_model selection_, _scaling_, _transformation of variables_, _bias-variance trade-off_), for example shrinkage methods, dimension reduction or dropout in neural networks. 

**Note**: The last sentence was taken out of the grading, because none of the answers are correct:

These approaches increase the robustness of the fitted models, where the main aim is to find the function that minimizes the expected test error, that is, the _\textcolor{red}{irreducible error}_ (_reducible error_, _bias_, _variance_, _signal-to-noise ratio_).




# Problem 2 (7P)

## a) (2P)
Sketch the tree corresponding to the partition of the predictor space illustrated in the figure. The numbers inside the boxes are the mean of the response $y$ within the regions.
 
![](trees.png){width=60%}

**Solution:**

1P for the correct tree structure with cut points, 1P for the correct values on the leafs.

![](trees_solution.png){width=40%}

## a) (5P)

In this problem, we consider a simulated data set with two classes (labelled 0 and 1) and two numerical
covariates $x_1$ and $x_2$ . Let $x = (x_1 , x_2)$ be a column vector with the two covariates. A training set with 500
observations of each class is available, and a scatter plot is given below. We simulate a data set as follows:

* Prior class probabilities: $\pi_0 = P(Y=0) = 0.5$ and $\pi_1= P(Y = 1) = 0.5$.
* Class-specific probabilities 

\begin{align*}
P(\bm{x}|y=0) = f_0(\bm{x})=
0.5 \cdot \frac{1}{2\pi |\Sigma|} \cdot \exp\left( -\frac{1}{2} (\bm{x}-\bm{\mu}_{01})^\top \Sigma^{-1} (\bm{x}- \bm{\mu}_{01})\right) + \\
 0.5 \cdot \frac{1}{2\pi |\Sigma|} \cdot \exp\left(- \frac{1}{2} (\bm{x}-\bm{\mu}_{02})^\top \Sigma^{-1} (\bm{x}- \bm{\mu}_{02})\right)
\end{align*}

$$
P(\bm{x}|y=1) = f_1(\bm{x})=
\frac{1}{2\pi |\Sigma|} \cdot \exp\left(-\frac{1}{2} (\bm{x}-\bm{\mu}_{2})^\top\Sigma^{-1}(\bm{x}-\bm{\mu}_{2})\right) 
$$ 

with $\bm{\mu}_{01}=\begin{pmatrix}2 \\ 2 \end{pmatrix}$, $\bm{\mu}_{02}=\begin{pmatrix}4 \\ 2 \end{pmatrix}$, $\bm{\mu}_{2}=\begin{pmatrix}3 \\ 5 \end{pmatrix}$ and $\Sigma = \begin{pmatrix} 1 & 0.5 \\0.5 & 1\end{pmatrix}$. 

```{r, eval=T, echo=F,fig.width=4,fig.height=3.5,out.width="150%"}
library(mvtnorm)
n0  <-500
n1 <- 500
p01 <- rmvnorm(n0/2,c(2,2),matrix(c(1,0.5,0.5,1.5),nrow=2))
p02 <- rmvnorm(n0/2,c(4,2),matrix(c(1,0.5,0.5,1),nrow=2))
p2 <- rmvnorm(n1,c(3,5),matrix(c(1,0.5,0.5,1),nrow=2))
data <- as.data.frame(rbind(p01,p02,p2))
data$col <- c(rep("red",n0),rep("blue",n1))
plot(V2~V1,col=col,data=data,xlab="x1",ylab="x2",pch=16,cex=0.5)
```

Given that $\pi_0=\pi_1=0.5$, and the knowledge about the class-specific distributions $f_0(x)$ and $f_1(x)$ given above, the aim is to derive the equation for the Bayes decision boundary to find the Bayes classifier. 

(i) (1P) Explain what the Bayes decision boundary actually is.
(ii) (3P) Write down the equation to be solved with the actual values (you are not supposed/asked to solve the equation), and explain what the unknowns are. 
(iii) (1P) Is the resulting boundary linear in the covariates? Why?

**Solution**:

(i) Say either: The Bayes decision boundary is where the probabilities for the two classes are equal (0.5P for this), or: observations are classified to _the most probable class_ (the class with the highest posterior
probability; 0.5P for this alternative argument). Using this boundary thus gives _the minimum expected 0/1 loss_ (0.5P for the last part, max 1P in total). 

(ii) Here we need to find the equation where

$$f_0(x)\pi_0 = f_1(x) \pi_1 \ , $$
and since $\pi_0=\pi_1$, we only need to solve

$$f_0(x) = f_1(x)   \ . $$
(1P if the students gets here).
(Note that log-transformation does not help much here due to the sum in the likelihood for group 0.)

Thus, the equation is given as 

\begin{align*}
0.5 & \cdot \frac{1}{2\pi |\Sigma|} \cdot \exp\left(- \frac{1}{2} (\bm{x}-\bm{\mu}_{01})^\top \Sigma^{-1} (\bm{x}- \bm{\mu}_{01})\right) + 0.5 \cdot \frac{1}{2\pi |\Sigma|} \cdot \exp\left( -\frac{1}{2} (\bm{x}-\bm{\mu}_{02})^\top \Sigma^{-1} (\bm{x}- \bm{\mu}_{02})\right)  \\
 = &   \frac{1}{2\pi |\Sigma|} \cdot \exp\left(-\frac{1}{2} (\bm{x}-\bm{\mu}_{2})^\top\Sigma^{-1}(\bm{x}-\bm{\mu}_{2})\right) \ ,
\end{align*}

(1P),

where the students are expected to plug in the actual values for $\bm{\mu}_{01}$, $\bm{\mu}_{02}$, $\Sigma$ etc. (1P for plugging in the values)

(iii) No, the boundary is not linear (and neither quadratic) in the covariates, because by log-transforming we do not get rid of the exponential term due to the sum in group 0.

###################################
# Problem 3 -- Data analysis 1 -- regression (17P)
###################################

Here we are looking at a regression problem, where we want to understand the factors that affect birth weight of babies. We use the `birthwt` data set from the MASS package, which you can load and investigate using the code below and by typing `?birthwt` into the R console:

```{r,fig.width=8,fig.height=8,out.width="80%"}
library(MASS)
data(birthwt)

d.bw <- birthwt

# Race is a categorical variables, so we have to convert it:
d.bw$race <- as.factor(d.bw$race)

# Look at the data, for example using:
pairs(d.bw)
str(d.bw)
```

In order to assess the robustness of our models, we split the data into a training and a test set as follows:

```{r}
set.seed(1234)
samples <- sample(1:189,132, replace=F)
d.bw.train <- d.bw[samples,]
d.bw.test <- d.bw[-samples,]
```


##  a) (4P)

(i) (1P) Fit a linear regression model on the training data, where you use birth weight in grams (`bwt`) as the response and all variables **except `low`** as predictors. Report the regression coefficients, standard errors and $p$-values (use the standard way to do this in R).

(ii) (1P) What is the expected difference in birth weight for babies of black women compared to white women?
(iii) (1P) Is there evidence for `race` being relevant in the model? Say which test you carry out and report the respective $p$-value.
(iv) (1P) Compare $R^2$ and $R^2_{adj}$ of the model and interpret what you find.

**Solution:**
(i)
```{r}
r.lm1 <- lm(bwt ~ age + lwt + race + smoke + ptl + ht + ui + ftv, d.bw.train)
summary(r.lm1)
```
(ii) `r round(summary(r.lm1)$coef["race2",1],0)` grams
(iii) F-test using the anova function in R:
```{r}
anova(r.lm1)
```
The p-value is small (<0.05), thus yes, there is evidence.

(iv) The two values should be reported here: $R^2=0.267$ and $R^2_{adj}=0.216$. The difference in the two $R^2$ values indicates that there are some unneccesary variables in the model. Note: Students don't need to interpret the $R^2$ itself, only the differnces. 

##  b) (3P)

(i) (2P) A medical researcher is interested to know whether the effect of the mother's smoking status during pregnancy changes with the age of the mother. Expand the model from a) such that it accounts for the possibility that the effect of smoking depends on age and interpret the results, still using the training data. In particular, answer the question of the researcher and compare $R^2$ to the one from a).
(ii) (1P) According to your modeling output, when the mother is smoking, how much does the expected birth weight change for a mother age 35 compared to a mother age 25, given that all other variables are the same?

**Solution**

(i) The model and its output are as follows (1P for the correct model, no point if the interaction is lacking and -0.5P for other mistakes):
```{r}
r.lm2 <- lm(bwt ~ age + lwt + race  + ptl + ht + ui + ftv + smoke*age, d.bw.train)
summary(r.lm2)
```

Interpretation: Yes, the effect of smoking changes with age (0.5P). Older women have an even higher risk of low birth weight when they smoke than younger women. The $R^2=0.30$ has clearly improved, underlining that the interaction term is relevant (0.5P). 

(ii) The overall effect of age is given by $\beta_{age} + \beta_{age:smoking}$, thus we have to multiply this term by 10 to obtain the expected reduction in birth weight: `r round((coef(r.lm2)[2] + coef(r.lm2)[11])*10,1)`. Here there is an all-or-nothing grading (1P for correct value, 0P otherwise).

##  c) (3P)

(i) (1P) Carry out Lasso regression on the training set excluding the variable `low` (like in a) and b)), and say how you choose $\lambda$.
(ii) (2P) Compare the regression coefficients from the Lasso with those from the linear regression in a). What pattern(s) do you notice? Would you see the same if you had carried out ridge regression?

**R-hints**:

```{r}
x.train <- model.matrix(bwt~. -low, data = d.bw.train)[,-1]
y.train <- d.bw.train$bwt
x.test = model.matrix(bwt~. -low, data = d.bw.test)[,-1]
y.test = d.bw.test$bwt
```

```{r,echo=T,eval=F}
library(glmnet)
set.seed(10)
cv.lasso <- cv.glmnet(x.train, y.train, alpha=...)
plot(cv.lasso)
cv.lasso$...
bw.lasso <- glmnet(..., ..., alpha = ..., lambda = ...)
```

**Solution**

(i) 
```{r,echo=T}
library(glmnet)
set.seed(10)
cv.lasso <- cv.glmnet(x.train, y.train, alpha=1)
plot(cv.lasso)
cv.lasso$lambda.min
bw.lasso <- glmnet(x.train, y.train, alpha = 1, lambda = cv.lasso$lambda.min)

coef(bw.lasso)
```

Alternatively, with `lambda.1se`: 

```{r, echo=T}
cv.lasso$lambda.1se
bw.lasso.1se <- glmnet(x.train, y.train, alpha = 1, lambda = cv.lasso$lambda.1se)

coef(bw.lasso.1se)
```

$\lambda$ is chosen using cross-validation (0.5P).

(ii) There are two patterns to notice: The coefficients are shrunken (0.5P) and some are zero (0.5P). For ridge we would also have expected to see shrinkage (0.5P), but none of the coefficients had gone to zero (0.5P). Here I expect to explicitly hint that the coefficients are getting smaller/shurunken (not just that some are close to zero for ridge, for example).


## d) (3P)

Report the MSEs for the test set for all the models fit in a), b) and c). Which method performs best?

**Solution: **

```{r}
r.lm1 <- lm(bwt ~ age + lwt + race + smoke + ptl + ht + ui + ftv, d.bw.train)
r.pred.lm1 <- predict(r.lm1, newdata=d.bw.test)
mse.a <- mean((r.pred.lm1-d.bw.test$bwt)^2)

r.lm2 <- lm(bwt ~ age + lwt + race + smoke*age + ptl + ht + ui + ftv, d.bw.train)
r.pred.lm2 <- predict(r.lm2, newdata=d.bw.test)
mse.b <- mean((r.pred.lm2-d.bw.test$bwt)^2)

r.lasso <- predict(bw.lasso,newx =x.test)
mse.c <- mean((r.lasso-d.bw.test$bwt)^2)

# Also for the case where the larger lambda is used in d):
r.lasso.1se <- predict(bw.lasso.1se,newx =x.test)
mse.c.1se <- mean((r.lasso.1se-d.bw.test$bwt)^2)

c(mse.a,mse.b,mse.c,mse.c.1se)
```

Lasso performs best if lambda.min was used, otherwise standard regression performs best.


## e) (4P)

Finally choose a more flexible method than those used above to find a model with _lower test error_.  Explain your model and the choices you make. Code without explanation will not give full score.

**R-hints:**
```{r,echo=T,eval=F}
library(gam)
library(randomForest)
```


**Solution: **

Here there is a lot of freedom. The students will probably use a GAM or a regression tree. 

In a regression tree, the students must explain the choice for the number of trees and mtry (if either is lacking, -1P for each). It's not enough to say "I choose the default no of trees from the lecture/function" or "I chose ntree=500", for example -- ntrees should be chosen _large enough_ (i.e., it is no tuning parameter), and that should be made clear. Note that ntrees is not a tuning parameter, so the students should not optimize it. 
If students choose a GAM, they also need to explain each term and the choices they made.

Here is one solution: The idea is to use only variables that are left from the Lasso and then apply a cubic spline on the only continuous variable `lwt`:

```{r}
library(gam)
r.gam <- gam(bwt ~  bs(lwt,4) + race + smoke + ptl + ht + ui , data=d.bw.train)
r.pred.gam <- predict(r.gam, newdata=d.bw.test)
mean((r.pred.gam-d.bw.test$bwt)^2)
```

Here is a solution with random forests. Note that the students then need to explain `mtry` (usually p/3 for regression problems) and say that number of trees must be "large enough".

```{r}
library(randomForest)
rf.model <- randomForest(x.train, y.train, mtry = 3, ntree = 500)
yhat.rf <- predict(rf.model, newdata = x.test)
(mean((yhat.rf - y.test)^2))
```



\clearpage

###################################
# Problem 4 -- Data analysis 2 -- classification (12P)
###################################

We look at the same data set as in Problem 3. Use the same code to load and prepare the data: 

```{r,fig.width=8,fig.height=8,out.width="80%"}
library(MASS)
data(birthwt)

d.bw <- birthwt

# Race is a categorical variables, so we have to convert it:
d.bw$race <- as.factor(d.bw$race)
# Also convert low for later use:
d.bw$low <- as.factor(d.bw$low)
```

In order to assess the robustness of our models, we split the data into a training and a test set as follows:

```{r}
set.seed(1234)
samples <- sample(1:189,132, replace=F)
d.bw.train <- d.bw[samples,]
d.bw.test <- d.bw[-samples,]
```

The aim is to both _understand_ why and _predict_ whether a newborn baby has low birthweight ($<2.5kg$), using the binary indicator variable `low` for being underweight.




## a) (3P)

(i) (1P) Fit a logistic regression model that can be used to predict whether a baby will be underweight, including all predictor variables **except `bwt`** as predictors. Report the coefficients, standard errors and $p$-values (use the standard way to do this in R).

(ii) (2P) Report the probability to give birth to an underweight baby for a female with the following characteristics:
* age=25
* lwt (weight in pounds)=155
* race=white
* smoke = 1 (yes)
* ptl=0
* ht=0
* ui=0
* ftw=1



**R-hints:**
```{r,eval=FALSE}
glm(...,family="binomial")
```


**Solution:**
```{r}
r.glm <- glm(low ~ age + lwt + race + smoke + ptl + ht + ui + ftv, d.bw.train,family="binomial")
summary(r.glm)
```

(ii) One way to implement this is as follows (but you can also use the `predict()` function):
```{r}
x.data <- c(1,25,155,0,0,1,0,0,0,1)

eta <- t(coef(r.glm))%*%x.data

(prob <- exp(eta)/(1+exp(eta)))
```
The probability is `r round(prob,3)`.



## b) (4P)

(i) (2P) A medical doctor is interested in the overall effect that smoking has on low birth weight, based on logistic regression. Use the regression output from a) and explain to the doctor how smoking affects the chance for low birth weight. We are interested in a general statement that is not dependent on the value of the other variables.

(ii) (1P) Report the error rate for the test data using the fitted model from a). 
(iii) (1P) Calculate sensitivity and specificity for the test data.

**R-hints:**
```{r,eval=FALSE}
glm(...,family="binomial")
predict(...)
confusionMatrix(data=...,reference=...)$table
```

**Solution:**

(i) Here the students must NOT repeat the analysis from (a) with smoke=0, but look at the odds-ratio:
```{r}
exp(coef(r.glm)["smoke"])
```
Interpretation: The odds for low birth weight increases by a _factor_ of `r round(exp(coef(r.glm)["smoke"]),2)` for smoking women compared to non-smoking women.

(ii) 
```{r}
t.pred.glm <- round(predict(r.glm,newdata=d.bw.test,type="response"),0)

(confMat <- confusionMatrix(data=as.factor(t.pred.glm),reference=as.factor(d.bw.test$low))$table)
1 - (sum(diag(confMat))/sum(confMat[1:2,1:2]))
```

(iii)
Sensitivity: Corresponds to $P(\hat{y}=1 | y=1)$, where $y$ is the true (reference) value, thus `r round(confMat[2,2]/sum(confMat[1:2,2]),3)`.
Specificity: Corresponds to $P(\hat{y}=0 | y=0)$, thus `r round(confMat[1,1]/sum(confMat[1:2,1]),3)`.



## c) (5P)

As a comparison to the logistic regression model we are now using a random forest.

(i) (2P) Use a random forest to fit a model on the training data. Justify the choice of any parameters that you use.
(ii) (3P) Report the misclassification error (1P) sensitivity and specificity (1P) and compare the values to the logistic regression model above. Which of the methods is preferable and why (1P)?


**R-hints**:

```{r, eval=F}
library(randomForest)
set.seed(4268)

randomForest(...)
```

**Solution**:

(i) 1P to fit the model:
```{r}
library(randomForest)
set.seed(4268)

rf.bw = randomForest(low ~ age + lwt + race + smoke + ptl + ht + ui + ftv, data=d.bw.train, mtry = 2, ntree = 1000,
importance = TRUE)
```

The most critical choice is `mtry`. Since we have 8 variables, and $\sqrt{8}=2.8$, we can choose `mtry` =2 or =3. Both are ok, but need to be justified (0.5P). The number of trees is less critical but needs to be "large enough" (0.5P for the argument). 

(ii)
```{r}
t.pred.rf <- as.factor(predict(rf.bw, d.bw.test, type = "class"))
(confMat.rf <- confusionMatrix(data=t.pred.rf,reference=d.bw.test$low)$table)
1 - (sum(diag(confMat.rf))/sum(confMat.rf[1:2,1:2]))
```

Sensitivity: Corresponds to $P(\hat{y}=1 | y=1)$ where $y$ is the true (reference) value, thus `r round(confMat.rf[2,2]/sum(confMat.rf[1:2,2]),3)`.  
Specificity: Corresponds to $P(\hat{y}=0 | y=0)$, thus `r round(confMat.rf[1,1]/sum(confMat.rf[1:2,1]),3)`.


Observation: In comparison to logistic regression, the misclassification error is lower for the random forest. However, The sensitivity for the forest is extremely low, and in a medical context that might be undesirable because we might overlook a risk patient. Therefore, logistic regression might be preferable.


<!-- ## d) (2P) -->

<!-- List two additional methods that you could have used for the classification task here. Give one advantage and one disadvantage for each method in comparison to logistic regression. -->


<!-- **Solution:** -->

<!-- There are several options: -->

<!-- * KNN classification -->
<!-- * Linear and quadratic discriminant analysis (but only 1P is given if those two are listed as the only alternatives) -->
<!-- * SVM -->
<!-- * Additional tree-based methods (e.g., boosting) -->
<!-- * Neural networks -->



###################################
# Multiple and single choice questions
###################################

# Problem 5 (5P, single choice, 1P each)

## a) 
We want to estimate the regression coefficients in a linear regression model by minimizing
$$\sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 \quad \text{subject to} \quad \sum_{j=1}^p |\beta_j| \leq s 
$$
for a particular value of $s$. Which statement is correct? 

As we increase $s$ from 0, the training RSS will

(i) first increase and then start decreasing in an inverted U-shape.
(ii) first decrease and then start increasing in a U-shape.
(iii) steadily increase.
(iv) steadily decrease.
(v) remain constant.
(vi) show an unpredictable behavior that is dependent on the data set.

**Solution:** (iv) because the model becomes more flexible/less regularized and can thus fit to the traning data.

## b) 
Same situation as in a). Which statement is correct?

As we increase $s$ from 0, the bias will

(i) first increase and then start decreasing in an inverted U-shape.
(ii) first decrease and then start increasing in a U-shape.
(iii) steadily increase.
(iv) steadily decrease.
(v) remain constant.
(vi) show an unpredictable behavior that is dependent on the data set.

**Solution:** (iv) because the smaller $s$ the more the estimates are biased.




\clearpage

## c) 

We again look at the birth weight data set from Problem 4 to study some properties of the bootstrap method. Below we estimated the standard errors of the regression coefficients in the logistic regression model using 1000 bootstrap iterations (column `std.error`). These standard errors can be compared to those that we obtain by fitting a single logistic regression model using the `glm()` function with the same predictor variables. Look at the R output below and compare the standard errors that we obtain from these two approaches (note that the `t1*` to `t9*` variables are sorted in the same way as for the `glm()` output).

```{r}
r.glm <- glm(low ~ age + lwt + race + smoke + ptl + ht + ui, d.bw.train,family="binomial")
summary(r.glm)$coef

library(boot)
boot.fn <- function(data,index){
  return(coefficients(glm(low ~ age + lwt + race + smoke + ptl + ht + ui, d.bw.train,family="binomial",subset=index)))
}
boot(d.bw.train,boot.fn,1000)
```

Which of the following statements is true?

(i) For some variables there are large differences between the estimated standard errors, which indicates a problem with the bootstrap.
(ii) The differences between the estimated standard errors indicate a problem with the assumptions taken about the distribution of the estimated parameters in logistic regression.
(iii) The bootstrap output indicates that the $p$-values estimated with logistic regression tend to be too large (i.e., too conservative).
(iv) The bootstrap relies on random sampling of the same data without replacement.
(v) The coefficients from logistic regression seem to be biased.


**Solution**: (ii)



## d) 

Imagine a particular data set with only five observations. We carry out hierarchical clustering twice, once using single linkage and once using complete linkage. We obtain two dendograms. At a certain point in the single linkage dendogram the clusters \{1,2\} and \{3,4,5\} fuse (fusion 1). In the complete linkage dendogram the clusters fuse as well at a certain point (fusion 2).

Which statement is true?

(i) Fusion 1 will occur higher in the tree.
(ii) Fusion 2 will occur higher in the tree.
(iii) The fusions occur at the same height.
(iv) There is not enough information to tell which fusion is higher.


**Solution**: (iv)



## e) 

We are dealing with a fully connected feed-forward neural network for classification into 5 categories. The input dimension of the data is 128 and we have two hidden layers with 32 and 64 nodes in layers 1 and 2, respectively. We fit the network using keras in R and decide to use a mini-batch of size 32, dropout of 20\% in each layer and softmax activation in the output layer. Question: How many parameters do we need to estimate in total?

(i) `r 129*32 + 33*64 + 65*5` 
(ii) `r 128*32 + 32*64 + 64*5`
(iii) `r 0.8*(129*32 + 33*64 + 65*5)`
(iv) `r round(0.8*(128*32 + 32*64 + 64*5),0)`
(v) `r 129*32 + 32*64 + 64*5`
(vi) `r 32*32 + 32*64 + 64*5`

**Solution:**
(i) is correct - remember that there is one bias-node in each layer!


# Problem 6 (10P, multiple choice, 2P each)

## a) (2P)

Which of the following methods can be used to select a subset of variables for prediction?
   
(i) All types of regularization methods
(ii) Convolutional neural networks (CNNs)
(iii) Simple regression trees
(iv) Lasso
   
**Solution:**  FALSE, FALSE, TRUE, TRUE

## b) (2P)

Select all statements that are true for automatic model selection that minimizes AIC via forward or backward selection:

(i) Automatic model selection is not justified when the aim of the model is explanation (inference).
(ii) The procedure may introduce model selection bias.
(iii) The p-values of the final model tend to be too large.
(iv) If AIC is replaced by BIC we can circumvent the problems of the respective model selection procedure. 


**Solution:**  TRUE, TRUE, FALSE, FALSE



## c) (2P)

Which statements about validation set approach, $k$-fold cross-validation (CV) and leave-one-out cross validation (LOOCV) are true?

(i) 5-fold CV will generally lead to more bias, but less variance than LOOCV in the estimated prediction error.
(ii) The validation set-approach is computationally cheaper than $10$-fold CV.
(iii) The validation set-approach is the same as $2$-fold CV.
(iv) LOOCV is always the cheapest way to do cross-validation.

**Solution:** 

TRUE, TRUE, FALSE, FALSE

(i) and (ii) are correct. (iii) is wrong, because in 2-fold CV we would fit the model twice (once with each half of the data), while the validation set approach uses only one half of the data to fit the model. (iv) is wrong, because LOOCV is actually very expensive - expect for linear regression, where a formula exists.



## d) (2P)

Choose all the statements that are true: 

(i) In principal component regression (PCR) we automatically do variable selection when choosing a small number of PCs.
(ii) In SVMs, when the cost parameter $C$ is small, we tend to have low bias but high variance, and vice versa. 
(iii) The smoothing spline ensures smoothness of its function, $g$, by having a penalty term $\int g^{\prime}(t)^2 dt$ in its loss. 
(iv) The $K$-nearest neighbors regression (local regression) has a high bias when its parameter, $K$, is high.

**Solution**

(i) False
(ii) True/False (both correct due to ambiguity)
(iii) False: The penalty term is $\int g^{\prime \prime}(t)^2 dt$. 
(iv) True: because the local regression is based on k-nearest neighbor algorithm.



## e) (2P)


We are looking at the following non-linear decision boundary for a support vector classifier:

$$f(X_1,X_2) = (X_1+2)^2 + (X_2+2)^2 - 2 X_2^3  = 0 \ .$$
We assume that class 1 fulfills $f(X_1,X_2) > 0$ and class 2 fulfils $f(X_1,X_2)<0$. 

Which of the following statements are true?

(i) This decision boundary is linear in terms of $X_1$, $X_2$, $X_1^2$ and $X_2^2$.
(ii) The decision boundary has the shape of a circle.
(iii) The point $(x_1,x_2)=(1,-1)$ belongs to class 1.
(iv) The point $(x_1,x_2)=(1,3)$ belongs to class 2.

**Solution**
(i) False ($X_2^3$ does not cancel out).
(ii) False
(iii) True
(iv) True
